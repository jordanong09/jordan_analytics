[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ong Zhi Rong Jordan",
    "section": "",
    "text": "Currently a full-time servicemen, assuming an appointment in Training & Doctrine Command (TRADOC)."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Ong Zhi Rong Jordan",
    "section": "Education",
    "text": "Education\nNanyang Technological University - NTU | Singapore Bachelor of Engineering in Computer Engineering | Aug 2013 - Aug 2016\nSingapore Management University - SMU | Singapore Master of IT in Business (MITB)| Jan 2022 - Nov 2022 (Ongoing)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ong Zhi Rong Jordan",
    "section": "Experience",
    "text": "Experience\nNil"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bringing Data to Life",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\nclustering\n\n\n\n\nOng Zhi Rong Jordan\n\n\nDecember 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the difference between traditional clustering algorithm and spatially constrained clustering algorithm.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\nclustering\n\n\n\n\nOng Zhi Rong Jordan\n\n\nDecember 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatially constrained methods has a hard requirement that spatial objects in the same cluster are also geographically linked.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\nclustering\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nDecember 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\ngeospatial\n\n\nsf\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\ntibble\n\n\nggstatsplot\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 8, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all"
  },
  {
    "objectID": "index.html#posts-of-my-own-works",
    "href": "index.html#posts-of-my-own-works",
    "title": "Bringing Data to Life",
    "section": "Posts of my own works",
    "text": "Posts of my own works\n\n\n\n\n  \n\n\n\n\nRFM Model\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n \n\n\n\n\nJune 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nSIS Visual Representation\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\n\ntibble\n\n\nggstatsplot\n\n\n \n\n\n\n\nJune 8, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items\n\n\n See all posts of my work"
  },
  {
    "objectID": "index.html#posts-from-geospatial-analytics",
    "href": "index.html#posts-from-geospatial-analytics",
    "title": "Bringing Data to Life",
    "section": "Posts from Geospatial Analytics",
    "text": "Posts from Geospatial Analytics\n\n\n\n\n  \n\n\n\n\nCalibrating Hedonic Pricing Model for Private Highrise Property with GWR Method\n\n\nGeographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\nclustering\n\n\n \n\n\n\n\nDecember 10, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nRegionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods\n\n\nUnderstanding the difference between traditional clustering algorithm and spatially constrained clustering algorithm.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\nclustering\n\n\n \n\n\n\n\nDecember 7, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Geographic Segmentation with Spatially Constrained Cluster Analysis\n\n\nSpatially constrained methods has a hard requirement that spatial objects in the same cluster are also geographically linked.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\nclustering\n\n\ntmap\n\n\n \n\n\n\n\nDecember 3, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nGeospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 30, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Global and Local Measures of Spatial Autocorrelation\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Spatial Weights and Application\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Choropleth Mapping\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n \n\n\n\n\nNovember 23, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nData Wrangling of Geospatial Data\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\n \n\n\n\n\nNovember 19, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all posts of Geospatial"
  },
  {
    "objectID": "index.html#posts-from-visual-analytics",
    "href": "index.html#posts-from-visual-analytics",
    "title": "Bringing Data to Life",
    "section": "Posts from Visual Analytics",
    "text": "Posts from Visual Analytics\n\n\n\n\n\nNo matching items\n\n\n See all posts of Visual"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html",
    "href": "posts/Geo/Geospatial_HOE1/index.html",
    "title": "Introduction to Choropleth Mapping",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates. tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation. tmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#dataset",
    "href": "posts/Geo/Geospatial_HOE1/index.html#dataset",
    "title": "Introduction to Choropleth Mapping",
    "section": "Dataset",
    "text": "Dataset\nFor this analysis, we will extract data that is available from the web.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nExtracting geographical information from Dataset\nWe will leverage on the st_read function to retrieve polygon, line and point feature in both ESRI shapefile and KML format.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                      layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-map",
    "href": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-map",
    "title": "Introduction to Choropleth Mapping",
    "section": "Plotting of map",
    "text": "Plotting of map\n\nplot(mpsz)\n\n\n\nplot(st_geometry(mpsz))\n\n\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#changing-of-projection",
    "href": "posts/Geo/Geospatial_HOE1/index.html#changing-of-projection",
    "title": "Introduction to Choropleth Mapping",
    "section": "Changing of Projection",
    "text": "Changing of Projection\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#importing-and-converting-an-aspatial-data",
    "href": "posts/Geo/Geospatial_HOE1/index.html#importing-and-converting-an-aspatial-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Importing and Converting An Aspatial Data",
    "text": "Importing and Converting An Aspatial Data\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\nlist(listings) \n\n[[1]]\n# A tibble: 4,252 × 16\n       id name         host_id host_name neighbourhood_g… neighbourhood latitude\n    <dbl> <chr>          <dbl> <chr>     <chr>            <chr>            <dbl>\n 1  50646 Pleasant Ro…  227796 Sujatha   Central Region   Bukit Timah       1.33\n 2  71609 Ensuite Roo…  367042 Belinda   East Region      Tampines          1.35\n 3  71896 B&B  Room 1…  367042 Belinda   East Region      Tampines          1.35\n 4  71903 Room 2-near…  367042 Belinda   East Region      Tampines          1.35\n 5 275343 Convenientl… 1439258 Joyce     Central Region   Bukit Merah       1.29\n 6 275344 15 mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n 7 294281 5 mins walk… 1521514 Elizabeth Central Region   Newton            1.31\n 8 301247 Nice room w… 1552002 Rahul     Central Region   Geylang           1.32\n 9 324945 20 Mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n10 330089 Accomo@ RED… 1439258 Joyce     Central Region   Bukit Merah       1.29\n# … with 4,242 more rows, and 9 more variables: longitude <dbl>,\n#   room_type <chr>, price <dbl>, minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>\n\n\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nglimpse(listings_sf)\n\nRows: 4,252\nColumns: 15\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275343, 275…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 178, 81, 81, 52, 40, 72, 41, 49, 49…\n$ minimum_nights                 <dbl> 90, 90, 90, 90, 14, 14, 90, 8, 14, 14, …\n$ number_of_reviews              <dbl> 18, 20, 24, 48, 20, 13, 133, 105, 14, 1…\n$ last_review                    <date> 2014-07-08, 2019-12-28, 2014-12-10, 20…\n$ reviews_per_month              <dbl> 0.22, 0.28, 0.33, 0.67, 0.20, 0.16, 1.2…\n$ calculated_host_listings_count <dbl> 1, 4, 4, 4, 50, 50, 7, 1, 50, 50, 50, 4…\n$ availability_365               <dbl> 365, 365, 365, 365, 353, 364, 365, 90, …\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9), POINT (…\n\n\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   6603.608    2553464 MULTIPOLYGON (((24786.75 46...           37"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-geographical-data",
    "href": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-geographical-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Plotting of geographical data",
    "text": "Plotting of geographical data\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#visualising-of-geographical-data-using-ggplot2",
    "href": "posts/Geo/Geospatial_HOE1/index.html#visualising-of-geographical-data-using-ggplot2",
    "title": "Introduction to Choropleth Mapping",
    "section": "Visualising of geographical data using ggplot2",
    "text": "Visualising of geographical data using ggplot2\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`), y = as.numeric(`PreSch Count`)))+\n  geom_point() +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school Count\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#choropleth-mapping-with-r",
    "href": "posts/Geo/Geospatial_HOE1/index.html#choropleth-mapping-with-r",
    "title": "Introduction to Choropleth Mapping",
    "section": "Choropleth Mapping with R",
    "text": "Choropleth Mapping with R\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nDIY using quantile and 4 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "posts/geo.html",
    "href": "posts/geo.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nGeographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\nclustering\n\n\n \n\n\n\n\nDecember 10, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUnderstanding the difference between traditional clustering algorithm and spatially constrained clustering algorithm.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\nclustering\n\n\n \n\n\n\n\nDecember 7, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nSpatially constrained methods has a hard requirement that spatial objects in the same cluster are also geographically linked.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\nclustering\n\n\ntmap\n\n\n \n\n\n\n\nDecember 3, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 30, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n \n\n\n\n\nNovember 23, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\n \n\n\n\n\nNovember 19, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nGeographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\nclustering\n\n\n\n\nOng Zhi Rong Jordan\n\n\nDecember 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the difference between traditional clustering algorithm and spatially constrained clustering algorithm.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\nclustering\n\n\n\n\nOng Zhi Rong Jordan\n\n\nDecember 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatially constrained methods has a hard requirement that spatial objects in the same cluster are also geographically linked.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\nclustering\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nDecember 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\ngeospatial\n\n\nsf\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\ntibble\n\n\nggstatsplot\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 8, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Me/RFM/index.html",
    "href": "posts/Me/RFM/index.html",
    "title": "RFM Model",
    "section": "",
    "text": "The RFM model has become essential for businesses to identify high value customers and possible churn customers to conduct targeted marketing. Businesses have leverage RFM model to better understand customer behaviours and also calculate Customer Life Time Value (LTV). This could also translate to better budgeting for marketing cost using the (3:1) ratio of LTV:CAC. In this article, I will demonstrate how we can leverage on existing libraries to conduct unsupervised classification and lastly potential future works to enhance the model.\n\nknitr::include_graphics(\"RFM.png\")"
  },
  {
    "objectID": "posts/Me/RFM/index.html#libraries",
    "href": "posts/Me/RFM/index.html#libraries",
    "title": "RFM Model",
    "section": "Libraries",
    "text": "Libraries\nFor this analysis, we will use the following packages from CRAN.\ncluster - Methods for Cluster analysis. Much extended the original from Peter Rousseeuw, Anja Struyf and Mia Hubert, based on Kaufman and Rousseeuw (1990) “Finding Groups in Data”.tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.factoextra - Extract and Visualize the Results of Multivariate Data Analyses. GGally Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\n\npacman::p_load(cluster, tidyverse, factoextra,lubridate,patchwork, GGally, moments,bestNormalize) #refer to 1st post to understand the usage of pacman\n\npackage 'estimability' successfully unpacked and MD5 sums checked\npackage 'ellipse' successfully unpacked and MD5 sums checked\npackage 'emmeans' successfully unpacked and MD5 sums checked\npackage 'flashClust' successfully unpacked and MD5 sums checked\npackage 'leaps' successfully unpacked and MD5 sums checked\npackage 'scatterplot3d' successfully unpacked and MD5 sums checked\npackage 'FactoMineR' successfully unpacked and MD5 sums checked\npackage 'factoextra' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages\npackage 'GGally' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages\npackage 'lamW' successfully unpacked and MD5 sums checked\npackage 'rngtools' successfully unpacked and MD5 sums checked\npackage 'lobstr' successfully unpacked and MD5 sums checked\npackage 'LambertW' successfully unpacked and MD5 sums checked\npackage 'nortest' successfully unpacked and MD5 sums checked\npackage 'doRNG' successfully unpacked and MD5 sums checked\npackage 'butcher' successfully unpacked and MD5 sums checked\npackage 'bestNormalize' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages"
  },
  {
    "objectID": "posts/Me/RFM/index.html#data-set",
    "href": "posts/Me/RFM/index.html#data-set",
    "title": "RFM Model",
    "section": "Data Set",
    "text": "Data Set\n\nWe will use a customer data set that consist of 6 columns.\n\nCustomer_ID: Identification of Customer\nCategoryGroup: Category group of the item purchased\nCategory: Category of the item purchased\nInvoiceDate: The date of purchased\nQuantity: The number of items purchased\nTotalPrice: The total amount spend on that item"
  },
  {
    "objectID": "posts/Me/RFM/index.html#data-wrangling",
    "href": "posts/Me/RFM/index.html#data-wrangling",
    "title": "RFM Model",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ncustomer <- readRDS(\"data/customer.rds\")\n\nLet’s examine the data!\nFrom the summary, we can identify a few potential problems!\n\nCustomer_ID is in numeric not character. # I prefer IDs to be in character form since it is for representation of customer instead of number of customers.\n\nInvoiceDate is not in date time format!\n\nTotalPrice is not in numeric (the symbol was attached to the number)\n\n\nsummary (customer)\n\n  Customer_ID    CategoryGroup        Category         InvoiceDate       \n Min.   :12348   Length:395888      Length:395888      Length:395888     \n 1st Qu.:14132   Class :character   Class :character   Class :character  \n Median :15535   Mode  :character   Mode  :character   Mode  :character  \n Mean   :15462                                                           \n 3rd Qu.:16841                                                           \n Max.   :18287                                                           \n    Quantity        TotalPrice       \n Min.   :   1.00   Length:395888     \n 1st Qu.:   2.00   Class :character  \n Median :   4.00   Mode  :character  \n Mean   :   8.29                     \n 3rd Qu.:  12.00                     \n Max.   :1500.00                     \n\nhead(customer$TotalPrice)\n\n[1] \"£8\" \"£2\" \"£1\" \"£3\" \"£3\" \"£5\"\n\n\nChange of data class\nFor the date time format, we will leverage on lubridate functions to convert our exisiting date to date time format. Since the format is Month/Day/Year, we will use the function mdy. For TotalPrice, there are two symbols found, £ and ,. We will use the gsub function and replace all symbols to an empty space. Lastly, using as.numeric to convert it to a numeric class. For CustomerID, simply use as.character to convert it to character class.\n\ncustomer$InvoiceDate <- mdy(customer$InvoiceDate)\ncustomer$TotalPrice <- as.numeric(gsub(\"[£]|[,]\",\"\",customer$TotalPrice, perl=TRUE))\ncustomer$Customer_ID <- as.character(customer$Customer_ID)\n\nExtracting Recency, Frequency and Monetary\nRecency\nTo extract how recent the customer purchase an item from the store, we will use the last InvoiceDate to substract all the dates a customer purchase from the store and retrieve the minimum number. Since the format of Recency will be in datetime format, we will convert it using the as.numeric function.\n\ncustomer_recency <- customer %>%\n  mutate(recency = (max(InvoiceDate) + 1) - InvoiceDate) %>%\n  group_by(Customer_ID) %>%\n  summarise (Recency = as.numeric(min(recency)))\n\nFrequency\nTo extract how frequent the customer purchase an item from the store, we will use the n() function to find out how many different dates the customer visited the store.\n\ncustomer_frequency <- customer %>%\n  group_by(Customer_ID,InvoiceDate) %>%\n  summarise (count = n()) %>%\n  ungroup() %>%\n  group_by (Customer_ID) %>%\n  summarise (Frequency = n()) %>%\n  ungroup()\n\n\ncustomer_monetary <- customer %>%\n  group_by(Customer_ID) %>%\n  summarise (Monetary = sum(TotalPrice))\n\n\ncustomer_RFM <- customer_recency %>%\n  left_join (customer_frequency, by = \"Customer_ID\") %>%\n  left_join (customer_monetary, by = \"Customer_ID\")\n\nExamining the distribution of the RFM model\n\n# Histogram overlaid with kernel density curve\nrdplot <- ggplot(customer_RFM, aes(x=Recency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=10,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#ff9285\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nfdplot <- ggplot(customer_RFM, aes(x=Frequency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=2,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#906efa\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nmdplot <- ggplot(customer_RFM, aes(x=Monetary)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=250,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#d18500\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\n\nrdplot + fdplot + mdplot\n\n\n\n\nThrough the skewness and the histogrm, we can conclude that the attributes does not conform to normal distribution. Since all three attributes does not conform to a normal distribution and K-means would perform better with a normal distributed data, we will conduct data transformation. Utilizing the bestNormalise library, we can identify which normalization techniques best suits each attributes based on their distribution.\n\nskewness(customer_RFM$Recency)\n\n[1] 0.7240294\n\nskewness(customer_RFM$Frequency)\n\n[1] 3.781094\n\nskewness(customer_RFM$Monetary)\n\n[1] 1.435198\n\n\nWe can\n\nbestNormalize(customer_RFM$Recency)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 11.2249\n - Box-Cox: 7.8746\n - Center+scale: 27.8779\n - Exp(x): 22.7727\n - Log_b(x+a): 11.2725\n - orderNorm (ORQ): 1.1695\n - sqrt(x + a): 10.1741\n - Yeo-Johnson: 8.0219\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 5010 nonmissing obs and ties\n - 552 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n   1   32  121  382  676 \n\nbestNormalize(customer_RFM$Frequency)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 82.5826\n - Box-Cox: 83.172\n - Center+scale: 82.3083\n - Exp(x): 74.932\n - Log_b(x+a): 82.5978\n - orderNorm (ORQ): 82.2105\n - sqrt(x + a): 82.6232\n - Yeo-Johnson: 83.3307\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized exp(x) Transformation with 5010 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 2.741283e+28 \n - sd (before standardization) = 1.940317e+30 \n\nbestNormalize(customer_RFM$Monetary)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.3416\n - Box-Cox: 1.8263\n - Center+scale: 20.5024\n - Log_b(x+a): 2.3412\n - orderNorm (ORQ): 1.199\n - sqrt(x + a): 5.5771\n - Yeo-Johnson: 1.815\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 5010 nonmissing obs and ties\n - 2312 unique values \n - Original quantiles:\n     0%     25%     50%     75%    100% \n   4.00  313.25  713.50 1555.00 5004.00 \n\n\n\ncustomer_RFM_dt <- customer_RFM %>%\n  select(Recency, Frequency, Monetary)\n\nRecency <- orderNorm(customer_RFM_dt$Recency)\nFrequency <- boxcox (customer_RFM_dt$Frequency)\nMonetary <- orderNorm(customer_RFM_dt$Monetary)\n\ncustomer_RFM_dt$Recency <- Recency$x.t\ncustomer_RFM_dt$Frequency <- Frequency$x.t\ncustomer_RFM_dt$Monetary <- Monetary$x.t\n\nskewness(customer_RFM_dt$Recency)\n\n[1] 0.01507482\n\nskewness(customer_RFM_dt$Frequency)\n\n[1] 0.08897177\n\nskewness(customer_RFM_dt$Monetary)\n\n[1] 0.0006851529\n\n\n\n# Histogram overlaid with kernel density curve\nrdplot_dt <- ggplot(customer_RFM_dt, aes(x=Recency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#ff9285\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nfdplot_dt <- ggplot(customer_RFM_dt, aes(x=Frequency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#906efa\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nmdplot_dt <- ggplot(customer_RFM_dt, aes(x=Monetary)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#d18500\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\n\nrdplot_dt + fdplot_dt + mdplot_dt\n\n\n\n\n\ncustomer_RFM_cluster <- customer_RFM_dt %>%\n  select(Recency, Frequency, Monetary)\n\nK-means Clustering\nTo identify the optimal number of clusters using K means clustering, we will use the fviz_nbclust function and the silhouette and wss. Based on the silhouette score, the optimal cluster is 2 while the WSS score shows either 2 or 3. We will now explore both cluster size.\n\nset.seed(1234)\n\nfviz_nbclust(customer_RFM_cluster, kmeans, method = \"silhouette\")\n\n\n\nfviz_nbclust(customer_RFM_cluster, kmeans, method = \"wss\")\n\n\n\n\nInsights from Cluster\n\nkm_cluster2 <- kmeans(customer_RFM_cluster, \n                     2, \n                     nstart = 25)\n\n\n\nkm_cluster3 <- kmeans(customer_RFM_cluster, \n                     3, \n                     nstart = 25)\n\ncustomer_RFM$km_cluster2 <- as.character(km_cluster2$cluster)\n\ncustomer_RFM$km_cluster3 <- as.character(km_cluster3$cluster)\n\nFrom the table, we can identify that cluster 1 consist of customers on average made a purchase within 94 days, frequent the store 5 times and spend 1.7k. Whereas for cluster 2, the customers recency period on average is about 325 days, frequent on average 1 time and spend about $392. We can say that cluster 1 consist of our high value customers and cluster 2 consist of potential churn customers.\n\ncustomer_RFM %>%\n  group_by(km_cluster2) %>%\n  summarise(mean_recency = mean(Recency),\n            mean_frequency = mean(Frequency),\n            mean_monetary = mean(Monetary),\n            members = n()) \n\n# A tibble: 2 × 5\n  km_cluster2 mean_recency mean_frequency mean_monetary members\n  <chr>              <dbl>          <dbl>         <dbl>   <int>\n1 1                   94.1           5.45         1774.    2595\n2 2                  326.            1.39          393.    2415\n\ncustomer_RFM %>%\n  group_by(km_cluster3) %>%\n  summarise(mean_recency = mean(Recency),\n            mean_frequency = mean(Frequency),\n            mean_monetary = mean(Monetary),\n            members = n()) \n\n# A tibble: 3 × 5\n  km_cluster3 mean_recency mean_frequency mean_monetary members\n  <chr>              <dbl>          <dbl>         <dbl>   <int>\n1 1                  364.            1.11          295.    1681\n2 2                   54.6           7.32         2431.    1391\n3 3                  177.            2.80          865.    1938\n\n\nTo better visualise the distribution of our customers based on their cluster, we will leverage on the ggparcoord to visualise the distribution using a parallel coordinates plot.\n\n# Plot\nggparcoord(customer_RFM,\n    columns = 2:4, groupColumn = 5,\n    showPoints = TRUE,\n    scale=\"uniminmax\",\n    title = \"Parallel Coordinate Plot for the Customer Data\",\n    alphaLines = 0.3\n    ) + \n  theme_classic()+\n  theme(\n    plot.title = element_text(size=10)\n  )  + scale_color_brewer(palette = \"Set2\") + \n  guides(color=guide_legend(title=\"Cluster\"))"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html",
    "href": "posts/Me/SIS Representation/index.html",
    "title": "SIS Visual Representation",
    "section": "",
    "text": "In this article, I will share how we can leverage on Static, Interactive and Statistical (SIS) graphs to conduct appropriate data visualisation and draw statistical conclusion from the data set. In this article, we will explore varios libraries such as parsetR, ggstatsplot and ggplot."
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#libraries",
    "href": "posts/Me/SIS Representation/index.html#libraries",
    "title": "SIS Visual Representation",
    "section": "Libraries",
    "text": "Libraries\nInstead of using the base R function such as library() or install.packages(),we will use the p_load function from the pacman package that combine these functions together. Before using the package, you will need to install the package from CRAN.\n\ninstall.packages(\"pacman\")\n\nFor this analysis, we will use the following packages from CRAN.\nparsetR - Visualize your data with interactive d3.js parallel sets with the power and convenience of an htmlwidget.tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.ggstatsplot - An extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.patchwork - Combine separate ggplots into the same graphic.\n\npacman::p_load(parsetR, tidyverse, ggstatsplot, patchwork, hrbrthemes)"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-set",
    "href": "posts/Me/SIS Representation/index.html#data-set",
    "title": "SIS Visual Representation",
    "section": "Data Set",
    "text": "Data Set\n\nTwo different data set for this analysis:\n\n\nParticipants.csv - Information of all participants.\n\nFinancialJournal.csv- Input of the participant’s wages and expenses."
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-wrangling",
    "href": "posts/Me/SIS Representation/index.html#data-wrangling",
    "title": "SIS Visual Representation",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nknitr::include_graphics(\"qn1_concept.png\")\n\n\n\n\n\n\n\n\nparticipants <- read_csv(\"rawdata/Participants.csv\")\nfinance <- read_csv(\"rawdata/FinancialJournal.csv\")\n\nReducing of File Size uploading to Git\nTo reduce the requirement to upload the original data set, I will use the saveRDS function to convert my working tibble dataframe to a R data format namely .rds. We will subsequently use the readRDS function to read the data files in R.\n\nsaveRDS(participants, \"participants.rds\")\nsaveRDS(finance, \"finance.rds\")\n\n\nparticipants <- readRDS(\"data/participants.rds\")\nfinance <- readRDS(\"data/finance.rds\")"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-preparation",
    "href": "posts/Me/SIS Representation/index.html#data-preparation",
    "title": "SIS Visual Representation",
    "section": "Data Preparation",
    "text": "Data Preparation\nThrough the data from the participants, we can identify a total of 1011 participants ad 6 different attributes. The finance data shows the timestamp of the participants log and a category column. It seems like the data is in the long format and therefore we will subsequently pivot the data table to a wide format. We can also see that household size should be a categorical data rather than a numerical data. We address these issues using the dplyr package.\n\nsummary(participants)\n\n participantId    householdSize    haveKids            age       \n Min.   :   0.0   Min.   :1.000   Mode :logical   Min.   :18.00  \n 1st Qu.: 252.5   1st Qu.:1.000   FALSE:710       1st Qu.:29.00  \n Median : 505.0   Median :2.000   TRUE :301       Median :39.00  \n Mean   : 505.0   Mean   :1.964                   Mean   :39.07  \n 3rd Qu.: 757.5   3rd Qu.:3.000                   3rd Qu.:50.00  \n Max.   :1010.0   Max.   :3.000                   Max.   :60.00  \n educationLevel     interestGroup        joviality       \n Length:1011        Length:1011        Min.   :0.000204  \n Class :character   Class :character   1st Qu.:0.240074  \n Mode  :character   Mode  :character   Median :0.477539  \n                                       Mean   :0.493794  \n                                       3rd Qu.:0.746819  \n                                       Max.   :0.999234  \n\nsummary(finance)\n\n participantId      timestamp                        category        \n Min.   :   0.0   Min.   :2022-03-01 00:00:00.00   Length:1856330    \n 1st Qu.: 222.0   1st Qu.:2022-06-14 12:30:00.00   Class :character  \n Median : 464.0   Median :2022-10-06 16:20:00.00   Mode  :character  \n Mean   : 480.8   Mean   :2022-10-07 12:36:41.13                     \n 3rd Qu.: 726.0   3rd Qu.:2023-01-29 19:10:00.00                     \n Max.   :1010.0   Max.   :2023-05-25 00:05:00.00                     \n     amount         \n Min.   :-1562.726  \n 1st Qu.:   -5.594  \n Median :   -4.000  \n Mean   :   19.922  \n 3rd Qu.:   22.856  \n Max.   : 4096.526  \n\n\nAs part of Data Preparation, I prefer to ensure my columns are well worded. This would reduce the need to reword the X and Y axis subsequently for all the plots.\n\nparticipants <- participants %>%\n  rename('Participant Id' = 'participantId', \n         'Household Size' = 'householdSize', \n         'Have Kids' = 'haveKids', \n         'Age' = 'age', \n         'Education Level' = 'educationLevel', \n         'Interest Group' = 'interestGroup', \n         'Joviality' = 'joviality')\n\n\ncolnames(participants) # verify if the columns have been renamed correctly \n\n[1] \"Participant Id\"  \"Household Size\"  \"Have Kids\"       \"Age\"            \n[5] \"Education Level\" \"Interest Group\"  \"Joviality\"      \n\n#rename value \nparticipants$`Education Level` <- sub('HighSchoolOrCollege', \n                                    'High School or College',\n                                    participants$`Education Level`)\n\nparticipants$`Household Size` <- as.factor(participants$`Household Size`)\nparticipants$`Education Level` <- factor(participants$`Education Level`, levels = c(\n  \"Low\", \"High School or College\", \"Bachelors\", \"Graduate\"), ordered = TRUE) #create factor data object to categorise the Education Level by levels.\n\nWe will now examine how many different input categories are there. There are 6 different categories and 1011 participants throughout the period of 1 year and 2 months based on the timestamp. There should be a total of 2,547,720 financial records but the total recorded data was only 1,856,330. This shows some participants might not have recorded their finance throughout the period. We will now identify participants that are not consistent in their input.\n\nunique(finance$category)\n\n[1] \"Wage\"           \"Shelter\"        \"Education\"      \"RentAdjustment\"\n[5] \"Food\"           \"Recreation\"    \n\n\nBased on our analysis of the data, there are 131 participants who have only logged in less than 12 times throughout the period of analysis. We will identify these participants as inactive and will exclude them during our analysis of the the population demographics.\n\nincome <- finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise(count = n()) %>%\n  ungroup()\n\ninactive <- finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise(count = n()) %>%\n  filter (count < 13) %>%\n  ungroup()\n\n\ninactivepart <- inactive$participantId\n\nactive_participants <- subset(participants, !(`Participant Id` %in% inactivepart))\n\nSince the period of study is 15 months, we will extract the average monthly wage of each active participants using the summarise function and rounding the answer to 2 decimal place.\n\nactive_finance <- subset(finance, !(participantId %in% inactivepart))\n\nactive_finance <- active_finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise (Income = round(sum(amount)/15,2)) %>%\n  ungroup()\n\n\nactive_participants <- active_participants %>%\n  left_join (active_finance, by = c(\"Participant Id\" = \"participantId\")) %>%\n  mutate(Joviality = Joviality * 100)"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#visualisation-and-insights",
    "href": "posts/Me/SIS Representation/index.html#visualisation-and-insights",
    "title": "SIS Visual Representation",
    "section": "Visualisation and Insights",
    "text": "Visualisation and Insights\nVisualising using Static Graph\nWe will first visualise the distribution of the different attributes.\n\n\ngeom_text() is used to add annotations of the count and % values for geom_bar()\n\nGrids and background color are removed for a cleaner look as annotations are included.\nTo choose the different colours for the graph, I use medialab to decide on the Hue colors based on the number of graphs.\n\n\nage <- ggplot (active_participants, aes (x=Age)) +\n  geom_histogram(binwidth=5, fill=\"#c96d44\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Age Distribution of Active Participants\", subtitle = \"Bin Size 5\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nhKids <- active_participants %>%\n  ggplot(aes(x = `Have Kids`)) +\n  geom_bar(fill= '#777acd') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nwith/without Kids\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'))\n\n\nhousehold <- active_participants %>%\n  ggplot(aes(x = `Household Size`)) +\n   geom_bar(fill= '#7aa456') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nbased on Household Size\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'))\n\neducation <- active_participants %>%\n  ggplot(aes(x = `Education Level`)) +\n   geom_bar(fill= '#c65999') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nbased on Education Level\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'), title = element_text(size = 10))\n\n\n(age + hKids)/(household + education) #using patchwork to stitch the different graphs together\n\n\n\n\n\njoy <- ggplot (active_participants, aes (x=Joviality)) +\n  geom_histogram(binwidth=5, fill=\"#9c954d\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Joviality Distribution of Active Participants\", subtitle = \"Bin Size 5\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nincome <- ggplot (active_participants, aes (x=Income)) +\n  geom_histogram(binwidth=1000, fill=\"#b067a3\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Income Distribution of Active Participants\", subtitle = \"Bin Size 1000\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nincome + joy\n\n\n\n\nWe will conduct binning on our numerical data such as Age, Income and Joviality. We use the ntile function to break the values and case_when() to change the group labels accordingly.\n\nactive_participants_grouped <- active_participants %>%\n  mutate (Income_group = ntile(Income, 4)) %>%\n  mutate (Joviality_group = ntile(Joviality, 4)) %>%\n  mutate (Income_group = case_when(\n    Income_group == 1 ~ \"Low Income\",\n    Income_group == 2 ~ \"Mid-Low Income\",\n    Income_group == 3 ~ \"Mid-High Income\",\n    Income_group == 4 ~ \"High Income\"\n  )) %>%\n  mutate (Joviality_group = case_when(\n    Joviality_group == 1 ~ \"Low Joy\",\n    Joviality_group == 2 ~ \"Mid-Low Joy\",\n    Joviality_group == 3 ~ \"Mid-High Joy\",\n    Joviality_group == 4 ~ \"High Joy\"\n  ))\n\nVisualising using Interactive Graph\nWe will now analyse the data using interactive graphs such as parallel set plot. We will leverage on the parset library to provide interactive function. The interesting feature about the parset function is that it allows the user to dynamically shift the levels of the attributes (top-bottom and left-right), providing the user a more interactive visualisation of the data set.\n\nactive_participants_parset <- active_participants_grouped %>%\n  select (`Household Size`, `Have Kids`, `Education Level`, `Interest Group`, Income_group, Joviality_group)\n\nparset(active_participants_parset)\n\n\n\n\n\nVisualising using Statistical Graph\nFrom the Parset plot, we identify a few probable relationship such as Education Level to Income Level etc. We will now use statistical plot to verify our claim. The ggstatsplot library provides a suite of statistical plot to allow user to choose the plot based on its data set. For this study, since our attributes are in categorical form, I will leverage on the ggbarstats.\nInsights\nPearson’s \\(x^2\\)-test of independence revealed that, across 880 participants,there was a significant association between Income Level, Education Level and Joviality Level. (p-value below alpha value of 0.05). The Bayes Factor for the left analysis revealed that the data were \\(8e^{66}\\) times more probable under the alternative hypothesis as compared to the null hypothesis. This can be considered extreme evidence (Sandra Andraszewicz, 2015) in favor of the alternative hypothesis. The Bayes Factor for the right analysis revealed that the data were 23968348874 times more probable under the alternative hypothesis as compared to the null hypothesis. This can also be considered extreme evidence in favor of the alternative hypothesis.\n\nactive_participants_parset$Income_group <- factor(active_participants_parset$Income_group, levels = c(\n  \"Low Income\", \"Mid-Low Income\", \"Mid-High Income\", \"High Income\"), ordered = TRUE) #create factor data object to segment the Education Level by levels.\n\nincome <- ggbarstats(\n  data = active_participants_parset,\n  x = `Education Level`,\n  y = Income_group,\n  type = \"np\",\n  xlab = \"Income Group\"\n)\n\njoy <- ggbarstats(\n  data = active_participants_parset,\n  x = Joviality_group,\n  y = Income_group,\n  type = \"np\",\n  xlab = \"Income Group\"\n)\n\n\nincome + joy"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#conclusion",
    "href": "posts/Me/SIS Representation/index.html#conclusion",
    "title": "SIS Visual Representation",
    "section": "Conclusion",
    "text": "Conclusion\nIt is important for data analyst to understand the importance of static and interactive graphs, how we should leverage these tools to provide appropriate data visualisation and subsequently use statistical graphs to draw statistical conclusion to support the hypothesis."
  },
  {
    "objectID": "posts/me.html",
    "href": "posts/me.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n \n\n\n\n\nJune 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\n\ntibble\n\n\nggstatsplot\n\n\n \n\n\n\n\nJune 8, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/visual.html",
    "href": "posts/visual.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html",
    "href": "posts/Geo/Geospatial_Choropleth/index.html",
    "title": "Introduction to Choropleth Mapping",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nleaflet - Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package.\n\nscales - Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\nviridis - color scales in this package to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale\n\n\nShow the codepacman::p_load(sf, tidyverse, tmap, leaflet, scales, viridis)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#libraries",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#libraries",
    "title": "Introduction to Choropleth Mapping",
    "section": "Libraries",
    "text": "Libraries\nFor this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nleaflet - Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package.\n\nscales - Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\nviridis - Color scales in this package to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale.\n\n\npacman::p_load(sf, tidyverse, tmap, leaflet, scales, viridis)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#data-preparation",
    "title": "Introduction to Choropleth Mapping",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\nImporting of Geospatial Data\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz. To view the tibble data frame, we can simply call the tibble file name mpsz. When you print a tibble, it only shows the first ten rows and all the columns that fit on one screen. It also prints an abbreviated description of the column type.\n\nShow the codempsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Choropleth\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nShow the codempsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nImporting Attribute Data into R\nNext, we will import respopagsex2000to2018.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\nShow the codepopdata <- read_csv(\"data/respopagesextod2011to2020.csv\")\n\n\nData Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\nShow the codepopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\nJoining of attribute and geospatial data frame\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nShow the codepopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#choropleth-mapping-of-geospatial-data",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#choropleth-mapping-of-geospatial-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Choropleth Mapping of Geospatial Data",
    "text": "Choropleth Mapping of Geospatial Data\nPlotting using TMap\ntmap has similar syntax to the popular ggplot2 but will also produce a reasonable map with only a couple of lines of code. A default colour scheme will be given where this is not specified and passed to tm_polygons and a legend will also be created by default.\ntmap also offer the user two views, static (plot) or interactive (view).\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\nPlotting a choropleth map quickly by using qtm()\nqtm is termed as quick thematic mode allow users to quickly draw a choropleth with a single line of code. It is concise and provides a good default visualisation in many cases. We will explore the different view that tmap provides.\nThe code chunk below will draw an interactive cartographic standard choropleth map as shown below. The fill argument is used to map the attribute. (i.e. DEPENDENCY)\nThe interactive mode uses the leaflet library. Since the leaflet library require the sf object to be in WGS84, we need to set the tmap_options to true to allow our data set which is SVY21 to be plotted on the leaflet map.\n\nShow the codetmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\nShow the codetmap_mode(\"plot\")\n\n\nThe code chunk below will draw a static cartographic standard choropleth map as shown below.\n\nShow the codeqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\nstatic map using qtm\n\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used. In the following sub-section, we will share with you tmap functions that used to plot these elements.\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\nstatic tmap with tmap elements\n\n\n\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\nShow the codetm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\nbase map without elements\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons(). This is similar to the qtm drawn earlier.\n\nThe default interval binning used to draw the choropleth map is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer\nBy default, Missing value will be shaded in grey.\n\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\nmap with polygons\n\n\n\n\nDrawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\nmap with fill only\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\nmap with fill and borders\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nThe code chunk below shows multiple data classification methods and classes to illustrate the difference. tmap_arrange is used to display the consolidated maps in grid form.\n\nShow the codetmap1 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Quantile - 4 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap2 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Jenks - 4 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap3 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Equal - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap4 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Hclust - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap5 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Sd - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\n\ntmap6 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Kmeans - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap_arrange(tmap1, tmap2, tmap3, tmap4, tmap5, tmap6, ncol = 3)\n\n\n\n\n\n\ntmap with multiple data classification methods and classes\n\n\n\n\n\nShow the codesummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\nShow the codetmap_mode(\"plot\")\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5) \n\n\n\n\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"white\")\n\n\n\n\n\nShow the codetm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\nShow the codetm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\nShow the codetm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nShow the codeyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\nShow the codetm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nPlotting using ggplot\nggplot provides the user much more flexibility in the layers required on the map. Since our object is an sf object, we will use geom_sf which will automatically detect a geometry column and map it. coord_sf is also used to govern the map projection.\n\nShow the codeggmap <- ggplot(data = mpsz_pop2020) +\n  geom_sf(aes(fill = YOUNG)) +\n  geom_text(aes(x = X_ADDR, y = Y_ADDR, label = PLN_AREA_C), size = 1) + #input the planning area labels\n  xlab(\"Longitude\") + ylab(\"Latitude\") + #x and y axis name\n  ggtitle(\"Dependency level across Planning Area\") + #title\n  theme_bw() + #theme chosen\n  theme(panel.grid.major = element_line(color = gray(.5), linetype = \"dashed\", size = 0.5),\n        panel.background = element_rect(fill = \"aliceblue\")) + \n  coord_sf(crs = st_crs(3414)) \n\nggmap\n\n\n\n\nThe viridis package also allow the user to improve the colour scaling on the plot. Since we use fill to fill the map with the Young attribute, we will use scale_fill_viridis to scale the variable based on the viridis palette.\n\nShow the codeggmap + scale_fill_viridis(option = \"magma\", direction = -1)\n\n\n\n\nPlotting using leaflet\nLeaflet is one of the most popular open-source JavaScript libraries for interactive maps. This package has grown significantly in popularity in recent years and has fast become common currency amongst companies wishing to dynamically visualize its data. It is an excellent option to consider where the patterns in your data are large and complex and where you have constituent polygons of varying sizes.\nFeatures\n\nInteractive panning/zooming\n\nCompose maps using arbitrary combinations of:\n\nMap tiles\nMarkers\nPolygons\nLines\nPopups\nGeoJSON\n\n\nCreate maps right from the R console or RStudio\nEmbed maps in knitr/R Markdown documents and Shiny apps\nEasily render spatial objects from the sp or sf packages, or data frames with latitude/longitude columns\nUse map bounds and mouse events to drive Shiny logic\nDisplay maps in non spherical mercator projections\nAugment map features using chosen plugins from leaflet plugins repository\n\nData Preparation for leaflet mapping\nFirstly, we create a new column and scale the Young attribute from 0 - 100. We use the colorBin function to maps numeric input data to a fixed number of output colors using the bin created. We then create the interactive labels using the sprintf function.\n\nShow the codempsz_pop2020_leaflet <- mpsz_pop2020 %>%\n  mutate (youngpct = rescale(YOUNG, to = c(0,100)))\n\nmpsz_pop2020_leaflet$youngpct[is.nan(mpsz_pop2020_leaflet$youngpct)]<-0\n\nbins <- c(0, 20, 30, 40, 50, 60, 70, 80, 90, Inf)\npal <- colorBin(\"YlOrRd\", domain = mpsz_pop2020_leaflet$youngpct, bins = bins)\n\nlabels <- sprintf(\n  \"<strong>%s</strong><br/>%g Young Pct\",\n  mpsz_pop2020_leaflet$PLN_AREA_N, mpsz_pop2020_leaflet$youngpct\n) %>% lapply(htmltools::HTML)\n\n\nCRS projection for leaflet mapping\nThe Leaflet package expects all point, line, and shape data to be specified in latitude and longitude using WGS 84 (a.k.a. EPSG:4326). By default, when displaying this data it projects everything to EPSG:3857 and expects that any map tiles are also displayed in EPSG:3857.\nTherefore, we will need to transform our sf object to the correct crs using st_transform.\n\nShow the codempsz_pop2020_leaflet <- mpsz_pop2020_leaflet %>%\n  st_transform(crs = 4326)\n\n\nPlotting of leaflet map\nThe easiest way to add tiles is by calling addTiles() with no arguments; by default, OpenStreetMap tiles are used. But many popular free third-party basemaps can be added using the addProviderTiles() function, which is implemented using the leaflet-providers plugin.\nAs a convenience, leaflet also provides a named list of all the third-party tile providers that are supported by the plugin. This enables you to use auto-completion feature of your favorite R IDE (like RStudio) and not have to remember or look up supported tile providers; just type providers$ and choose from one of the options. You can also use names(providers) to view all of the options. For this visualisation, I will use the CartoDB.Positron tiles.\n\nShow the codeleaflet(mpsz_pop2020_leaflet) %>%\n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolygons(\n  fillColor = ~pal(youngpct),\n  weight = 1,\n  opacity = 1,\n  color = \"white\",\n  dashArray = \"3\",\n  fillOpacity = 0.7,\n  highlight = highlightOptions(\n    weight = 5,\n    color = \"#666\",\n    dashArray = \"\",\n    fillOpacity = 0.7,\n    bringToFront = TRUE),\n  label = labels,\n  labelOptions = labelOptions(\n    style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n    textsize = \"15px\",\n    direction = \"auto\")) %>%\naddLegend(pal = pal, values = ~youngpct, opacity = 0.7, title = NULL,\n                position = \"bottomright\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html",
    "title": "Data Wrangling of Geospatial Data",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#dataset",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#dataset",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Dataset",
    "text": "Dataset\nFor this analysis, we will extract data that is available from the web.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nExtracting geographical information from Dataset\nWe will leverage on the st_read function to retrieve polygon, line and point feature in both ESRI shapefile and KML format.\nThe st_geometry function returns an object of class sfc whereas the glimpse function from dplyr act as a transposed version of the print function that shows the values of the different columns.\nTo check all the classes within the dataset, we use the sapply function to run the class function through all the columns within the data set and return their classes.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath <- st_read(dsn = \"data/geospatial\", \n                      layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool <- st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nsapply(mpsz, class)\n\n$OBJECTID\n[1] \"integer\"\n\n$SUBZONE_NO\n[1] \"integer\"\n\n$SUBZONE_N\n[1] \"character\"\n\n$SUBZONE_C\n[1] \"character\"\n\n$CA_IND\n[1] \"character\"\n\n$PLN_AREA_N\n[1] \"character\"\n\n$PLN_AREA_C\n[1] \"character\"\n\n$REGION_N\n[1] \"character\"\n\n$REGION_C\n[1] \"character\"\n\n$INC_CRC\n[1] \"character\"\n\n$FMEL_UPD_D\n[1] \"Date\"\n\n$X_ADDR\n[1] \"numeric\"\n\n$Y_ADDR\n[1] \"numeric\"\n\n$SHAPE_Leng\n[1] \"numeric\"\n\n$SHAPE_Area\n[1] \"numeric\"\n\n$geometry\n[1] \"sfc_MULTIPOLYGON\" \"sfc\""
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geospatial-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geospatial-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Plotting of geospatial data",
    "text": "Plotting of geospatial data\nUnlike non-geospatial dataset where we plot the data using charts, we will leverage on map-based visualisation to draw insights from our geospatial data. The plot function uses the geometry data, contained primarily in the polygons slot. plot is one of the most useful functions in R, as it changes its behaviour depending on the input data. From the example below, we can see how we manipulate the plot based on how we subset the dataset.\n\nplot(mpsz) #plot based on the different column attributes\n\n\n\nplot(mpsz[\"PLN_AREA_N\"]) #colour plot based on column `PLN_AREA_N`\n\n\n\nplot(st_geometry(mpsz)) #only plot the basic geometry of the polygon data\ncondition <- mpsz$SUBZONE_NO > 5 #set a condition\nplot(mpsz[condition, ], col = \"turquoise\", add = TRUE) #layer the condition above the initial plot"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#changing-of-projection",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#changing-of-projection",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Changing of Projection",
    "text": "Changing of Projection\nThe Coordinate Reference System (CRS) of spatial objects defines where they are placed on the Earth’s surface. We need to ensure the CRS of our sf objects are correct. Since Singapore uses EPSG:3414 - SVY21 / Singapore TM and from the above details, we understand that all the sf object does not conform to the correct CRS (WGS 84 or SVY21). We will utilise two different function, st_set_crs or st_transform to manually change the CRS of our sp object to the desired value.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n\n\nst_geometry(mpsz3414)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\nst_geometry(preschool3414)\n\nGeometry set for 1359 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25667.6 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#importing-and-converting-an-aspatial-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#importing-and-converting-an-aspatial-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Importing and Converting an Aspatial Data",
    "text": "Importing and Converting an Aspatial Data\nR provides the function to convert any foreign object to an sf object using the st_as_sf function. This will allow user to provide a data table that consist of the longitude and latitude and select the correct CRS to transform it to the approriate sf object.\nAfter importing the data, we will examine the dataframe using the list function.\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\nlist(listings) \n\n[[1]]\n# A tibble: 4,252 × 16\n       id name         host_id host_name neighbourhood_g… neighbourhood latitude\n    <dbl> <chr>          <dbl> <chr>     <chr>            <chr>            <dbl>\n 1  50646 Pleasant Ro…  227796 Sujatha   Central Region   Bukit Timah       1.33\n 2  71609 Ensuite Roo…  367042 Belinda   East Region      Tampines          1.35\n 3  71896 B&B  Room 1…  367042 Belinda   East Region      Tampines          1.35\n 4  71903 Room 2-near…  367042 Belinda   East Region      Tampines          1.35\n 5 275343 Convenientl… 1439258 Joyce     Central Region   Bukit Merah       1.29\n 6 275344 15 mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n 7 294281 5 mins walk… 1521514 Elizabeth Central Region   Newton            1.31\n 8 301247 Nice room w… 1552002 Rahul     Central Region   Geylang           1.32\n 9 324945 20 Mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n10 330089 Accomo@ RED… 1439258 Joyce     Central Region   Bukit Merah       1.29\n# … with 4,242 more rows, and 9 more variables: longitude <dbl>,\n#   room_type <chr>, price <dbl>, minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>\n\n\nThe output reveals that the data frame consists of 4252 rows and 16 columns. The column longtitude and latitude will be required for to transform this data frame to a sf object.\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nst_crs(listings_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#joining-sf-and-tibble-dataframe",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#joining-sf-and-tibble-dataframe",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Joining sf and tibble dataframe",
    "text": "Joining sf and tibble dataframe\nOne way to manipulate a dataframe is to combine two different sets of data frame together to combine the information retrieved. We will now aggregate the room price of the apartment based on the planning area.\n\n\nmutate: Adds new variables and preserves existing ones. If the new column is referencing an exisiting column, it will replace the variable. Since all the planning area are in uppercase in the mpsz data frame, we will use toupper to convert all the variables inside neighbourhood to uppercase.\n\nfilter: To remove irrelevant rows that are not required for the join.\n\nrename: Rename the column. I will be changing the neighbourhood to PLN_AREA_N to allow both data frame to identify the keys for the join.\n\nsummarise: After grouping the variables through the group_by function, we will summarise it to one row with the average price using the mean function.\n\n\nlistings_tidy <- listings %>%\n  mutate (neighbourhood = toupper(neighbourhood)) %>%\n  filter ((neighbourhood %in% unique(mpsz$PLN_AREA_N))) %>%\n  rename(\"PLN_AREA_N\" = \"neighbourhood\") %>%\n  group_by(PLN_AREA_N) %>%\n  summarise (avgprice = mean(price)) %>%\n  ungroup()\n\n\nmpsz3414 <- mpsz3414 %>%\n  left_join(listings_tidy)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#geoprocessing-with-sf-package",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#geoprocessing-with-sf-package",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nBuffering\nIn some cases, there is a need to create a buffering zone along the linestring object. An example would be to expand 5m along a road and understanding the total area increased through the expansion. One way we can do this is to use the st_buffer function that computes a buffer around this geometry/each geometry. To find out the overall area, st_area will be used. If the coordinates are in degrees longtitude/latitude, st_geod_area is used for area calculation.\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\nVisualising of buffering\nFrom the below visualisation, we are able to better understand how the buffer distance is being calculated and the different endCapStyle to be use for the buffer.\n\ncyclingpath_buffer <- cyclingpath[1,] %>%\n  select (-CYL_PATH_1)\n\nop = par(mfrow=c(2,3))\nplot(st_buffer(cyclingpath_buffer, dist = 1, endCapStyle=\"ROUND\"), reset = FALSE, main = \"endCapStyle: ROUND, distance 1\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\nplot(st_buffer(cyclingpath_buffer, dist = 2, endCapStyle=\"FLAT\"), reset = FALSE, main = \"endCapStyle: FLAT, distance 2\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\nplot(st_buffer(cyclingpath_buffer, dist = 3, endCapStyle=\"SQUARE\"), reset = FALSE, main = \"endCapStyle: SQUARE, distance 3\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\n\n\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area avgprice                       geometry PreSch Count\n1   6603.608    2553464 74.53191 MULTIPOLYGON (((24786.75 46...           37"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geographical-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geographical-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Plotting of geographical data",
    "text": "Plotting of geographical data\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#visualising-of-geographical-data-using-ggplot2",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#visualising-of-geographical-data-using-ggplot2",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Visualising of geographical data using ggplot2",
    "text": "Visualising of geographical data using ggplot2\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`), y = as.numeric(`PreSch Count`)))+\n  geom_point() +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school Count\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#conclusion",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#conclusion",
    "title": "Introduction to Choropleth Mapping",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html",
    "title": "Introduction to Spatial Weights and Application",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#data-preparation",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nHunan.shp: A shapefile of the Hunan Province that consist of all the capital\nHunan.csv: A csv file containing multiple attributes of each capital within Hunan\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment. We will then use a relational join left_join to combine the spatial and aspatial data together.\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan <- left_join(hunan,hunan2012)"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#visualisation-of-spatial-data",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#visualisation-of-spatial-data",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Visualisation of spatial data",
    "text": "Visualisation of spatial data\nFor the visualisation, we will only be using tmap. We will prepare a basemap anbd a choropleth map to visualise the distribution of GDP per capita among the capital.\n\nbasemap <- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.4)\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\nbase map and choropleth map"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-contiguity-spatial-weights",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-contiguity-spatial-weights",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nContiguity means that two spatial units share a common border of non-zero length. There are multiple criterion of contiguity such as:\n\n\nRook: When only common sides of the polygons are considered to define the neighbor relation (common vertices are ignored).\n\nQueen: The difference between the rook and queen criterion to determine neighbors is that the latter also includes common vertices.\n\nBishop: Is based on the existence of common vertices between two spatial units.\n\n\n\n\n\nContiguity Weights\n\n\n\n\nExcept in the simplest of circumstances, visual examination or manual calculation cannot be used to create the spatial weights from the geometry of the data. It is necessary to utilize explicit spatial data structures to deal with the placement and layout of the polygons in order to determine whether two polygons are contiguous.\nWe will use the poly2nb function to construct neighbours list based on the regions with contiguous boundaries. Based on the documentation, user will be able to pass a queen argument that takes in True or False. The argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nComputing (QUEEN) contiguity based neighbour\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nbased on the summary report above,the report shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nhunan$GDPPC[wm_q[[1]]]\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nComputing (ROOK) contiguity based neighbour\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\nVisualising the weights matrix\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. To retrieve the centroid of each area, we will use the st_centroid function.\n\ncoords <- st_centroid(st_geometry(hunan))\n\nPlotting Queen and Rook contiguity based neighbours map\n\npar(mfrow=c(1,2))\n\nplot(st_geometry(hunan), border=\"grey\", main = \"Queen Contiguity\")\nplot(wm_q, coords,pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\nplot(st_geometry(hunan), border=\"grey\", main = \"Rook Contiguity\")\nplot(wm_r, coords,pch = 19, cex = 0.6, add = TRUE, col= \"blue\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-distance-based-neighbours",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-distance-based-neighbours",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Computing distance based neighbours",
    "text": "Computing distance based neighbours\nIn this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package. The function identifies neighbours of region points by Euclidean distance in the metric of the points between lower (greater than or equal to and upper (less than or equal to) bounds.\nDetermine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.80   32.53   38.06   38.91   44.66   58.25 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 58.25 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\nComputing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d59 <- dnearneigh(coords, 0, 59)\nwm_d59\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 298 \nPercentage nonzero weights: 3.84814 \nAverage number of links: 3.386364 \n\n\nThe report shows that on average, every area should have at least 3 neighbours (links).\nTo display the structure of the weight matrix is to combine table() and card() of spdep.\n\nhead(table(hunan$County, card(wm_d59)),10)\n\n           \n            1 2 3 4 5 6\n  Anhua     1 0 0 0 0 0\n  Anren     0 0 0 1 0 0\n  Anxiang   0 0 0 0 1 0\n  Baojing   0 0 0 1 0 0\n  Chaling   0 0 1 0 0 0\n  Changning 0 0 1 0 0 0\n  Changsha  0 0 0 1 0 0\n  Chengbu   1 0 0 0 0 0\n  Chenxi    0 0 0 1 0 0\n  Cili      1 0 0 0 0 0\n\n\nVisualising distance weight matrix\nThe left graph with the red lines show the links of 1st nearest neighbours and the right graph with the black lines show the links of neighbours within the cut-off distance of 59km.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d59, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\nAdaptive distance weight matrix\nOther than using distance as a criteria to decide the neighbours, it is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below:\n\nknn6 <- knn2nb(knearneigh(coords, k=6)) #k refers to the number of neighbours per area\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nPlotting adaptive distance weight\nWe can plot the adaptive distance weight matrix using the code chunk below:\n\nplot(hunan$geometry, border=\"lightgrey\", main = \"Adaptive Distance Weight\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\nWeights based on Inversed Distance Weighting (IDW)\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\nWe will use the [lapply()] (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lapply) to apply the inverse function through the list.\n\ndist <- nbdists(wm_q, coords, longlat = TRUE)\nids <- lapply(dist, function(x) 1/(x))\nhead(ids,5)\n\n[[1]]\n[1] 1.694446 3.805533 1.847048 2.867007 1.166097\n\n[[2]]\n[1] 1.694446 1.832614 1.889267 2.537233 1.681209\n\n[[3]]\n[1] 3.805533 3.007305 3.588446 1.468997\n\n[[4]]\n[1] 1.847048 3.007305 3.731278 1.490622\n\n[[5]]\n[1] 3.588446 3.731278 1.526472 1.775459\n\n\nNext, we will use the nb2listw to apply the weights list with values given by the coding scheme style chosen. There are multiple style to choose from:\n\nB (Basic Binary Coding)\nW (Row Standardised) - sums over all links to n\nC (Globally Standardised) - sums over all links to n\nU (Globally Standardised / No of neighbours) - sums over all links to unity\nS (Variance-Stabilizing Coding Scheme) - sums over all links to n\nminmax - divides the weights by the minimum of the maximum row sums and maximum column sums of the input weights\n\nFor the simplifed analysis, we will use the W (Row Standardised).\n\nrswm_q <- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nFrom the earlier example, we know that the first Id has 5 neighbours. We take a look at the weight distribution of these 5 neighours. Since we are using Row Standardised, they should be equal.\n\nrswm_q$weights[1]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nEach neighbor is assigned a 0.2 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied."
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#application-of-spatial-weight-matrix",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#application-of-spatial-weight-matrix",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Application of Spatial Weight Matrix",
    "text": "Application of Spatial Weight Matrix\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights\nspatial lag as a sum of neighbouring values\nspatial window average\nspatial window sum\n\nSpatial lag with row-standardized weights\nFirstly, we’ll compute the average neighbor GDPPC value for each polygon using the lag.listw() that can compute the lag of a vector. These values are often referred to as spatially lagged values.\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nhead(GDPPC.lag)\n\n[1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80\n\n\nIn the previous section, we retrieved the GDPPC of the neighbours of the first area by using the following code chunk:\n\nhunan$GDPPC[wm_q[[1]]]\n\n[1] 20981 34592 24473 21311 22879\n\n\nFrom this, we can understand that the spatial lag with row-standardized weights is actually the average GDPPC of its neighbours.\n\\[(20981+34592+24473+21311+22879/5 = 24847.20)\\]\nWe will now append these lagged values to our Hunan data frame.\n\nlag.df <- as.data.frame(list(hunan$NAME_3,GDPPC.lag))\ncolnames(lag.df) <- c(\"NAME_3\", \"lag GDPPC\")\nhunan <- left_join(hunan,lag.df)\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\nGDPPC vs lag GDPPC\n\n\n\n\nSpatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\n\nb_weights2 <- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_df <- as.data.frame (list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC)))\ncolnames(lag_df) <- c(\"NAME_3\", \"lag_sum GDPPC\")\nhunan <- left_join(hunan, lag_df)\n\nlag_df\n\n          NAME_3 lag_sum GDPPC\n1        Anxiang        124236\n2        Hanshou        113624\n3         Jinshi         96573\n4             Li        110950\n5          Linli        109081\n6         Shimen        106244\n7        Liuyang        174988\n8      Ningxiang        235079\n9      Wangcheng        273907\n10         Anren        256221\n11       Guidong         98013\n12         Jiahe        104050\n13         Linwu        102846\n14       Rucheng         92017\n15       Yizhang        133831\n16      Yongxing        158446\n17        Zixing        141883\n18     Changning        119508\n19      Hengdong        150757\n20       Hengnan        153324\n21      Hengshan        113593\n22       Leiyang        129594\n23        Qidong        142149\n24        Chenxi        100119\n25     Zhongfang         82884\n26       Huitong         74668\n27      Jingzhou         43184\n28        Mayang         99244\n29       Tongdao         46549\n30      Xinhuang         20518\n31          Xupu        140576\n32      Yuanling        121601\n33      Zhijiang         92069\n34 Lengshuijiang         43258\n35    Shuangfeng        144567\n36        Xinhua        132119\n37       Chengbu         51694\n38        Dongan         59024\n39       Dongkou         69349\n40       Longhui         73780\n41      Shaodong         94651\n42       Suining        100680\n43        Wugang         69398\n44       Xinning         52798\n45       Xinshao        140472\n46      Shaoshan        118623\n47    Xiangxiang        180933\n48       Baojing         82798\n49     Fenghuang         83090\n50       Guzhang         97356\n51       Huayuan         59482\n52        Jishou         77334\n53      Longshan         38777\n54          Luxi        111463\n55      Yongshun         74715\n56         Anhua        174391\n57           Nan        150558\n58     Yuanjiang        122144\n59      Jianghua         68012\n60       Lanshan         84575\n61      Ningyuan        143045\n62     Shuangpai         51394\n63       Xintian         98279\n64       Huarong         47671\n65      Linxiang         26360\n66         Miluo        236917\n67     Pingjiang        220631\n68      Xiangyin        185290\n69          Cili         64640\n70       Chaling         70046\n71        Liling        126971\n72       Yanling        144693\n73           You        129404\n74       Zhuzhou        284074\n75       Sangzhi        112268\n76       Yueyang        203611\n77        Qiyang        145238\n78      Taojiang        251536\n79      Shaoyang        108078\n80      Lianyuan        238300\n81     Hongjiang        108870\n82      Hengyang        108085\n83       Guiyang        262835\n84      Changsha        248182\n85       Taoyuan        244850\n86      Xiangtan        404456\n87           Dao         67608\n88     Jiangyong         33860\n\n\nFrom the above data table and the GDPPC from the previous section, we know that the lagged sum is the addition of all the GDPPC of its neighbours.\n\\[(20981+34592+24473+21311+22879 = 124236)\\]\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(gdppc, lag_gdppc, lag_sum_gdppc, asp=1, ncol=3)\n\n\n\n\n\nGDPPC vs lag GDPPC vs lag sum GDPPC\n\n\n\n\nSpatial Window Average\nThe spatial window average uses row-standardized weights and includes the diagonal element. (region itself) We will use the include.self().\n\nwm_q_self <- include.self(wm_q)\n\nWe will now obtain the weight and retrieve the new spatial window average and combine it with our exisiting Hunan dataframe.\n\nwm_q_self_list <- nb2listw(wm_q_self)\nlag_w_avg_gpdpc <- lag.listw(wm_q_self_list, \n                             hunan$GDPPC)\n\nlag_w_avg_df <- as.data.frame(list(hunan$NAME_3, lag_w_avg_gpdpc))\n\ncolnames(lag_w_avg_df) <- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nhunan <- left_join(hunan, lag_w_avg_df)\n\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_avg_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_window_avg GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(gdppc, lag_gdppc, lag_sum_gdppc, lag_sum_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\nGDPPC vs lag GDPPC vs lag sum GDPPC vs lag avg GDPPC\n\n\n\n\nSpatial Window Sum\nThe spatial Window sum is similar to the window average but using the binary weights. Therefore we will repeat the following steps of the Spatial lag as a sum of neighboring values and to include its own region.\n\nb_weights <- lapply(wm_q_self, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nFrom the result, we can see now the first area instead of 5 neighbours, it has 6 neighbours which include itself. We will now retrieve the spatial window sum and combine it with our exisiting Hunan dataframe.\n\nb_weights2 <- nb2listw(wm_q_self, \n                       glist = b_weights, \n                       style = \"B\")\nw_sum_gdppc_df <- as.data.frame(list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC)))\ncolnames(w_sum_gdppc_df) <- c(\"NAME_3\", \"w_sum GDPPC\")\n\nhunan <- left_join(hunan, w_sum_gdppc_df)\n\nWe will now visualise all the plots we created and visualise the difference in each method (excluding the original GDPPC).\n\nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_avg_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_window_avg GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_window_gdppc <- tm_shape(hunan) +\n  tm_fill(\"w_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(lag_gdppc, lag_sum_gdppc, lag_sum_avg_gdppc, lag_sum_window_gdppc, asp=1, ncol=2)\n\n\n\n\n\nlag GDPPC vs lag sum GDPPC vs lag avg GDPPC vs lag sum avg GDPPC"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#conclusion",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#conclusion",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Conclusion",
    "text": "Conclusion\nThis study allow us to understand the different contiguity spatial weights and different methods to utilise the neighbours information. There is no best method but which method suits your analysis of your work."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "The Spatial Autocorrelation measures spatial autocorrelation based on feature locations and feature values simultaneously. Given a set of features and an associated attribute, it evaluates whether the expressed pattern is clustered, scattered, or random. The tool calculates the Moran’s I index value as well as a z-score and p-value to assess the significance of this index. P-values are numerical approximations of the area under the curve for a known distribution, bounded by the test statistic.\nIn this study we will explore the computation of Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#libraries",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#libraries",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Libraries",
    "text": "Libraries\nFor this study, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\nShow the codepacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#data-preparation",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nHunan.shp: A shapefile of the Hunan Province that consist of all the capital\nHunan.csv: A csv file containing multiple attributes of each capital within Hunan\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment. We will then use a relational join left_join to combine the spatial and aspatial data together.\n\nShow the codehunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan <- left_join(hunan,hunan2012)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#visualisation-of-spatial-data",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#visualisation-of-spatial-data",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualisation of spatial data",
    "text": "Visualisation of spatial data\nFor the visualisation, we will only be using tmap to show the distribution of GDPPC 2021.\n\nShow the codetm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDDPC using Quantile classification\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#global-spatial-autocorrelation",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#global-spatial-autocorrelation",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Global Spatial Autocorrelation",
    "text": "Global Spatial Autocorrelation\nBefore we commence with spatial autocorrelation, we need to construct the spatial weights of the study region. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area. Refer to my previous post to understand the flow of constructing the spatial weight.\nComputing Contiguity Spatial Weights\nFor this study, we will be using the Queen contiguity weight matrix. The code chunk below will construct the weight matrix and subsequently implement the row-standardised weight matrix using the nb2listw() function.\n\nShow the codewm_q <- poly2nb(hunan, \n                queen=TRUE)\n\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nGlobal Spatial Autocorrelation: Moran’s I\nWe will now perform Moran’s I statistics testing by using the moran.test() from spdep.\n\nShow the codemoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nTo better understand the result, we will reference the following table.\n\n\n\n\n\n\n\nThe p-value is not statistically significant.\nYou cannot reject the null hypothesis. It is quite possible that the spatial distribution of feature values is the result of random spatial processes. The observed spatial pattern of feature values could very well be one of many, many possible versions of complete spatial randomness (CSR).\n\n\nThe p-value is statistically significant, and the z-score is positive.\nYou may reject the null hypothesis. The spatial distribution of high values and/or low values in the dataset is more spatially clustered than would be expected if underlying spatial processes were random.\n\n\nThe p-value is statistically significant, and the z-score is not positive.\nYou may reject the null hypothesis. The spatial distribution of high values and low values in the dataset is more spatially dispersed than would be expected if underlying spatial processes were random. A dispersed spatial pattern often reflects some type of competitive process—a feature with a high value repels other features with high values; similarly, a feature with a low value repels other features with low values.\n\n\n\nThe hypothesis:\nH0 : The attribute being analyzed is randomly distributed among the features in your study area.\nH1: The attribute being analyzed is not randomly distributed among the features in your study area.\nSince the above result has a p-value below 0.05 and a positive z-score, we can conclude with statistical evidence that the attribute a not randomly distributed and the spatial distribution of high values and/or low values in the dataset is more spatially clustered\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nShow the codeset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nVisualising Monte Carlo Moran’s I\nWe will use a density plot to visualise the output of the Monte Carlo Moran’s I. First, we need to extract the res value and convert it into a dataframe. We then visualise the test statistic result using geom_density from the ggplot package.\n\nShow the codemonte_carlo <- as.data.frame(bperm[7])\n\nggplot(monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.30075),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() \n\n\n\n\nFrom the plot, we can see the actual Moran’s I statistic (blue line) is far outside the simulated data (shaded in blue), indicating a significant evidence of positive autocorrelation.\nGlobal Spatial Autocorrelation: Geary’s\nWe will now perform the Geary’c statistic testing by using the geary.test() function. We will also compute the Monte Carlo Geary’sc using the geary.mc.\n\nShow the codegeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\nShow the codeset.seed(1234)\ngperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\ngperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nBased on the result, the p-value is below the alpha value of 0.05 and therefore we can statistical evidence to reject the null hypothesis.\nVisualising the Monte Carlo Geary’s C\n\nShow the codemonte_carlo_geary <- as.data.frame(gperm[7])\n\nggplot(monte_carlo_geary, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.69072),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Geary’s C\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() \n\n\n\n\nUnlike the Moran’s I where the statistical value is located on the right side of the density graph, the test statistic for Geary’s C is inversely related to Moran’s I where the value less than 1 indicates positive spatial autocorrelation, while a value larger than 1 points to negative spatial autocorrelation. Therefore, based on the plot, we can conclude there is significant evidence of positive autocorrelation."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#spatial-correlogram",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#spatial-correlogram",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nA nonparametric spatial correlogram is another measure of overall spatial autocorrelation that does not rely on specifying a matrix of spatial weights. Instead, a local regression is fitted to the calculated covariances or correlations for all pairs of observations based on the distance between them. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\nCompute Moran’s I and Geary’C correlogram\nWe will utilise the sp.correlogram() function from the spdep package and compute a 6-lag spatial correlogram of GDPPC. We then use the plot() and print() function to visualise the output. The method function can take in three different inputs:\n\ncorr - correlation\nI - Moran’s I\nC - Geary’s C\n\nWe will illustrate the Moran’s I correlogram.\n\nShow the codeMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\nShow the codeprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we will illustrate the Geary’C correlogram.\n\nShow the codeGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\nShow the codeprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation of results\nFrom the results and plot shown above, we understand the relationship between Moran’s I and Geary’C (inverse). We also can identify lag 1 and 2 with statistical evidence to have a positive autocorrelation and lag 5 and 6 to have a negative correlation with p-values below 0.05."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#cluster-and-outlier-analysis",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#cluster-and-outlier-analysis",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nLocal Indicators of Spatial Association (LISA) is a technique that allows analysts to identify areas on the map where data values are strongly positively or negatively correlated. We will now use techniques to detect clusters and/or outliers.\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nShow the codefips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe z-scores and pseudo p-values represent the statistical significance of the computed index values.\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nShow the codeprintCoefmat(data.frame(localMI[fips,], row.names=hunan$County[fips]), check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\nMapping the local Moran’s I\nWe have to combine the local Moran’s dataframe with the our exisiting Hunan spatialdataframe before plotting. We will use the cbind() function.\n\nShow the codehunan.localMI <- cbind(hunan,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\nMapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\nShow the codetm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran I value\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nA positive value for I indicates that a feature has neighboring features with similarly high or low attribute values; this feature is part of a cluster. A negative value for I indicates that a feature has neighboring features with dissimilar values; this feature is an outlier. The choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nMapping local Moran’s I p-values\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\nShow the codetm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nMapping Moran’s I values and p-values\nIt will be more useful to understand which area Moran’s I value is statistically significant. We will create another dataframe with only areas that are statistically significant with a p value < 0.05. We will then plot the overlay above the base map and identify the area with positive and negative I value.\n\nShow the codehunan.localMI.sub <- hunan.localMI %>%\n  filter (Pr.Ii <= 0.05)\n\nimap <- tm_shape(hunan.localMI) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(hunan.localMI.sub) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\")\n\nimap \n\n\n\n\nFrom our plot, we are able to derive that 11 areas have I values that are statically significant, and 2 areas have negative I value which means a dissimilar features."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#creating-a-lisa-cluster-map",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#creating-a-lisa-cluster-map",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\nPlotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nShow the codenci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\nIn the upper right quadrant, there are cases where both the attribute value and the local average are greater than the global average. Similarly, in the lower left quadrant, there are cases where both the attribute value and the local mean are below the global mean. These conditions confirm positive spatial autocorrelation. Cases in the other two quadrants show negative spatial autocorrelation.\nPreparing LISA map classes\nWe will now prepare the LISA map classes. We first need to retrieve the quadrant for each area.\n\nShow the codequadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\n\nNext, we scale the GDPPC.\n\nShow the codeDV <- scale(hunan.localMI$GDPPC)   \n\n\nThis is follow by finding the lag of the scaled GDPPC.\n\nShow the codeC_mI <- lag.listw(rswm_q, DV)   \n\n\nUsing the Moran Scatterplot below, we filter all the area with p value < 0.05 and identify significant areas. We can see that the plot below is align with our Moran I plot where there are a total of 11 significant areas, 2 areas that are outliers (LH), and 9 areas that are clusters (7 HH and 2 LL).\n\nShow the codeMIplot <- data.frame(cbind(DV,C_mI,localMI[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\nplot(x = MIplot$X1, y = MIplot$X2, main = \"Moran Scatterplot PPOV\", xlab = \"scaled GDDPC\", ylab = \"Lag scaled GDPPC\")\nabline(h = 0, v = 0)\n\n\n\n\nWe will now then define the quadrant based on the following criteria and place non-significant Moran (p value < 0.05) in the category 0.:\n\nShow the codesignif <- 0.05 \nquadrant[DV >0 & C_mI>0] <- 4      \nquadrant[DV <0 & C_mI<0] <- 1      \nquadrant[DV <0 & C_mI>0] <- 2\nquadrant[DV >0 & C_mI<0] <- 3\nquadrant[localMI[,5]>signif] <- 0\n\n\nPlotting LISA map\nOnce the quadrant of each area has been decided, we will now plot the LISA map using tmap. We will plot both the base map with the GDDPC distribution and the LISA map to better understand the relationship.\n\nShow the codetmap_mode(\"plot\")\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map with Quadrant\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")\n\nbasemap <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDDPC using Quantile classification\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")\n\ntmap_arrange (imap,lisamap,basemap)\n\n\n\n\nBased on the map plot above, we can see that the Moran I value map provide us with insights on which areas are consider outliers or clusters. The LISA map provide us with more in-depth information on whether the outliers are HL or LH and whether the clusters are HH or LL. These attributes can further be confirmed by referencing the base map on the right."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#hot-spot-and-cold-spot-area-analysis",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Hot Spot and Cold Spot Area Analysis",
    "text": "Hot Spot and Cold Spot Area Analysis\nBy grouping points of occurrence into polygons or converging points that are close to one another based on a calculated distance, Hotspot Analysis uses vectors to locate statistically significant hot spots and cold spots in your data.\nGetis and Ord’s G-Statistics\nThe Getis and Ord’s G-statistics is used to measure the degree of clustering for either high or low values. The High/Low Clustering (Getis-Ord General G) statistic is an inferential statistic, which means that the null hypothesis is used to interpret the analysis’s findings. It is assumed that there is no spatial clustering of feature values when using the High/Low Clustering (General G) statistic.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\nDeriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of , they are:\n\nfixed distance weight matrix\nadaptive distance weight matrix\n\nThese methods were explained on the previous post and therefore will not be elaborated here.\nThe code chunk below is to generate the fixed distance weight matrix:\n\nShow the codecoords <- st_centroid(st_geometry(hunan))\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nwm_d59 <- dnearneigh(coords, 0, 59)\nwm59_lw <- nb2listw(wm_d59, style = 'W')\n\n\nThe output spatial weights object is called wm59_lw.\nThe code chunk below is to generate the adaptive distance weight matrix:\n\nShow the codeknn <- knn2nb(knearneigh(coords, k=8))\nknn_lw <- nb2listw(knn, style = 'B')"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#computing-gi-statistics",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#computing-gi-statistics",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Gi statistics",
    "text": "Computing Gi statistics\nGi statistics using fixed distance\n\nShow the codefips <- order(hunan$County)\ngi.fixed <- localG(hunan$GDPPC, wm59_lw)\ngi.fixed\n\n [1]  0.43607584 -0.26550565 -0.07303367  0.41301703  0.37236574 -0.37751078\n [7]  2.86389882  2.79435042  5.21612540  0.22823660  0.95103535 -0.53633423\n[13]  0.17676156  1.19556402 -0.03302061  1.24516289 -0.58575676 -0.41968056\n[19]  0.39900582  0.01205611 -0.14571653 -0.02715869 -0.31861529 -0.74894605\n[25] -0.96170058 -0.79685134 -1.03394977 -0.46097916 -0.28044932 -0.26667151\n[31] -0.88616861 -0.85547697 -0.92214318 -1.16232860 -0.14981495  0.20687930\n[37] -0.56988403 -1.25929908 -1.45225651 -1.54067112 -1.39501141 -1.63849079\n[43] -1.31411071 -0.76794446 -0.19288934  3.24270315  1.80919136 -0.85166126\n[49] -0.51198447 -0.83454636 -0.90817907 -1.54108152 -1.00485524 -1.07508016\n[55] -1.63107596 -0.74347225  0.41884239  0.83294375 -0.55849880 -0.44971882\n[61] -0.49323874 -1.08338678  0.04297905 -0.06907285  0.13633747  2.20341174\n[67]  2.69032995  4.45370322  0.17800261 -0.12931859  0.73780663 -1.24691266\n[73]  0.63536412  0.80351189 -0.99800120  1.21584987 -0.48719642  1.62617404\n[79] -1.06041680  0.84902447 -0.64525926 -0.43486100 -0.09446279  4.42439262\n[85] -0.22952462  1.36459800 -0.84344663 -0.71800062\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm59_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nShow the codehunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\nMapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\nShow the codegdppc <- qtm(hunan, \"GDPPC\")\n\nGimap <-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\nBased on the map plot, we can observe that the hotspots (higher GDDPC) are located within the northern eastern region and the coldspot (lower GDDPC) is located at the southern western and northern western region. The area consist of mostly cold spots than hot spots which might signify a generally uneven distribution of wealth within the Hunan province.\nGi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nShow the codefips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\nMapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\nShow the codeGimap <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\nThe map plot differ slightly from the fixed weight GI statistic where we see a more concentrated hotspot at the north east region and only one concentrated cold spot at the south west region. The eastern region are mostly region with higher GDDPC compared to the western region (more hotspot in the East and more cold spot in the West.)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html",
    "href": "posts/Geo/Geospatial_Exercise/index.html",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "",
    "text": "The government of Nigeria deemed the Water, Sanitation, and Hygiene (WASH) sector to be in a state of emergency in 2018. In 2019, 60 million Nigerians were without access to basic drinking water due to a combination of bad infrastructure, a lack of necessary human resources, low investment, and a weak enabling regulatory environment, among other issues. 167 million people lacked access to even the most basic handwashing facilities, and 80 million lacked access to better sanitation facilities.\nOnly half of rural families have access to improved sanitation, and 39% of households in these areas conduct open defecation, a percentage that has barely changed since 1990.\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, I will be applying appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Non-Functional water points.\n\nFor this study, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’.\n\npatchwork - Combine separate ggplots into the same graphic.\n\n\nShow the codepacman::p_load(sf, spdep, tmap, tidyverse,patchwork)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#libraries",
    "href": "posts/Geo/Geospatial_Exercise/index.html#libraries",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Libraries",
    "text": "Libraries\nFor this study, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’.\n\npatchwork - Combine separate ggplots into the same graphic.\n\n\nShow the codepacman::p_load(sf, spdep, tmap, tidyverse,patchwork)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Exercise/index.html#data-preparation",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nNigeria.shp: A shapefile of Nigeria from Humanitarian Data Exchange Portal that consist of all the Level-2 Administrative Boundary (also known as Local Government Area)\nNigeriaAttribute.csv: A csv file containing multiple water point attributes of each Level-2 Administrative Nigeria Boundary from the WPdx Global Data Repositories\n\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment.\n\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Exercise\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\nShow the codenigeria <- st_read(dsn = \"data\", \n                 layer = \"geoBoundaries-NGA-ADM2\")\n\nnigeria_attribute <- read_csv(\"data/nigeriaattribute.csv\")\nnigeria <- nigeria %>%\n  st_transform(crs = 26391)\n\n\nData Wrangling\nThe practice of correcting or deleting inaccurate, damaged, improperly formatted, duplicate, or incomplete data from a dataset is known as data wrangling. There are numerous ways for data to be duplicated or incorrectly categorized when merging multiple data sources. We willl now proceed to ensure our data is cleaned before conducting our analysis.\nChecking of duplicated area name\nFirstly, we will order our dataframe by alphabetical order based on the shapeName. We will then use the duplicated function to retrieve all the shapeName that has duplicates and store it in a list. From the result below, we identified 12 shapeNames that are duplicates.\n\nShow the codenigeria <- (nigeria[order(nigeria$shapeName), ])\n\nduplicate_area <- nigeria$shapeName[ nigeria$shapeName %in% nigeria$shapeName[duplicated(nigeria$shapeName)] ]\n\nduplicate_area\n\n [1] \"Bassa\"    \"Bassa\"    \"Ifelodun\" \"Ifelodun\" \"Irepodun\" \"Irepodun\"\n [7] \"Nasarawa\" \"Nasarawa\" \"Obi\"      \"Obi\"      \"Surulere\" \"Surulere\"\n\n\nNext, we will leverage on the interactive viewer of tmap to check the location of each area. Through the use of Google, we are able to retrieve the actual name and state of the areas. The table below shows the index and the actual name of the area.\n\n\nIndex\nActual Area Name\n\n\n\n94\nBassa (Kogi)\n\n\n95\nBassa (Plateau)\n\n\n304\nIfelodun (Kwara)\n\n\n305\nIfelodun (Osun)\n\n\n355\nIrepodun (Kwara)\n\n\n356\nIrepodun (Osun)\n\n\n518\nNassarawa\n\n\n546\nObi (Benue)\n\n\n547\nObi(Nasarawa)\n\n\n693\nSurulere (lagos)\n\n\n694\nSurulere (Oyo)\n\n\n\n\nShow the codetmap_mode(\"view\")\n\ntm_shape(nigeria[nigeria$shapeName %in% duplicate_area,]) +\n  tm_polygons()\n\n\n\n\n\nShow the codetmap_mode(\"plot\")\n\n\nWe will now access the individual index of the nigeria data frame and change the value. Lastly, we use the length() function to ensure there is no more duplicated shapeName.\n\nShow the codenigeria$shapeName[c(94,95,304,305,355,356,519,546,547,693,694)] <- c(\"Bassa (Kogi)\",\"Bassa (Plateau)\",\n                                                                               \"Ifelodun (Kwara)\",\"Ifelodun (Osun)\",\n                                                                               \"Irepodun (Kwara)\",\"Irepodun (Osun)\",\n                                                                               \"Nassarawa\",\"Obi (Benue)\",\"Obi(Nasarawa)\",\n                                                                               \"Surulere (Lagos)\",\"Surulere (Oyo)\")\n\nlength((nigeria$shapeName[ nigeria$shapeName %in% nigeria$shapeName[duplicated(nigeria$shapeName)] ]))\n\n[1] 0\n\n\nProjection of sf dataframe\nSince our aspatial data was imported to a tibble dataframe, we will need to convert it to an sf object. First, we rename the columns for ease of representation using the rename() function from dyplr. We then only retain the columns required for analysis such as the name of the area name, latitude, longitude and status. We realised there were NA values within the status column, we will replace the NA values with Unknown using the mutate() function.\nWe will then use the st_as_sf() function to convert the dataframe to an sf object. We will have to input the column that specify the longitude and latitude, and lastly, the CRS projection of the coordinates.\n\nShow the codenigeriaT <- nigeria_attribute  %>%\n  rename (\"Country\" = \"#clean_country_name\",\n          \"clean_adm2\" = \"#clean_adm2\",\n          \"status\" = \"#status_clean\",\n          \"lat\" = \"#lat_deg\",\n          \"long\" = \"#lon_deg\") %>%\n  select (clean_adm2,status,lat,long) %>%\n  mutate(status = replace_na(status, \"Unknown\"))\n\nnigeriaT_sf <- st_as_sf(nigeriaT, coords = c(\"long\", \"lat\"),  crs = 4326)\n\n\nWe will now transform the coordinates from 4326 to 26391 projection using the st_transform() function.\n\nShow the codenigeriaT_sf <- st_transform(nigeriaT_sf, crs = 26391)\n\nst_crs (nigeria)\nst_crs (nigeriaT_sf)\n\n\nVisualising of distribution using ggplot\nWe will use the ggplot function to visualise the distribution of the different status. To sort the distribution by descending order fct_infreq will be use.\n\nShow the codeggplot(data= nigeriaT_sf, \n       aes(x= fct_infreq(status))) +\n  geom_bar(aes(fill = status), show.legend = FALSE) +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             2), '%')), vjust= -0.5, size= 2.5) +\n  labs(y= 'No. of\\nOccurence', x= 'Status',\n       title = \"Distribution of Water Tap Status\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'),\n        axis.text.x = element_text(angle = 90, vjust = 0.5))\n\n\n\n\n\n\nggplot of status frequency count\n\n\n\n\n\n\n\n\n\n\nExtracting Status of Water Point\nSince this analysis is on the functionality of the water taps, we have to extract the number of functional and non-functional water taps from the nigeriaT_sf dataframe. The status column reveal the status of the water tap. We will now see what values are recorded by using the unique() function. From the result below, we can identify mainly four categories of statuses; Functional, Non-Functional, Abandoned, Unknown.\nNote: For this analysis, abandoned water taps will be analysed under non-functional.\nFrom the result below, we can identify a pattern to classify the status based on our criteria. By extracting the first word of the sentence before the punctuation, we will be able to extract the word; Unknown, Abandoned, Functional and Non. This will assist us in grouping these status.\n\nShow the codeunique(nigeriaT_sf$status)\n\n[1] \"Unknown\"                          \"Abandoned/Decommissioned\"        \n[3] \"Non-Functional\"                   \"Functional\"                      \n[5] \"Functional but needs repair\"      \"Functional but not in use\"       \n[7] \"Abandoned\"                        \"Non functional due to dry season\"\n[9] \"Non-Functional due to dry season\"\n\n\nTo replace the original values, we will use the gsub() function. A regular expression “([A-Za-z]+).*” is used to extract all letters and \\\\1 is used to back reference the first capturing group. The result below shows the unique values left within the column.\n\nShow the codenigeriaT_sf$status <- gsub(\"([A-Za-z]+).*\", \"\\\\1\", nigeriaT_sf$status)\nunique(nigeriaT_sf$status)\n\n[1] \"Unknown\"    \"Abandoned\"  \"Non\"        \"Functional\"\n\n\nComputing Ratio of Functional and Non Functional Water Point\nInstead of creating another data frame to store the new values, we will leverage on the filter function by using the single square bracket “[]” operator. The coordinates beings with a row position and that will be used for our filtering condition. R Dataframe. Since our water taps are point data, we will use st_intersects() to retrieve every geometry point that intersect with the polygon of the Nigeria ADM area, and subsequently use the function lengths() to retrieve the number of points that intersects with the polygon.\n\nShow the codenigeria$functional <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Functional\",]))\nnigeria$nonfunctional <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Non\",])) + lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Abandoned\",]))\nnigeria$unknown <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Unknown\",]))\nnigeria$total <- lengths(st_intersects(nigeria, nigeriaT_sf))\n\n\nNext, for areas without any water taps, my assumption is that these areas do not need water taps for many possible reasons (lack of habitat, urbanised areas, etc), and therefore will be excluded from the analysis. I use the filter() function to remove areas without any water taps and mutate() function to create two new columns that shows the percentage of functional and non-functional water points over the total water point in the area. (Including unknown water point status)\n\nShow the codenigeria <- nigeria %>%\n  filter (total != 0) %>%\n  mutate (pct_functional = case_when(\n    functional == 0 ~ 0,\n    TRUE ~ (functional/total) * 100\n  )) %>%\n  mutate (pct_nonfunctional = case_when(\n    nonfunctional == 0 ~ 0,\n    TRUE ~ (nonfunctional/total) * 100\n  ))"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#spatial-correlogram",
    "href": "posts/Geo/Geospatial_Exercise/index.html#spatial-correlogram",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nA nonparametric spatial correlogram is another measure of overall spatial autocorrelation that does not rely on specifying a matrix of spatial weights. Instead, a local regression is fitted to the calculated covariances or correlations for all pairs of observations based on the distance between them. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips <- order(nigeria$shapeName)\nlocalMI_dw <- localmoran(nigeria$nonfunctional, rswm_dw)\nlocalMI_adp <- localmoran(nigeria$nonfunctional, bwm_apd)\n\nhead(localMI_dw)\n\n            Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.361394136 -9.995243e-04 1.128237e-02  3.4117747   0.0006454144\n2  0.074414950 -4.092463e-05 4.705097e-04  3.4325327   0.0005979717\n3  1.258199847 -1.627684e-03 6.280738e-01  1.5896655   0.1119102304\n4 -0.006652507 -5.427505e-05 4.151689e-03 -0.1024036   0.9184363392\n5  0.082615173 -2.590965e-04 3.325093e-03  1.4372021   0.1506605779\n6  0.006672593 -1.538445e-07 5.523369e-06  2.8392431   0.0045220690\n\nhead(localMI_adp)\n\n             Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.0039915738 -7.478709e-05 7.815010e-04 -0.1401087    0.888574092\n2  0.0017815926 -3.170287e-06 3.404249e-05  0.3058932    0.759685936\n3  0.0109484618 -1.416360e-05 1.297391e-05  3.0435412    0.002338114\n4 -0.0010787454 -1.239643e-06 3.442702e-06 -0.5807242    0.561426373\n5  0.0034790130 -1.055977e-05 4.200930e-05  0.5383932    0.590305635\n6  0.0001767241 -4.123706e-09 1.126535e-08  1.6650737    0.095898060\n\n\nMapping the local Moran’s I\nWe have to combine the local Moran’s dataframe with the our exisiting Nigeria spatialdataframe before plotting. We will use the cbind() function.\n\nnigeria_localMI_dw <- cbind(nigeria,localMI_dw) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nnigeria_localMI_adp <- cbind(nigeria,localMI_adp) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nMapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\nnigeria_localMI_dw_sub <- nigeria_localMI_dw %>%\n  filter (Pr.Ii <= 0.05)\n\ntm_shape(nigeria_localMI_dw) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_dw_sub) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\")\n\n\n\n\n\nnigeria_localMI_adp_sub <- nigeria_localMI_adp %>%\n  filter (Pr.Ii <= 0.05)\n\ntm_shape(nigeria_localMI_adp) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_adp_sub) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#creating-a-lisa-cluster-map",
    "href": "posts/Geo/Geospatial_Exercise/index.html#creating-a-lisa-cluster-map",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\nPreparing LISA map classes\nWe will now prepare the LISA map classes. We first need to retrieve the quadrant for each area.\n\nShow the codequadrant <- vector(mode=\"numeric\",length=nrow(localMI_dw))\n\n\nWe will now then define the quadrant based on the following criteria and place non-significant Moran (p value < 0.05) in the category 0:\nNote: We will use the scaled variable and the lag scaled variable used earlier on for the Moran Scatterplot.\n\nShow the codesignif <- 0.05 \nquadrant[DV >0 & C_mI>0] <- 4      \nquadrant[DV <0 & C_mI<0] <- 1      \nquadrant[DV <0 & C_mI>0] <- 2\nquadrant[DV >0 & C_mI<0] <- 3\nquadrant[localMI_dw[,5]>signif] <- 0\nnigeria_localMI_dw$quadrant <- quadrant\n\nquadrant[DV_Fun >0 & C_mI>0] <- 4      \nquadrant[DV_Fun <0 & C_mI<0] <- 1      \nquadrant[DV_Fun <0 & C_mI>0] <- 2\nquadrant[DV_Fun >0 & C_mI<0] <- 3\nquadrant[localMI_dw[,5]>signif] <- 0\nnigeria_localMI_dw$quadrantfun <- quadrant\n\nquadrant[DV >0 & C_mI_adp>0] <- 4      \nquadrant[DV <0 & C_mI_adp<0] <- 1      \nquadrant[DV <0 & C_mI_adp>0] <- 2\nquadrant[DV >0 & C_mI_adp<0] <- 3\nquadrant[localMI_adp[,5]>signif] <- 0\nnigeria_localMI_adp$quadrant <- quadrant\n\nquadrant[DV_Fun >0 & C_mI_adp_Fun>0] <- 4      \nquadrant[DV_Fun <0 & C_mI_adp_Fun<0] <- 1      \nquadrant[DV_Fun <0 & C_mI_adp_Fun>0] <- 2\nquadrant[DV_Fun >0 & C_mI_adp_Fun<0] <- 3\nquadrant[localMI_adp[,5]>signif] <- 0\nnigeria_localMI_adp$quadrantfun <- quadrant\n\n\nPlotting LISA map\nOnce the quadrant of each area has been decided, we will now plot the LISA map using tmap.\n\nShow the codetmap_mode(\"plot\")\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap_fd <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Non-Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\nlisamap_fd_fun <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(col = \"quadrantfun\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          title = \"quadrant\") +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\n\n\n\n\n\nLISA map of Fixed Distance (Non-Functional & Functional)\n\n\n\n\n\nShow the codetmap_mode(\"plot\")\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap_ad <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Non Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nlisamap_ad_fun <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(col = \"quadrantfun\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          title = \"quadrant\") +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nfixedD_Lisa <- tmap_arrange (lisamap_fd,lisamap_fd_fun, ncol = 2, widths = 10)\nadaptiveD_Lisa <- tmap_arrange (lisamap_ad,lisamap_ad_fun, ncol = 2, widths = 10)\n\n\n\n\n\n\nLISA map of Adaptive Distance (Non-Functional & Functional)\n\n\n\n\nAnalysis of LISA map result\nFor both distance weight methods, we can see that the South West region shows a H-H for non-functional water point percentage and a few regions shows a L-L and L-H for functional water point percentage. These regions should be prioritize for repair since they have their regions and neighbouring regions most likely have a lot of non-functional water point and also very little functional water point.\nWhat we can see from the map is also the North East region seems to have good access to water point with regions showing H-H for functional water point percentage and a L-L or L-H for non-functional water point percentage."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#conclusion",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#conclusion",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Conclusion",
    "text": "Conclusion\nGeospatial autocorrelation is important for us to draw statistical conclusion on whether areas are correlated with one another based on various attributes and of course, the connectivity of the land with others."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#lisa-cluster-map",
    "href": "posts/Geo/Geospatial_Exercise/index.html#lisa-cluster-map",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "LISA Cluster Map",
    "text": "LISA Cluster Map\nIntroduction\nThe Spatial Autocorrelation measures spatial autocorrelation based on feature locations and feature values simultaneously. Given a set of features and an associated attribute, it evaluates whether the expressed pattern is clustered, scattered, or random. The tool calculates the Moran’s I index value as well as a z-score and p-value to assess the significance of this index. P-values are numerical approximations of the area under the curve for a known distribution, bounded by the test statistic.\nIn this study we will explore the computation of Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package.\nFor this study, we will be using the distance based weight matrix. There are two type of distance-based proximity matrix, they are:\n\nFixed Distance Weight Matrix\nAdaptive Distance Weight Matrix\n\nSince the study is regarding the prioritisation of water tap repair and to identify areas which have restriction in water supply access, neighbouring regions that are nearer to the selected region should have greater weights compared to neighbouring regions that are further away. Therefore, with this concept in mind, we will employ the Inverse distance weighting to take distance decay into consideration.\nDeriving distance-based and adaptive weight matrix\nThe first step is to retrieve the centroid for each area. To retrieve the centroid of each area, we will use the st_centroid() function. The st_centroid() function will calculates the geometric center of a spatial object.\n\nShow the codecoords <- st_centroid(st_geometry(nigeria))\ncoords[1]\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 549364 ymin: 123694.9 xmax: 549364 ymax: 123694.9\nProjected CRS: Minna / Nigeria West Belt\n\n\nDetermine the cut-off distance for fixed distance weight matrix\nSecondly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nShow the codek1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2669   12808   20008   21804   27013   72139 \n\nShow the codethreshold <- max(unlist(nbdists(k1, coords)))\n\n\nThe summary report shows that the largest first nearest neighbour distance is 72.139 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour. We then save the max value as the threshold for the subsequent function.\nComputing Fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below. The function identifies neighbours of region points by Euclidean distance in the metric of the points between lower (greater than or equal to and upper (less than or equal to) bounds.\n\nShow the codewm_d73 <- dnearneigh(coords, 0, threshold)\nwm_d73\n\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 18022 \nPercentage nonzero weights: 3.111958 \nAverage number of links: 23.682 \n\n\nFrom the result above, we can identify an average of 23 neighbours per region using the distance based weight matrix.\nComputing Inverse Adaptive distance weight matrix\nNext, we will compute the inverse adaptive distance weight matrix. The knearneigh() function uses spartial indexing to identify the nearest neighbour. For this analysis, we will set to number of neighbours to 8.\n\nShow the codek8 <- knn2nb(knearneigh(coords, k = 8))\nk8\n\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 6088 \nPercentage nonzero weights: 1.051248 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nComputing Inverse Distance Weights\nTo compute the inverse distance, we need a function that applies \\(\\frac{1}{x}\\) to the entire distance data structure. We use lapply to achieve this. The required parameters are the distance and the function specified by lapply. Use the functional operator and \\(\\frac{1}{x}\\) to get the corresponding function. The important thing to note is that the distance units in the dataset are meters. This means that the distance values between points can be very large, resulting in small reciprocals. To fix this scale dependency, rescale the distances by doing a \\(\\frac{x}{1000}\\)in the function before computing the reciprocal.\nComputing Inverse Distance Weights for Fixed Distance\n\nShow the codedistances <- nbdists(wm_d73,coords)\ndistances[1]\n\n[[1]]\n [1]  4004.008 45439.251 37710.356 51041.840 67445.464 65694.575 70278.881\n [8] 66402.395 70754.787 63293.565 44070.130 67132.783 30584.808 31191.772\n[15] 31060.775 61914.751 40430.849 50524.637 68364.757 57815.248 58326.473\n[22] 21477.914 55573.697 46904.765 62426.753 37887.549 52063.146 40086.745\n[29] 52077.350 19783.847 31237.020 65772.348 68438.043 48339.227 66025.875\n[36] 63788.895 32358.664 62335.064 71500.748 11065.063 48564.675 29667.531\n[43] 49875.572 55850.546 70099.782 57661.903 16123.664 69453.919 46496.314\n[50]  9313.577 56305.742 48780.437 56595.108 31208.261 53734.648 40998.087\n[57] 10676.236 35065.479 20859.370 23059.238 53898.687 43026.271 61625.223\n\nShow the codedistances <- lapply(distances, function(x) (1/(x/1000)))\ndistances[1]\n\n[[1]]\n [1] 0.24974975 0.02200741 0.02651791 0.01959177 0.01482679 0.01522196\n [7] 0.01422903 0.01505970 0.01413332 0.01579939 0.02269111 0.01489585\n[13] 0.03269597 0.03205974 0.03219495 0.01615124 0.02473359 0.01979232\n[19] 0.01462742 0.01729648 0.01714487 0.04655946 0.01799412 0.02131980\n[25] 0.01601877 0.02639390 0.01920744 0.02494590 0.01920221 0.05054629\n[31] 0.03201330 0.01520396 0.01461176 0.02068713 0.01514558 0.01567671\n[37] 0.03090362 0.01604234 0.01398587 0.09037454 0.02059110 0.03370688\n[43] 0.02004990 0.01790493 0.01426538 0.01734247 0.06202064 0.01439804\n[49] 0.02150708 0.10737014 0.01776018 0.02050002 0.01766937 0.03204280\n[55] 0.01860997 0.02439138 0.09366597 0.02851808 0.04794009 0.04336657\n[61] 0.01855333 0.02324161 0.01622712\n\n\nComputing Inverse Distance Weights for Adaptive Distance\n\nShow the codek.distances <- nbdists(k8, coords)\nk.distances[1]\n\n[[1]]\n[1]  4004.008 21477.914 19783.847 11065.063 16123.664  9313.577 10676.236\n[8] 20859.370\n\nShow the codeinvdistance <- lapply(k.distances, function(x) (1/(x/1000)))\ninvdistance[1]\n\n[[1]]\n[1] 0.24974975 0.04655946 0.05054629 0.09037454 0.06202064 0.10737014 0.09366597\n[8] 0.04794009\n\n\nVisualising Weight Matrices\nUsing the base R plot function, we will now plot to visualise the areas with their respective neighbours after assignment based on the various methods. The left graph with the red lines show the adaptive distance with 8 neighbours and the right graph with the black lines show the links of neighbours within the cut-off distance of the above threshold.\n\nShow the codepar(mfrow=c(1,2))\nplot(nigeria$geometry, border=\"lightgrey\", main=\"Adaptive Distance (8)\")\nplot(k8, coords, add=TRUE, col=\"red\", length=0.08)\nplot(nigeria$geometry, border=\"lightgrey\", main=\"Fixed Distance\")\nplot(wm_d73, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\nConstructing Distance Binary Weight Matrix\nNow we will have to construct the distance weight matrix using the neighbours and inverse distance that were computed above. Since we have the inverse distance as a variable, we will use the Binary Weight Matrix and input the inverse distance as the weight. The nb2listw() function takes in a list input glist to identify the weight for each neighbour.\nThe code chunk below will construct the fixed distance weight matrix and subsequently implement the binary weight matrix using the nb2listw() function.\n\nShow the codebwm_fd <- nb2listw(wm_d73,\n                    glist = distances,\n                   style=\"B\", \n                   zero.policy = TRUE)\nsummary (bwm_fd)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 18022 \nPercentage nonzero weights: 3.111958 \nAverage number of links: 23.682 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 6 11 10 17 30 31 31 33 29 36 26 18 21 23 16 12 11 11 17 16 16 10 14 10  9  9 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n12  6 15 13 11  9  8  8 12  6 12 17 14  9  9  4  5  3  8  5 11  8  5  5  4  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 67 68 70 \n 3  3  6  3  5  6  2  5  5  8  6  6  4  3  1  1 \n6 least connected regions:\n88 110 121 235 657 753 with 1 link\n1 most connected region:\n572 with 70 links\n\nWeights style: B \nWeights constants summary:\n    n     nn       S0       S1       S2\nB 761 579121 486.0122 43.00868 2155.714\n\n\nThe code chunk below will construct the adaptive distance weight matrix and subsequently implement the Binary weight matrix using the nb2listw() function.\n\nShow the codebwm_apd <- nb2listw(k8,\n                    glist = invdistance,\n                    style = \"B\",\n                    zero.policy = TRUE)\nsummary (bwm_apd)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 6088 \nPercentage nonzero weights: 1.051248 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n  8 \n761 \n761 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 with 8 links\n761 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 with 8 links\n\nWeights style: B \nWeights constants summary:\n    n     nn       S0       S1       S2\nB 761 579121 237.6033 30.43191 453.5401\n\n\nGlobal Spatial Autocorrelation: Moran’s I\nTo identify whether the area analysed is clustered, dispersed or random, we will have to perform the spatial autocorrelation Moran’s I test.\nComputing Global Moran’s I\nWe will now perform Moran’s I statistics testing by using the moran.test() from spdep on both functional and non functional water point percentage. The global Moran’s I test will also be performed on both the fixed distance and adaptive distance weight matrices.\n\nShow the codemoran.test(nigeria$pct_nonfunctional, \n           listw=bwm_fd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_nonfunctional  \nweights: bwm_fd    \n\nMoran I statistic standard deviate = 32.052, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.4210179196     -0.0013157895      0.0001736248 \n\n\n\nShow the codemoran.test(nigeria$pct_functional, \n           listw=bwm_fd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_functional  \nweights: bwm_fd    \n\nMoran I statistic standard deviate = 42.929, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5644376118     -0.0013157895      0.0001736823 \n\n\n\nShow the codemoran.test(nigeria$pct_nonfunctional, \n           listw=bwm_apd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_nonfunctional  \nweights: bwm_apd    \n\nMoran I statistic standard deviate = 22.233, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5115959890     -0.0013157895      0.0005322145 \n\n\n\nShow the codemoran.test(nigeria$pct_functional, \n           listw=bwm_apd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_functional  \nweights: bwm_apd    \n\nMoran I statistic standard deviate = 24.492, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5638015977     -0.0013157895      0.0005323946 \n\n\nInterpretation of results\nBased on all the results above, the p value is below the alpha value of 0.05, therefore we have enough statistical evidence to reject the null hypothesis that the attribute is randomly distributed and a positive Moran I value indicate more spatially clustered than would be expected if underlying spatial processes were random.\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nShow the codeset.seed(1234)\nfd_MC = moran.mc(nigeria$pct_nonfunctional, \n                listw=bwm_fd, \n                nsim=999, \n                zero.policy = TRUE,\n                na.action=na.omit)\nfd_MC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_nonfunctional \nweights: bwm_fd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.42102, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nfd_MC_fun = moran.mc(nigeria$pct_functional, \n                listw=bwm_fd, \n                nsim=999, \n                zero.policy = TRUE,\n                na.action=na.omit)\nfd_MC_fun\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_functional \nweights: bwm_fd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.56444, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nadp_MC= moran.mc(nigeria$pct_nonfunctional, \n                listw=bwm_apd, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nadp_MC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_nonfunctional \nweights: bwm_apd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.5116, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nadp_MC_fun = moran.mc(nigeria$pct_functional, \n                listw=bwm_apd, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nadp_MC_fun\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_functional \nweights: bwm_apd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.5638, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nVisualising Monte Carlo Moran’s I\nWe will use a density plot to visualise the output of the Monte Carlo Moran’s I. First, we need to extract the res value and convert it into a dataframe. We then visualise the test statistic result using geom_density() from the ggplot package. geom_vline is use to represent the actual Moran I value.\nThe code chunk below is to extract the res value and convert it to a dataframe format using as.data.frame() function. The code chunk will generate the density plot using ggplot.\n\nShow the code# Fixed Distance Monte Carlo\n\nfd_monte_carlo <- as.data.frame(fd_MC[7])\n\nfd_mc <- ggplot(fd_monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.42102),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8))\n\nfd_monte_carlo_fun <- as.data.frame(fd_MC_fun[7])\n\nfd_mc_fun <- ggplot(fd_monte_carlo_fun, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.56444),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8)) \n\n# Adaptive Distance Monte Carlo\n\nadp_monte_carlo <- as.data.frame(adp_MC[7])\n\nad_mc <- ggplot(adp_monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.5116),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8)) \n\nadp_monte_carlo_fun <- as.data.frame(adp_MC_fun[7])\n\nad_mc_fun <- ggplot(adp_monte_carlo_fun, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.5638),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the code(fd_mc + fd_mc_fun) / (ad_mc + ad_mc_fun) \n\n\n\n\nInterpretation of Monte Carlo Results\nBased on the above plots and results of the Monte Carlos Moran I test, we can conclude that all the results are statistically significant with a p value < 0.05 and all the Moran I values falls way to the right of the distribution suggesting that the functional and non functional water points are clustered (a positive Moran’s I value suggests clustering)."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#introduction-1",
    "href": "posts/Geo/Geospatial_Exercise/index.html#introduction-1",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#mapping-of-functional-and-non-functional-water-point",
    "href": "posts/Geo/Geospatial_Exercise/index.html#mapping-of-functional-and-non-functional-water-point",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Mapping of Functional and Non Functional Water Point",
    "text": "Mapping of Functional and Non Functional Water Point\nJenks Choropleth Map\nWe will now plot the choropleth map using tmap and jenks classification.\n\nShow the codetmap_mode (\"plot\")\nfun <- tm_shape (nigeria) +\n  tm_fill(\"pct_functional\",\n          style = \"jenks\",\n          n=6,\n          title = \"Functional (%)\") +\n  tm_layout(main.title = \"Distribution of Functional Water Tap (%) by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nnfun <- tm_shape (nigeria) +\n  tm_fill(\"pct_nonfunctional\",\n          style = \"jenks\",\n          n=6,\n          title = \"Non-Functional (%)\") +\n  tm_layout(main.title = \"Distribution of Non Functional Water Tap (%) by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange (fun, nfun, ncol = 2, asp = 1)\n\n\n\n\n\n\ntmap of water tap % distribution (Functional & Non-Functional\n\n\n\n\nObservations from Jenks Choropleth Map\nBy looking at the two chloropleth map, we can make 2 inference: - The northen region have relatively higher percentage of functional water point compared to the southern region. - The southern region also have relatively higher percentage of non-functional water point.\nNow, answering the business question, should we focus our resources in repairing the water tap located at the southern region? Are there any more regions that require our attention? We will now proceed to conduct geospatial analysis to justify or rebut the claims made based on the choropleth map."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#cluster-and-outlier-analysis",
    "href": "posts/Geo/Geospatial_Exercise/index.html#cluster-and-outlier-analysis",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nThe Cluster and Outlier Analysis tool locates spatial clusters of features with high or low values given a set of features (Input Feature Class) and an analysis field (Input Field). Additionally, the program finds spatial outliers. For each statistically significant feature, the tool determines a local Moran’s I value, a z-score, a pseudo p-value, and a code designating the cluster type. The statistical significance of the obtained index values is represented by the z-scores and pseudo p-values.\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nShow the codefips <- order(nigeria$shapeName)\nlocalMI_dw <- localmoran(nigeria$nonfunctional, bwm_fd)\nlocalMI_dw_fun <- localmoran(nigeria$functional, bwm_fd)\nlocalMI_adp <- localmoran(nigeria$nonfunctional, bwm_apd)\nlocalMI_adp_fun <- localmoran(nigeria$functional, bwm_apd)\n\n\nhead(localMI_dw,3)\n\n           Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.45028841 -2.015620e-03 0.0998886136  1.4311082     0.15239922\n2  0.13674255 -9.714502e-05 0.0050358216  1.9283104     0.05381653\n3 -0.00773083 -1.704297e-05 0.0004332892 -0.3705772     0.71095246\n\nShow the codehead(localMI_dw_fun,3)\n\n            Ii          E.Ii      Var.Ii        Z.Ii Pr(z != E(Ii))\n1  0.848290175 -0.0014284243 0.070810822  3.19319400   0.0014070838\n2  0.577851238 -0.0005902825 0.030591200  3.30720567   0.0009423168\n3 -0.002253816 -0.0001063072 0.002701745 -0.04131549   0.9670443851\n\nShow the codehead(localMI_adp,3)\n\n           Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.02804274 -7.980020e-04 0.0819743028 -0.09515767      0.9241896\n2  0.02276868 -3.975715e-05 0.0041969420  0.35206997      0.7247858\n3 -0.01086825 -1.515025e-05 0.0004136275 -0.53364083      0.5935901\n\nShow the codehead(localMI_adp_fun,3)\n\n            Ii          E.Ii      Var.Ii        Z.Ii Pr(z != E(Ii))\n1  0.333118954 -5.655260e-04 0.058111406  1.38422062      0.1662909\n2  0.252550827 -2.415764e-04 0.025495242  1.58319514      0.1133770\n3 -0.005107121 -9.450119e-05 0.002579146 -0.09870221      0.9213747\n\n\nMapping the local Moran’s I\nWe have to combine the local Moran’s dataframe with the our exisiting Nigeria spatialdataframe before plotting. We will use the cbind() function.\n\nShow the codenigeria_localMI_dw <- cbind(nigeria,localMI_dw) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nnigeria_localMI_dw <- cbind(nigeria_localMI_dw,localMI_dw_fun) %>%\n  rename(Pr.Ii.fun = Pr.z....E.Ii..)\n\nnigeria_localMI_adp <- cbind(nigeria,localMI_adp) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nnigeria_localMI_adp <- cbind(nigeria_localMI_adp,localMI_adp_fun) %>%\n  rename(Pr.Ii.fun = Pr.z....E.Ii..)\n\n\nVisualisation of Moran Scatterplot\nWe will now visualise the moran values using ggplot.\nUsing the Moran Scatterplot below, we filter all the area with p value < 0.05 and identify significant areas.\nWe will first have to calculate the scaled attribute and the lagged scaled attribute using the scale() function and lag.listw() function.\nWe scale the the percentage for both functional and non functional water points.\n\nShow the codeDV <- scale(nigeria_localMI_dw$pct_nonfunctional)\nDV_Fun <- scale(nigeria_localMI_dw$pct_functional)   \n\n\nThis is follow by finding the lag of the scaled percentage of functional and non-functional water point.\n\nShow the codeC_mI <- lag.listw(bwm_fd, DV)\nC_mI_Fun <- lag.listw(bwm_fd, DV_Fun)\nC_mI_adp <- lag.listw(bwm_apd, DV)\nC_mI_adp_Fun <- lag.listw(bwm_apd, DV_Fun)\n\n\nOnce we are done with the computation, we will plot the scatterplot using the variables above.\n\nShow the codeMIplot <- data.frame(cbind(DV,C_mI,localMI_dw[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\n\nfd_MI_Plot <- ggplot(MIplot, aes(x = X1, y = X2)) +\n  geom_point() +\n  ylim(-2,1) +\n  xlim (-2,3) +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  geom_vline(xintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  labs(title = \"Moran Scatterplot Fixed Distance (Non-Functional)\", x = \"scaled Non-Functional (%)\", y = \"Lag scaled Non-Functional (%)\") +\n  theme_classic() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the codeMIplot <- data.frame(cbind(DV_Fun,C_mI,localMI_dw[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\n\nfd_MI_Plot_fun <- ggplot(MIplot, aes(x = X1, y = X2)) +\n  geom_point() +\n  ylim(-2,1) +\n  xlim (-2,3) +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  geom_vline(xintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  labs(title = \"Moran Scatterplot Fixed Distance (Functional)\", x = \"scaled Functional (%)\", y = \"Lag scaled Functional (%)\") +\n  theme_classic() + \n  theme(plot.title = element_text(size=8))\n\n\n\nShow the codeMIplot <- data.frame(cbind(DV,C_mI_adp,localMI_adp[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\n\nad_MI_Plot <- ggplot(MIplot, aes(x = X1, y = X2)) +\n  geom_point() +\n  ylim(-2,1) +\n  xlim (-2,3) +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  geom_vline(xintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  labs(title = \"Moran Scatterplot Adapative Distance (Non-Functional)\", x = \"scaled Non-Functional (%)\", y = \"Lag scaled Non-Functional (%)\") +\n  theme_classic() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the codeMIplot <- data.frame(cbind(DV_Fun,C_mI_adp_Fun,localMI_adp_fun[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\n\nad_MI_Plot_fun <- ggplot(MIplot, aes(x = X1, y = X2)) +\n  geom_point() +\n  ylim(-2,1) +\n  xlim (-2,3) +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  geom_vline(xintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  labs(title = \"Moran Scatterplot Adapative Distance (Functional)\", x = \"scaled Functional (%)\", y = \"Lag scaled Functional (%)\") +\n  theme_classic() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the code(fd_MI_Plot + fd_MI_Plot_fun)/(ad_MI_Plot + ad_MI_Plot_fun)\n\n\n\n\nDue to the number of areas that are significant, it is hard to draw any statistical conclusion or inference from the scatterplot. Therefore, we will now visual the Moran I results on a chloropleth map instead.\nMapping local Moran’s I values\nTo better understand which area are outliers/clusters, we will visualise the Moran I values of each area using tmap. Firstly, we will filter all the areas that are not statistically significant (p value >= 0.05). We will then plot the base nigeria map and overlay with the filtered map.\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chunk below.\n\nShow the codefd_moran <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_dw[nigeria_localMI_dw$Pr.Ii < 0.05,]) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05 (Non Functional)\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\nfd_moran_fun <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_dw[nigeria_localMI_dw$Pr.Ii.fun < 0.05,]) +\n  tm_fill (col = \"Ii.1\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05 (Functional)\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\ntmap_arrange (fd_moran, fd_moran_fun, asp = 1, ncol = 2)\n\n\n\n\n\n\ntmap of Fixed Distance Moran I (Functional & Non-Functional)\n\n\n\n\n\nShow the codeadp_moran <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_adp[nigeria_localMI_adp$Pr.Ii < 0.05,]) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05 (Non Functional)\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nadp_moran_fun <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_adp[nigeria_localMI_adp$Pr.Ii.fun < 0.05,]) +\n  tm_fill (col = \"Ii.1\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05 (Functional)\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\ntmap_arrange (adp_moran, adp_moran_fun, asp = 1, ncol = 2)\n\n\n\n\n\n\ntmap of Adaptive Distance Moran I (Functional & Non-Functional)\n\n\n\n\nInterpretation of Local Moran I Map\nThe local moran I mapping shows the area that are classified as clusters and outliers. From the Local Moran I plot, we can see that the clusters for Non-Functional water point are situation at the North-East region and Southern region. The functional water point are clustered at the northern region.\nBut the limitation of the Moran I mapping is that there is no information on whether the value of the area is high or low percentage. We then have to use the Local Indicators of Spatial Association (LISA) map to show the different quadrant of the area based on their Moran I classification. (Outliers or Cluster)\nCreating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\nPreparing LISA map classes\nWe will now prepare the LISA map classes. We first need to retrieve the quadrant for each area.\n\nShow the codequadrant <- vector(mode=\"numeric\",length=nrow(localMI_dw))\n\n\nWe will now then define the quadrant based on the following criteria and place non-significant Moran (p value < 0.05) in the category 0:\nNote: We will use the scaled variable and the lag scaled variable used earlier on for the Moran Scatterplot.\n\nShow the codesignif <- 0.05 \nquadrant[DV >0 & C_mI>0] <- 4      \nquadrant[DV <0 & C_mI<0] <- 1      \nquadrant[DV <0 & C_mI>0] <- 2\nquadrant[DV >0 & C_mI<0] <- 3\nquadrant[localMI_dw[,5]>signif] <- 0\nnigeria_localMI_dw$quadrant <- quadrant\n\nquadrant[DV_Fun >0 & C_mI>0] <- 4      \nquadrant[DV_Fun <0 & C_mI<0] <- 1      \nquadrant[DV_Fun <0 & C_mI>0] <- 2\nquadrant[DV_Fun >0 & C_mI<0] <- 3\nquadrant[localMI_dw[,5]>signif] <- 0\nnigeria_localMI_dw$quadrantfun <- quadrant\n\nquadrant[DV >0 & C_mI_adp>0] <- 4      \nquadrant[DV <0 & C_mI_adp<0] <- 1      \nquadrant[DV <0 & C_mI_adp>0] <- 2\nquadrant[DV >0 & C_mI_adp<0] <- 3\nquadrant[localMI_adp[,5]>signif] <- 0\nnigeria_localMI_adp$quadrant <- quadrant\n\nquadrant[DV_Fun >0 & C_mI_adp_Fun>0] <- 4      \nquadrant[DV_Fun <0 & C_mI_adp_Fun<0] <- 1      \nquadrant[DV_Fun <0 & C_mI_adp_Fun>0] <- 2\nquadrant[DV_Fun >0 & C_mI_adp_Fun<0] <- 3\nquadrant[localMI_adp[,5]>signif] <- 0\nnigeria_localMI_adp$quadrantfun <- quadrant\n\n\n\nShow the codeh <- nigeria_localMI_dw[nigeria_localMI_dw$quadrant != 0,]\n\n\nPlotting LISA map\nOnce the quadrant of each area has been decided, we will now plot the LISA map using tmap.\n\nShow the codetmap_mode(\"plot\")\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap_fd <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Non-Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\nlisamap_fd_fun <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(col = \"quadrantfun\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          title = \"quadrant\") +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\n\n\n\n\n\nLISA map of Fixed Distance (Non-Functional & Functional)\n\n\n\n\n\nShow the codetmap_mode(\"plot\")\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap_ad <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Non Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nlisamap_ad_fun <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(col = \"quadrantfun\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          title = \"quadrant\") +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nfixedD_Lisa <- tmap_arrange (lisamap_fd,lisamap_fd_fun, ncol = 2, widths = 10)\nadaptiveD_Lisa <- tmap_arrange (lisamap_ad,lisamap_ad_fun, ncol = 2, widths = 10)\n\n\n\n\n\n\nLISA map of Adaptive Distance (Non-Functional & Functional)\n\n\n\n\nAnalysis of LISA map result\nFor both distance weight methods, we can see that the South West region shows a H-H for non-functional water point percentage and a few regions shows a L-L and L-H for functional water point percentage. These regions should be prioritize for repair since they have their regions and neighbouring regions most likely have a lot of non-functional water point and also very little functional water point.\nWhat we can see from the map is also the North East region seems to have good access to water point with regions showing H-H for functional water point percentage and a L-L or L-H for non-functional water point percentage.\nThe green marked circle shows the area of least concern while the blue marked circle shows the area of concern/focused that the organisation should pay attention to."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#future-work",
    "href": "posts/Geo/Geospatial_Exercise/index.html#future-work",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Future Work",
    "text": "Future Work\nTo enhance the quality of research and analysis, I proposed to enhance the result of this analysis by conducting these research:\n\nPopulation - Instead of purely looking at the proportion of functional and non-functional water point, population of area can be considered to understand the needs of the region based on it’s population size. Tap per population must be considered to provide a better understanding of what the region is lacking and which region should be an area of focus to increase water access.\nk-nearest neighbour - Due to the short timeframe I have to conduct this analysis, I proposed to have more samples for different neighbours and compare the results. This will better justify the regions that are indeed an area of concern."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#hotspot-and-coldspot-analysis",
    "href": "posts/Geo/Geospatial_Exercise/index.html#hotspot-and-coldspot-analysis",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Hotspot and Coldspot Analysis",
    "text": "Hotspot and Coldspot Analysis\nGetis-Ord Gi* (G-I-star), another name for hotspot analysis, operates by examining each feature in the dataset in the context of nearby features in the same dataset. Despite having a high value, a feature can not be a statistically significant hotspot. A feature with a high value must be surrounded by other features with high values in order to constitute a significant hotspot.\nGenerating Z score for Gertis-Ord Gi\nTo generate the Z score for each region, we will use the localG() function that takes in the attribute and the neighbour list. Since the output of the localG() function is in a list, we will convert it to a data frame by using the as.matrix() function. We will do the following steps on both distance weight methods and for both functional and non functional percentage attribute.\n\nShow the codegi.adaptive <- as.matrix(localG(nigeria$pct_nonfunctional, bwm_apd))\ngi.adaptive_fun <- as.matrix(localG(nigeria$pct_functional, bwm_apd))\nnigeria.gi <- cbind(nigeria, gi.adaptive) %>%\n  rename(gstat_adaptive = gi.adaptive) \nnigeria.gi <- cbind(nigeria.gi, gi.adaptive_fun) %>%\n  rename(gstat_adaptive_fun = gi.adaptive_fun) \n\ngi.fixed <- as.matrix(localG(nigeria$pct_nonfunctional, bwm_fd))\ngi.fixed_fun <- as.matrix(localG(nigeria$pct_functional, bwm_fd))\nnigeria.gi <- cbind(nigeria.gi, gi.fixed) %>%\n  rename(gstat_fixed = gi.fixed) \nnigeria.gi <- cbind(nigeria.gi, gi.fixed_fun) %>%\n  rename(gstat_fixed_fun = gi.fixed_fun) \n\n\nVisualising Z score for Gertis-Ord Gi\nOnce we have attained the Z score for each area, we will have to filter the area with z score less than the given critical value of 3.886. Since we are doing a two-tailed test, we will halve the critical value and check for Z score < - CV/2 and Z score > CV/2. We will then visualise the variable using tmap.\nThe code chunk below will generate the map:\n\nShow the code#Fixed Distance Gi Map\nGimap_fixed <- tm_shape(nigeria.gi) +\n  tm_borders(col = \"lightgrey\", alpha = 0.5) +\n  tm_shape(nigeria.gi[nigeria.gi$gstat_fixed < -1.943 | nigeria.gi$gstat_fixed > 1.943,]) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Gi Map (Functional)\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\nGimap_fixed_fun <-tm_shape(nigeria.gi) +\n  tm_borders(col = \"lightgrey\", alpha = 0.5) +\n  tm_shape(nigeria.gi[nigeria.gi$gstat_fixed_fun < -1.943 | nigeria.gi$gstat_fixed_fun > 1.943,]) +\n  tm_fill(col = \"gstat_fixed_fun\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Gi Map (Functional)\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\ntmap_arrange(Gimap_fixed,Gimap_fixed_fun)\n\n\n\nShow the code#Adaptive Distance Gi Map\nGimap_adaptive <-tm_shape(nigeria.gi) +\n  tm_borders(col = \"lightgrey\", alpha = 0.5) +\n  tm_shape(nigeria.gi[nigeria.gi$gstat_adaptive < -1.943 | nigeria.gi$gstat_adaptive > 1.943,]) +\n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Gi Map (Non-Functional)\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nGimap_adaptive_fun <-tm_shape(nigeria.gi) +\n  tm_borders(col = \"lightgrey\", alpha = 0.5) +\n  tm_shape(nigeria.gi[nigeria.gi$gstat_adaptive_fun < -1.943 | nigeria.gi$gstat_adaptive_fun > 1.943,]) +\n  tm_fill(col = \"gstat_adaptive_fun\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Gi Map (Functional)\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\ntmap_arrange(Gimap_adaptive,Gimap_adaptive_fun)\n\n\n\n\nAnalysis of Gi map result\nThe results from the Gi map seems to align to what the LISA map have shown. From the Gi Map, we can see the southern area having more clustered non-functional water point and does not see any significance of clustered or dispersed functional water point. This would likely be the priority of repair if resources are limited.\nSimilarly, the map on the right also shows more clustered functional water point at the northern regions and dispersed functional water points at the southern region.\nThe result of both LISA map and Gi map shows consistency in the area of concerns and area of focused. This is align to the recent initiatives by ENI that focus on ensuring water access to the North-East region of Nigeria that shows why these regions are not an area of concern. The study by Adebayo Oluwole Eludoyin also substantiate our claim on the importance of focusing on the South-West region of Nigeria to improve the water access for it’s population."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#conclusion",
    "href": "posts/Geo/Geospatial_Exercise/index.html#conclusion",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Conclusion",
    "text": "Conclusion\nGeospatial Autocorrelation remains an important tool for organization to better allocate limited resources to solve any issues. The use of LISA and Gi map will allow organisation to better understand the geographical relationship based on the attribute analysed and enhance the quality of decision-making."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#geospatial-autocorrelation",
    "href": "posts/Geo/Geospatial_Exercise/index.html#geospatial-autocorrelation",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "GeoSpatial Autocorrelation",
    "text": "GeoSpatial Autocorrelation\nIntroduction\nThe Spatial Autocorrelation measures spatial autocorrelation based on feature locations and feature values simultaneously. Given a set of features and an associated attribute, it evaluates whether the expressed pattern is clustered, scattered, or random. The tool calculates the Moran’s I index value as well as a z-score and p-value to assess the significance of this index. P-values are numerical approximations of the area under the curve for a known distribution, bounded by the test statistic.\nIn this study we will explore the computation of Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package.\nFor this study, we will be using the distance based weight matrix. There are two type of distance-based proximity matrix, they are:\n\nFixed Distance Weight Matrix\nAdaptive Distance Weight Matrix\n\nSince the study is regarding the prioritisation of water tap repair and to identify areas which have restriction in water supply access, neighbouring regions that are nearer to the selected region should have greater weights compared to neighbouring regions that are further away. Therefore, with this concept in mind, we will employ the Inverse distance weighting to take distance decay into consideration.\nDeriving distance-based and adaptive weight matrix\nThe first step is to retrieve the centroid for each area. To retrieve the centroid of each area, we will use the st_centroid() function. The st_centroid() function will calculates the geometric center of a spatial object.\n\nShow the codecoords <- st_centroid(st_geometry(nigeria))\ncoords[1]\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 549364 ymin: 123694.9 xmax: 549364 ymax: 123694.9\nProjected CRS: Minna / Nigeria West Belt\n\n\nDetermine the cut-off distance for fixed distance weight matrix\nSecondly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nShow the codek1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2669   12808   20008   21804   27013   72139 \n\nShow the codethreshold <- max(unlist(nbdists(k1, coords)))\n\n\nThe summary report shows that the largest first nearest neighbour distance is 72.139 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour. We then save the max value as the threshold for the subsequent function.\nComputing Fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below. The function identifies neighbours of region points by Euclidean distance in the metric of the points between lower (greater than or equal to and upper (less than or equal to) bounds.\n\nShow the codewm_d73 <- dnearneigh(coords, 0, threshold)\nwm_d73\n\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 18022 \nPercentage nonzero weights: 3.111958 \nAverage number of links: 23.682 \n\n\nFrom the result above, we can identify an average of 23 neighbours per region using the distance based weight matrix.\nComputing Inverse Adaptive distance weight matrix\nNext, we will compute the inverse adaptive distance weight matrix. The knearneigh() function uses spartial indexing to identify the nearest neighbour. For this analysis, we will set to number of neighbours to 8.\n\nShow the codek8 <- knn2nb(knearneigh(coords, k = 8))\nk8\n\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 6088 \nPercentage nonzero weights: 1.051248 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nComputing Inverse Distance Weights\nTo compute the inverse distance, we need a function that applies \\(\\frac{1}{x}\\) to the entire distance data structure. We use lapply to achieve this. The required parameters are the distance and the function specified by lapply. Use the functional operator and \\(\\frac{1}{x}\\) to get the corresponding function. The important thing to note is that the distance units in the dataset are meters. This means that the distance values between points can be very large, resulting in small reciprocals. To fix this scale dependency, rescale the distances by doing a \\(\\frac{x}{1000}\\)in the function before computing the reciprocal.\nComputing Inverse Distance Weights for Fixed Distance\n\nShow the codedistances <- nbdists(wm_d73,coords)\ndistances[1]\n\n[[1]]\n [1]  4004.008 45439.251 37710.356 51041.840 67445.464 65694.575 70278.881\n [8] 66402.395 70754.787 63293.565 44070.130 67132.783 30584.808 31191.772\n[15] 31060.775 61914.751 40430.849 50524.637 68364.757 57815.248 58326.473\n[22] 21477.914 55573.697 46904.765 62426.753 37887.549 52063.146 40086.745\n[29] 52077.350 19783.847 31237.020 65772.348 68438.043 48339.227 66025.875\n[36] 63788.895 32358.664 62335.064 71500.748 11065.063 48564.675 29667.531\n[43] 49875.572 55850.546 70099.782 57661.903 16123.664 69453.919 46496.314\n[50]  9313.577 56305.742 48780.437 56595.108 31208.261 53734.648 40998.087\n[57] 10676.236 35065.479 20859.370 23059.238 53898.687 43026.271 61625.223\n\nShow the codedistances <- lapply(distances, function(x) (1/(x/1000)))\ndistances[1]\n\n[[1]]\n [1] 0.24974975 0.02200741 0.02651791 0.01959177 0.01482679 0.01522196\n [7] 0.01422903 0.01505970 0.01413332 0.01579939 0.02269111 0.01489585\n[13] 0.03269597 0.03205974 0.03219495 0.01615124 0.02473359 0.01979232\n[19] 0.01462742 0.01729648 0.01714487 0.04655946 0.01799412 0.02131980\n[25] 0.01601877 0.02639390 0.01920744 0.02494590 0.01920221 0.05054629\n[31] 0.03201330 0.01520396 0.01461176 0.02068713 0.01514558 0.01567671\n[37] 0.03090362 0.01604234 0.01398587 0.09037454 0.02059110 0.03370688\n[43] 0.02004990 0.01790493 0.01426538 0.01734247 0.06202064 0.01439804\n[49] 0.02150708 0.10737014 0.01776018 0.02050002 0.01766937 0.03204280\n[55] 0.01860997 0.02439138 0.09366597 0.02851808 0.04794009 0.04336657\n[61] 0.01855333 0.02324161 0.01622712\n\n\nComputing Inverse Distance Weights for Adaptive Distance\n\nShow the codek.distances <- nbdists(k8, coords)\nk.distances[1]\n\n[[1]]\n[1]  4004.008 21477.914 19783.847 11065.063 16123.664  9313.577 10676.236\n[8] 20859.370\n\nShow the codeinvdistance <- lapply(k.distances, function(x) (1/(x/1000)))\ninvdistance[1]\n\n[[1]]\n[1] 0.24974975 0.04655946 0.05054629 0.09037454 0.06202064 0.10737014 0.09366597\n[8] 0.04794009\n\n\nVisualising Weight Matrices\nUsing the base R plot function, we will now plot to visualise the areas with their respective neighbours after assignment based on the various methods. The left graph with the red lines show the adaptive distance with 8 neighbours and the right graph with the black lines show the links of neighbours within the cut-off distance of the above threshold.\n\nShow the codepar(mfrow=c(1,2))\nplot(nigeria$geometry, border=\"lightgrey\", main=\"Adaptive Distance (8)\")\nplot(k8, coords, add=TRUE, col=\"red\", length=0.08)\nplot(nigeria$geometry, border=\"lightgrey\", main=\"Fixed Distance\")\nplot(wm_d73, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\nConstructing Distance Binary Weight Matrix\nNow we will have to construct the distance weight matrix using the neighbours and inverse distance that were computed above. Since we have the inverse distance as a variable, we will use the Binary Weight Matrix and input the inverse distance as the weight. The nb2listw() function takes in a list input glist to identify the weight for each neighbour.\nThe code chunk below will construct the fixed distance weight matrix and subsequently implement the binary weight matrix using the nb2listw() function.\n\nShow the codebwm_fd <- nb2listw(wm_d73,\n                    glist = distances,\n                   style=\"B\", \n                   zero.policy = TRUE)\nsummary (bwm_fd)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 18022 \nPercentage nonzero weights: 3.111958 \nAverage number of links: 23.682 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 6 11 10 17 30 31 31 33 29 36 26 18 21 23 16 12 11 11 17 16 16 10 14 10  9  9 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n12  6 15 13 11  9  8  8 12  6 12 17 14  9  9  4  5  3  8  5 11  8  5  5  4  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 67 68 70 \n 3  3  6  3  5  6  2  5  5  8  6  6  4  3  1  1 \n6 least connected regions:\n88 110 121 235 657 753 with 1 link\n1 most connected region:\n572 with 70 links\n\nWeights style: B \nWeights constants summary:\n    n     nn       S0       S1       S2\nB 761 579121 486.0122 43.00868 2155.714\n\n\nThe code chunk below will construct the adaptive distance weight matrix and subsequently implement the Binary weight matrix using the nb2listw() function.\n\nShow the codebwm_apd <- nb2listw(k8,\n                    glist = invdistance,\n                    style = \"B\",\n                    zero.policy = TRUE)\nsummary (bwm_apd)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 6088 \nPercentage nonzero weights: 1.051248 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n  8 \n761 \n761 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 with 8 links\n761 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 with 8 links\n\nWeights style: B \nWeights constants summary:\n    n     nn       S0       S1       S2\nB 761 579121 237.6033 30.43191 453.5401\n\n\nGlobal Spatial Autocorrelation: Moran’s I\nTo identify whether the area analysed is clustered, dispersed or random, we will have to perform the spatial autocorrelation Moran’s I test.\nComputing Global Moran’s I\nWe will now perform Moran’s I statistics testing by using the moran.test() from spdep on both functional and non functional water point percentage. The global Moran’s I test will also be performed on both the fixed distance and adaptive distance weight matrices.\n\nShow the codemoran.test(nigeria$pct_nonfunctional, \n           listw=bwm_fd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_nonfunctional  \nweights: bwm_fd    \n\nMoran I statistic standard deviate = 32.052, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.4210179196     -0.0013157895      0.0001736248 \n\n\n\nShow the codemoran.test(nigeria$pct_functional, \n           listw=bwm_fd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_functional  \nweights: bwm_fd    \n\nMoran I statistic standard deviate = 42.929, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5644376118     -0.0013157895      0.0001736823 \n\n\n\nShow the codemoran.test(nigeria$pct_nonfunctional, \n           listw=bwm_apd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_nonfunctional  \nweights: bwm_apd    \n\nMoran I statistic standard deviate = 22.233, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5115959890     -0.0013157895      0.0005322145 \n\n\n\nShow the codemoran.test(nigeria$pct_functional, \n           listw=bwm_apd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_functional  \nweights: bwm_apd    \n\nMoran I statistic standard deviate = 24.492, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5638015977     -0.0013157895      0.0005323946 \n\n\nInterpretation of results\nBased on all the results above, the p value is below the alpha value of 0.05, therefore we have enough statistical evidence to reject the null hypothesis that the attribute is randomly distributed and a positive Moran I value indicate more spatially clustered than would be expected if underlying spatial processes were random.\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nShow the codeset.seed(1234)\nfd_MC = moran.mc(nigeria$pct_nonfunctional, \n                listw=bwm_fd, \n                nsim=999, \n                zero.policy = TRUE,\n                na.action=na.omit)\nfd_MC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_nonfunctional \nweights: bwm_fd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.42102, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nfd_MC_fun = moran.mc(nigeria$pct_functional, \n                listw=bwm_fd, \n                nsim=999, \n                zero.policy = TRUE,\n                na.action=na.omit)\nfd_MC_fun\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_functional \nweights: bwm_fd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.56444, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nadp_MC= moran.mc(nigeria$pct_nonfunctional, \n                listw=bwm_apd, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nadp_MC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_nonfunctional \nweights: bwm_apd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.5116, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nadp_MC_fun = moran.mc(nigeria$pct_functional, \n                listw=bwm_apd, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nadp_MC_fun\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_functional \nweights: bwm_apd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.5638, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nVisualising Monte Carlo Moran’s I\nWe will use a density plot to visualise the output of the Monte Carlo Moran’s I. First, we need to extract the res value and convert it into a dataframe. We then visualise the test statistic result using geom_density() from the ggplot package. geom_vline is use to represent the actual Moran I value.\nThe code chunk below is to extract the res value and convert it to a dataframe format using as.data.frame() function. The code chunk will generate the density plot using ggplot.\n\nShow the code# Fixed Distance Monte Carlo\n\nfd_monte_carlo <- as.data.frame(fd_MC[7])\n\nfd_mc <- ggplot(fd_monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.42102),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8))\n\nfd_monte_carlo_fun <- as.data.frame(fd_MC_fun[7])\n\nfd_mc_fun <- ggplot(fd_monte_carlo_fun, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.56444),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8)) \n\n# Adaptive Distance Monte Carlo\n\nadp_monte_carlo <- as.data.frame(adp_MC[7])\n\nad_mc <- ggplot(adp_monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.5116),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8)) \n\nadp_monte_carlo_fun <- as.data.frame(adp_MC_fun[7])\n\nad_mc_fun <- ggplot(adp_monte_carlo_fun, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.5638),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the code(fd_mc + fd_mc_fun) / (ad_mc + ad_mc_fun) \n\n\n\n\nInterpretation of Monte Carlo Results\nBased on the above plots and results of the Monte Carlos Moran I test, we can conclude that all the results are statistically significant with a p value < 0.05 and all the Moran I values falls way to the right of the distribution suggesting that the functional and non functional water points are clustered (a positive Moran’s I value suggests clustering)."
  },
  {
    "objectID": "posts/Geo/Geographic_Segmentation/index.html",
    "href": "posts/Geo/Geographic_Segmentation/index.html",
    "title": "Introduction to Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "",
    "text": "Any organization that needs to discover distinct groupings of consumers, sales transactions, or other types of behaviors and items may find cluster analysis to be a helpful data-mining tool. For instance, banks employ cluster analysis for credit rating and insurance companies use it to identify fraudulent claims.\nFinding comparable groups of individuals is the goal of cluster analysis, where “similarity” between each pair of subjects refers to a general measure over the entire collection of attributes. In this post, we’ll talk about several clustering techniques and the crucial part that distance plays in determining how close together two points are.\nHowever, the inclusion of spatial data might possibly change the way we should conduct our clustering analysis. Spatial aspects such as location and contiguity can be considered as locational similarity and should be taken into consideration as one of the important aspect of geospatial clustering analysis.\nFor this study, we will explore both conventional clustering methods and spatial clustering methods to visualise the different in results for the two methods."
  },
  {
    "objectID": "posts/Geo/Geographic_Segmentation/index.html#libraries",
    "href": "posts/Geo/Geographic_Segmentation/index.html#libraries",
    "title": "Introduction to Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Libraries",
    "text": "Libraries\nFor this study, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\nShow the codepacman::p_load(rgdal, spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geographic_Segmentation/index.html#data-preparation",
    "href": "posts/Geo/Geographic_Segmentation/index.html#data-preparation",
    "title": "Introduction to Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nMyanmar Township Boundary Data: his is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\n\nShan-ICT..csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\nImporting of data\nIn this section, you will import Myanmar Township Boundary GIS data and its associated attrbiute table into R environment.\nThe Myanmar Township Boundary GIS data is in ESRI shapefile format. It will be imported into R environment by using the st_read() function of sf.\nThe code chunks used are shown below:\n\nShow the codeshan_sf <- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %>%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\"))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geographic_Segmentation\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported township boundary object is called shan_sf. It is saved in simple feature data.frame format. We can view the content of the newly created shan_sf simple features data.frame by using the code chunk below.\n\nShow the codeshan_sf\n\nSimple feature collection with 55 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   OBJECTID           ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1       163 Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2       203 Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3       240 Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4       106 Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5        72 Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6        40 Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7       194 Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8       159 Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9        61 Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10      124 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                 ST_2            LABEL2 SELF_ADMIN ST_RG T_NAME_WIN T_NAME_M3\n1  Shan State (North)    Mongmit\\n61072       <NA> State   rdk;rdwf      မိုးမိတ်\n2  Shan State (South)    Pindaya\\n77769       Danu State     yif;w,     ပင်းတယ\n3  Shan State (South)    Ywangan\\n76933       Danu State      &GmiH       ရွာငံ\n4  Shan State (South)  Pinlaung\\n162537       Pa-O State  yifavmif;   ပင်လောင်း\n5  Shan State (North)     Mabein\\n35718       <NA> State     rbdrf;      မဘိမ်း\n6  Shan State (South)     Kalaw\\n163138       <NA> State       uavm      ကလော\n7  Shan State (South)      Pekon\\n94226       <NA> State     z,fcHk       ဖယ်ခုံ\n8  Shan State (South)          Lawksawk       <NA> State   &yfapmuf    ရပ်စောက်\n9  Shan State (North) Nawnghkio\\n128357       <NA> State  aemifcsdK    နောင်ချို\n10 Shan State (North)   Kyaukme\\n172874       <NA> State   ausmufrJ    ကျောက်မဲ\n       AREA                       geometry\n1  2703.611 MULTIPOLYGON (((96.96001 23...\n2   629.025 MULTIPOLYGON (((96.7731 21....\n3  2984.377 MULTIPOLYGON (((96.78483 21...\n4  3396.963 MULTIPOLYGON (((96.49518 20...\n5  5034.413 MULTIPOLYGON (((96.66306 24...\n6  1456.624 MULTIPOLYGON (((96.49518 20...\n7  2073.513 MULTIPOLYGON (((97.14738 19...\n8  5145.659 MULTIPOLYGON (((96.94981 22...\n9  3271.537 MULTIPOLYGON (((96.75648 22...\n10 3920.869 MULTIPOLYGON (((96.95498 22...\n\n\nNotice that sf.data.frame is conformed to Hardy Wickham’s tidy framework.\nSince shan_sf is conformed to tidy framework, we can also glimpse() to reveal the data type of it’s fields.\n\nShow the codeglimpse(shan_sf)\n\nRows: 55\nColumns: 15\n$ OBJECTID   <dbl> 163, 203, 240, 106, 72, 40, 194, 159, 61, 124, 71, 155, 101…\n$ ST         <chr> \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (Sout…\n$ ST_PCODE   <chr> \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\",…\n$ DT         <chr> \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"…\n$ DT_PCODE   <chr> \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MM…\n$ TS         <chr> \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kal…\n$ TS_PCODE   <chr> \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR015…\n$ ST_2       <chr> \"Shan State (North)\", \"Shan State (South)\", \"Shan State (So…\n$ LABEL2     <chr> \"Mongmit\\n61072\", \"Pindaya\\n77769\", \"Ywangan\\n76933\", \"Pinl…\n$ SELF_ADMIN <chr> NA, \"Danu\", \"Danu\", \"Pa-O\", NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ST_RG      <chr> \"State\", \"State\", \"State\", \"State\", \"State\", \"State\", \"Stat…\n$ T_NAME_WIN <chr> \"rdk;rdwf\", \"yif;w,\", \"&GmiH\", \"yifavmif;\", \"rbdrf;\", \"uavm…\n$ T_NAME_M3  <chr> \"မိုးမိတ်\", \"ပင်းတယ\", \"ရွာငံ\", \"ပင်လောင်း\", \"မဘိမ်း\", \"ကလော\", \"ဖယ်ခုံ\", \"…\n$ AREA       <dbl> 2703.611, 629.025, 2984.377, 3396.963, 5034.413, 1456.624, …\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (…\n\n\nImporting aspatial data into R environment\nThe csv file will be import using read_csv function of readr package.\nThe code chunks used are shown below:\n\nShow the codeict <- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\nShow the codesummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThere are a total of eleven fields and 55 observation in the tibble data.frame.\nDerive new variables using dplyr package\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\n\nShow the codeict_derived <- ict %>%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\n\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\nShow the codesummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR."
  },
  {
    "objectID": "posts/Geo/Geographic_Segmentation/index.html#exploratory-data-analysis-eda",
    "href": "posts/Geo/Geographic_Segmentation/index.html#exploratory-data-analysis-eda",
    "title": "Introduction to Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nEDA using statistical graphics\nWe can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\nShow the codeggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\nShow the codeggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\nShow the codeggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\nWhat can you observed from the distributions reveal in the histogram and boxplot.\nIn the figure below, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\nShow the coderadio <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv <- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone <- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone <- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer <- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet <- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\nNext, the ggarange() function of ggpubr package is used to group these histograms together.\n\nShow the codeggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\nEDA using choropleth map\nJoining geospatial data with aspatial data\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\n\nShow the codeshan_sf <- left_join(shan_sf, \n                     ict_derived, \n                     by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n\n\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.\nPreparing a choropleth map\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\nShow the codeqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nShow the codeTT_HOUSEHOLDS.map <- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map <- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\nShow the codetm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)"
  },
  {
    "objectID": "posts/Geo/Geographic_Segmentation/index.html#correlation-analysis",
    "href": "posts/Geo/Geographic_Segmentation/index.html#correlation-analysis",
    "title": "Introduction to Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\nShow the codecluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "posts/Geo/Geographic_Segmentation/index.html#hierarchy-cluster-analysis",
    "href": "posts/Geo/Geographic_Segmentation/index.html#hierarchy-cluster-analysis",
    "title": "Introduction to Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Hierarchy Cluster Analysis",
    "text": "Hierarchy Cluster Analysis\nIn this section, you will learn how to perform hierarchical cluster analysis. The analysis consists of four major steps:\nExtrating clustering variables\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\nShow the codecluster_vars <- shan_sf %>%\n  st_set_geometry(NULL) %>%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nShow the coderow.names(cluster_vars) <- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\nShow the codeshan_ict <- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nFeature Scaling\nIn reality, we frequently come across various kinds of variables in the same dataset. The fact that the variables’ ranges can be very different is a serious problem. The variables with a wide range may receive greater weight if the original scale is used. In the stage of data pre-processing, we must apply the technique of features rescaling to independent variables or features of the data in order to address this issue.\nApplying feature scaling aims to ensure that features are roughly on the same scale, making each feature equally important and making it simpler for most ML algorithms to process.\nSome machine learning models, like Hierarchical Clustering, K-Nearest Neighbors, SVM, and Neural Network, are primarily based on distance matrix, commonly known as the distance-based classifier. These models undoubtedly require feature scaling, especially when the range of the features is relatively diverse. Otherwise, features with a wide range will have a significant impact on how the distance is calculated.\nMax-Min Normalization\nMax-Min Normalization will rescale features value to have a distribution value between 0 and 1. Every feature has a minimum value of 0 and a maximum value of 1, with 0 being the default value for each feature. The general formula is displayed below:\n\nShow the codeshan_ict.std <- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nZ-score Normalization\nStandardization, also known as Z-score normalization, causes the characteristics to be rescaled so that the mean and standard deviation are, respectively, 0 and 1.\nZ-score normalization can be performed easily by using scale() of Base R. The code chunk below will be used to perform the normalization of the clustering variables by using Z-score method. We will also use the describe() from psych package instead of summary() for the report of the standard deviation.\n\nShow the codeshan_ict.z <- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nVisualising the standardised clustering variables\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\nShow the coder <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Original Distribution\") +\n  theme_classic()\n\nshan_ict_s_df <- as.data.frame(shan_ict.std)\ns <- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\") +\n  theme_classic()\n\nshan_ict_z_df <- as.data.frame(shan_ict.z)\nz <- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\") +\n  theme_classic()\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\nNotice that the overall distribution of the clustering variables will change after the data standardisation. Hence, it is advisible NOT to perform data standardisation if the values range of the clustering variables are not very large or if your modelling does not require the implementation of distance-based algorithm.\nComputing proximity matrix\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nShow the codeproxmat <- dist(shan_ict, method = 'euclidean')\n\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\nShow the codeproxmat\n\n\nComputing hierarchical clustering\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nShow the codehclust_ward <- hclust(proxmat, method = 'ward.D')\n\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nShow the codeplot(hclust_ward, cex = 0.6)\n\n\n\n\nSelecting the optimal clustering algorithm\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nShow the codem <- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\nDetermining Optimal Clusters\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\nRefer to my RFM post for more explanation on these 3 methods. For this analysis we will use the gap statistic method.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nShow the codeset.seed(12345)\ngap_stat <- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50,     nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --> Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nShow the codefviz_gap_stat(gap_stat)\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\nInterpreting the dendrograms\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nShow the codeplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\nVisually-driven hierarchical clustering analysis\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\nTransforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nShow the codeshan_ict_mat <- data.matrix(shan_ict)\n\n\nPlotting interactive cluster heatmap using heatmaply()\n\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nShow the codeheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n\nMapping the clusters formed\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\nShow the codegroups <- as.factor(cutree(hclust_ward, k=6))\n\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nShow the codeshan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\nShow the codeqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "posts/Geo/Geographic_Segmentation/index.html#spatially-constrained-clustering---skater-approach",
    "href": "posts/Geo/Geographic_Segmentation/index.html#spatially-constrained-clustering---skater-approach",
    "title": "Introduction to Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Spatially Constrained Clustering - SKATER approach",
    "text": "Spatially Constrained Clustering - SKATER approach\nIn this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\nConverting into SpatialPolygonsDataFrame\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nShow the codeshan_sp <- as_Spatial(shan_sf)\n\n\nComputing Neighbour List\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nShow the codeshan.nb <- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nShow the codeplot(shan_sp, \n     border=grey(.5))\nplot(shan.nb, \n     coordinates(shan_sp), \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\nNote: If you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\nComputing minimum spanning tree\nCalculating edge costs\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nShow the codelcosts <- nbcosts(shan.nb, shan_ict)\n\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\nShow the codeshan.w <- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\nComputing minimum spanning tree\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nShow the codeshan.mst <- mstree(shan.w)\n\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below.\n\nShow the codeclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\nShow the codedim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nWe can display the content of shan.mst by using head() as shown in the code chunk below.\n\nShow the codehead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   31   25 229.44658\n[2,]   25   10 163.95741\n[3,]   10    1 144.02475\n[4,]   10    9 157.04230\n[5,]    9    8  90.82891\n[6,]    8    6 140.01101\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nShow the codeplot(shan_sp, border=gray(.5))\nplot.mst(shan.mst, \n         coordinates(shan_sp), \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\nComputing spatially constrained clusters using SKATER method\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nShow the codeclust6 <- skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nShow the codestr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 31 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 31 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the conde chunk below.\n\nShow the codeccs6 <- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetically, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\nShow the codetable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nShow the codeplot(shan_sp, border=gray(.5))\nplot(clust6, \n     coordinates(shan_sp), \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\nVisualising the clusters in choropleth map\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\nShow the codegroups_mat <- as.matrix(clust6$groups)\nshan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other\n\nShow the codehclust.map <- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map <- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)"
  },
  {
    "objectID": "posts/Geo/Geographic_Segmentation/index.html#spatially-constrained-clustering-clustgeo-method",
    "href": "posts/Geo/Geographic_Segmentation/index.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Introduction to Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Spatially Constrained Clustering: ClustGeo Method",
    "text": "Spatially Constrained Clustering: ClustGeo Method\nIn this section, you will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\nWard-like hierarchical clustering: ClustGeo\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nShow the codenongeo_cluster <- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\nMapping the clusters formed\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\nShow the codegroups <- as.factor(cutree(nongeo_cluster, k=6))\nshan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER` = `as.matrix.groups.`)\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\nSpatially Constrained Hierarchical Clustering\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\nShow the codedist <- st_distance(shan_sf, shan_sf)\ndistmat <- as.dist(dist)\n\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\nShow the codecr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nShow the codeclustG <- hclustgeo(proxmat, distmat, alpha = 0.3)\n\n\nNext, cutree() is used to derive the cluster object.\n\nShow the codegroups <- as.factor(cutree(clustG, k=6))\n\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chun below.\n\nShow the codeshan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nWe can not plot the map of the newly delineated spatially constrained clusters.\n\nShow the codeqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "posts/Geo/geospatial_nigeriasegmentation/index.html",
    "href": "posts/Geo/geospatial_nigeriasegmentation/index.html",
    "title": "Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "",
    "text": "A regionalization is a specific type of clustering whose goal is to combine data that are comparable in both their statistical characteristics and their geographic locations. Regionalization uses the same logic as conventional clustering approaches in this regard, but it additionally imposes a number of geographical restrictions. These restrictions frequently have to do with connectivity. It is not always necessary for connectedness to be true for all regions, and in some situations it makes sense to loosen connectivity or to impose alternative kinds of geographic restrictions.\nFor this post, we will examine Traditional Clustering Algorithm and Spatially Constrained Clustering Algorithm to identify its utility and limitations.\n\nFor this study, we will use the following packages from CRAN.\nFor Geographical Analysis and Visualisation the packages used are: \n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’.\nClustGeo - Ward-like hierarchical clustering algorithm including spatial/geographical constraints.\nrgdal - Provides bindings to the ‘Geospatial’ Data Abstraction Library (‘GDAL’) and access to projection/transformation operations from the ‘PROJ’ library.\nrgeoda - For spatial data analysis based on libgeoda and GeoDa.\n\nFor Traditional Clustering Algorithm the packages used are: \n\ncluster - Provides bindings to the ‘Geospatial’ Data Abstraction Library (‘GDAL’) and access to projection/transformation operations from the ‘PROJ’ library.\nfactoextra - Extract and visualize the output of multivariate data analyses.\nNbClust - Determining the Best Number of Clusters in a Data Set.\n\nFor Data Wrangling and Simple Visualisation the packages used are: \n\npatchwork - Combine separate ggplots into the same graphic.\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\nheatmaply - Cluster heatmap based on plotly.\ncorrplot - A graphical display of a correlation matrix, confidence interval.\npsych - A general purpose toolbox for personality, psychometric theory and experimental psychology.\nreshape2 - Flexibly restructure and aggregate data using just two functions: melt and ‘dcast’.\nhrbrthemes - Provides typography-centric themes and theme components for ggplot2.\nggstatsplot - An extension of {ggplot2} package for creating graphics with details from statistical tests included in the information-rich plots themselves\n\n\nShow the codepacman::p_load(rgdal, spdep, tmap, sf, ClustGeo,rgeoda, \n               cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse,reshape2,hrbrthemes, GGally, ggstatsplot, patchwork)"
  },
  {
    "objectID": "posts/Geo/geospatial_nigeriasegmentation/index.html#data-preparation",
    "href": "posts/Geo/geospatial_nigeriasegmentation/index.html#data-preparation",
    "title": "Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nNigeria.shp: A shapefile of Nigeria from Humanitarian Data Exchange Portal that consist of all the Level-2 Administrative Boundary (also known as Local Government Area)\nNigeriaAttribute.csv: A csv file containing multiple water point attributes of each Level-2 Administrative Nigeria Boundary from the WPdx Global Data Repositories\n\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment.\n\nShow the codenigeria <- st_read(dsn = \"data\", \n                 layer = \"geoBoundaries-NGA-ADM2\")\n\nnigeria_attribute <- read_csv(\"data/nigeriaattribute.csv\")\n\nnigeria <- nigeria %>%\n  select(shapeName) %>%\n  st_transform(crs = 26391)\n\n\n\n\n\n\n\n\nData Wrangling\nThe practice of correcting or deleting inaccurate, damaged, improperly formatted, duplicate, or incomplete data from a dataset is known as data wrangling. There are numerous ways for data to be duplicated or incorrectly categorized when merging multiple data sources. We willl now proceed to ensure our data is cleaned before conducting our analysis.\nChecking of duplicated area name\nFirstly, we will order our dataframe by alphabetical order based on the shapeName. We will then use the duplicated function to retrieve all the shapeName that has duplicates and store it in a list. From the result below, we identified 12 shapeNames that are duplicates.\n\nShow the codenigeria <- (nigeria[order(nigeria$shapeName), ])\n\nnigeria<- nigeria %>%\n  mutate(shapeName = tolower(shapeName))\n\nduplicate_area <- nigeria$shapeName[ nigeria$shapeName %in% nigeria$shapeName[duplicated(nigeria$shapeName)] ]\n\nduplicate_area\n\n [1] \"bassa\"    \"bassa\"    \"ifelodun\" \"ifelodun\" \"irepodun\" \"irepodun\"\n [7] \"nasarawa\" \"nasarawa\" \"obi\"      \"obi\"      \"surulere\" \"surulere\"\n\n\nNext, we will leverage on the interactive viewer of tmap to check the location of each area. Through the use of Google, we are able to retrieve the actual name and state of the areas. The table below shows the index and the actual name of the area.\n\n\nIndex\nActual Area Name\n\n\n\n94\nBassa (Kogi)\n\n\n95\nBassa (Plateau)\n\n\n304\nIfelodun (Kwara)\n\n\n305\nIfelodun (Osun)\n\n\n355\nIrepodun (Kwara)\n\n\n356\nIrepodun (Osun)\n\n\n518\nNassarawa\n\n\n546\nObi (Benue)\n\n\n547\nObi(Nasarawa)\n\n\n693\nSurulere (lagos)\n\n\n694\nSurulere (Oyo)\n\n\n\n\nShow the codetmap_mode(\"view\")\n\ntm_shape(nigeria[nigeria$shapeName %in% duplicate_area,]) +\n  tm_polygons()\n\n\n\n\n\nShow the codetmap_mode(\"plot\")\n\n\nWe will now access the individual index of the nigeria data frame and change the value. Lastly, we use the length() function to ensure there is no more duplicated shapeName.\n\nShow the codenigeria$shapeName[c(94,95,304,305,355,356,519,546,547,693,694)] <- c(\"bassa kogi\",\"bassa plateau\",\n                                                                               \"ifelodun kwara\",\"ifelodun osun\",\n                                                                               \"irepodun kwara\",\"irepodun osun\",\n                                                                               \"nassarawa\",\"obi benue\",\"obi nasarawa\",\n                                                                               \"surulere lagos\",\"surulere oyo\")\n\nlength((nigeria$shapeName[ nigeria$shapeName %in% nigeria$shapeName[duplicated(nigeria$shapeName)] ]))\n\n[1] 0\n\n\nProjection of sf dataframe\nSince our aspatial data was imported to a tibble dataframe, we will need to convert it to an sf object. First, we rename the columns for ease of representation using the rename() function from dyplr. We then only retain the columns required for analysis such as the name of the latitude, longitude, status, water_tech_category, usage_capacity, is_urban and water_point_population. \nWe realised there were NA values within the multiple columns, so we will replace the NA values with Unknown and 0 using the mutate() function.\nWe will then use the st_as_sf() function to convert the dataframe to an sf object. We will have to input the column that specify the longitude and latitude, and lastly, the CRS projection of the coordinates.\n\nShow the codenigeriaT <- nigeria_attribute  %>%\n  rename (\"status\" = \"#status_clean\",\n          \"lat\" = \"#lat_deg\",\n          \"long\" = \"#lon_deg\",\n          \"water_tech\" = \"#water_tech_category\") %>%\n  select (status,lat,long,water_tech,usage_capacity,is_urban,water_point_population) %>%\n  mutate(status = replace_na(status, \"Unknown\")) %>%\n  mutate (water_tech = replace_na(water_tech, \"Unknown\")) %>%\n  mutate(water_point_population = replace_na(water_point_population,0))\n\nnigeriaT_sf <- st_as_sf(nigeriaT, coords = c(\"long\", \"lat\"),  crs = 4326)\n\n\nWe will now transform the coordinates from 4326 to 26391 projection using the st_transform() function.\n\nShow the codenigeriaT_sf <- st_transform(nigeriaT_sf, crs = 26391)\n\nst_crs (nigeria)\n\nCoordinate Reference System:\n  User input: EPSG:26391 \n  wkt:\nPROJCRS[\"Minna / Nigeria West Belt\",\n    BASEGEOGCRS[\"Minna\",\n        DATUM[\"Minna\",\n            ELLIPSOID[\"Clarke 1880 (RGS)\",6378249.145,293.465,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4263]],\n    CONVERSION[\"Nigeria West Belt\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",4,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",4.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.99975,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",230738.26,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Nigeria - onshore west of 6°30'E, onshore and offshore shelf.\"],\n        BBOX[3.57,2.69,13.9,6.5]],\n    ID[\"EPSG\",26391]]\n\nShow the codest_crs (nigeriaT_sf)\n\nCoordinate Reference System:\n  User input: EPSG:26391 \n  wkt:\nPROJCRS[\"Minna / Nigeria West Belt\",\n    BASEGEOGCRS[\"Minna\",\n        DATUM[\"Minna\",\n            ELLIPSOID[\"Clarke 1880 (RGS)\",6378249.145,293.465,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4263]],\n    CONVERSION[\"Nigeria West Belt\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",4,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",4.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.99975,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",230738.26,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Nigeria - onshore west of 6°30'E, onshore and offshore shelf.\"],\n        BBOX[3.57,2.69,13.9,6.5]],\n    ID[\"EPSG\",26391]]\n\n\nUsing the unique() function, we can identify how many variables are stored within the columns.\n\nShow the codeunique (nigeriaT_sf$usage_capacity)\n\n[1]  250 1000  300   50\n\nShow the codeunique (nigeriaT_sf$water_tech)\n\n[1] \"Tapstand\"        \"Mechanized Pump\" \"Hand Pump\"       \"Unknown\"        \n[5] \"Rope and Bucket\"\n\nShow the codeunique (nigeriaT_sf$status)\n\n[1] \"Unknown\"                          \"Functional\"                      \n[3] \"Abandoned/Decommissioned\"         \"Non-Functional\"                  \n[5] \"Functional but not in use\"        \"Functional but needs repair\"     \n[7] \"Abandoned\"                        \"Non functional due to dry season\"\n[9] \"Non-Functional due to dry season\"\n\n\nWe will also use the st_join() function from the sf package to retrieve the shapeName from the polygon geometry based on the point geometry from the attribute file.\n\nShow the codenigeriaT_sf <- st_join(nigeriaT_sf,nigeria)\n\n\nVisualising of distribution using ggplot\nWe will use the ggplot function to visualise the distribution of the different columns. To sort the distribution by descending order fct_infreq will be use.\nDistribution of status\n\nShow the codeggplot(data= nigeriaT_sf, \n       aes(x= fct_infreq(status))) +\n  geom_bar(aes(fill = status), show.legend = FALSE) +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             2), '%')), vjust= -0.5, size= 2.5) +\n  labs(y= 'No. of\\nOccurence', x= 'Status',\n       title = \"Distribution of Water Tap Status\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'),\n        axis.text.x = element_text(angle = 90, vjust = 0.5))\n\n\n\n\nDistribution of Water Technology\n\nShow the codeggplot(data= nigeriaT_sf, \n       aes(x= fct_infreq(water_tech))) +\n  geom_bar(aes(fill = water_tech), show.legend = FALSE) +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             2), '%')), vjust= -0.5, size= 2.5) +\n  labs(y= 'No. of\\nOccurence', x= 'Tap Technology',\n       title = \"Distribution of Water Tap Technology\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'),\n        axis.text.x = element_text(angle = 90, vjust = 0.5))\n\n\n\n\nDistribution of Usage_Capacity\nSince usage_capacity is in numeric, we will use the as.character() function to convert it into character class before plotting their distribution.\n\nShow the codeggplot(data= nigeriaT_sf, \n       aes(x= fct_infreq(as.character(usage_capacity)))) +\n  geom_bar(aes(fill = as.character(usage_capacity)), show.legend = FALSE) +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             2), '%')), vjust= -0.5, size= 2.5) +\n  labs(y= 'No. of\\nOccurence', x= 'Tap Technology',\n       title = \"Distribution of Water Tap Technology\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'),\n        axis.text.x = element_text(angle = 90, vjust = 0.5))\n\n\n\n\nDistribution of Rural Water Point\n\nShow the codeggplot(data= nigeriaT_sf, \n       aes(x= fct_infreq(as.character(is_urban)))) +\n  geom_bar(aes(fill = as.character(is_urban)), show.legend = FALSE) +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             2), '%')), vjust= -0.5, size= 2.5) +\n  labs(y= 'No. of\\nOccurence', x= 'Urban Water Point?',\n       title = \"Distribution of Rural Water Point\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'),\n        axis.text.x = element_text(angle = 90, vjust = 0.5))\n\n\n\n\nExtracting Status of Water Point\nSince this analysis is on the functionality of the water taps, we have to extract the number of functional and non-functional water taps from the nigeriaT_sf dataframe. The status column reveal the status of the water tap. We will now see what values are recorded by using the unique() function. From the result below, we can identify mainly four categories of statuses; Functional, Non-Functional, Abandoned, Unknown.\nNote: For this analysis, abandoned water taps will be analysed under non-functional.\nFrom the result below, we can identify a pattern to classify the status based on our criteria. By extracting the first word of the sentence before the punctuation, we will be able to extract the word; Unknown, Abandoned, Functional and Non. This will assist us in grouping these status.\n\nShow the codeunique(nigeriaT_sf$status)\n\n[1] \"Unknown\"                          \"Functional\"                      \n[3] \"Abandoned/Decommissioned\"         \"Non-Functional\"                  \n[5] \"Functional but not in use\"        \"Functional but needs repair\"     \n[7] \"Abandoned\"                        \"Non functional due to dry season\"\n[9] \"Non-Functional due to dry season\"\n\n\nTo replace the original values, we will use the gsub() function. A regular expression “([A-Za-z]+).*” is used to extract all letters and \\\\1 is used to back reference the first capturing group. The result below shows the unique values left within the column.\n\nShow the codenigeriaT_sf$status <- gsub(\"([A-Za-z]+).*\", \"\\\\1\", nigeriaT_sf$status)\nunique(nigeriaT_sf$status)\n\n[1] \"Unknown\"    \"Functional\" \"Abandoned\"  \"Non\"       \n\n\nComputing Ratio of Functional and Non Functional Water Point\nInstead of creating another data frame to store the new values, we will leverage on the filter function by using the single square bracket “[]” operator. The coordinates beings with a row position and that will be used for our filtering condition. R Dataframe. Since our water taps are point data, we will use st_intersects() to retrieve every geometry point that intersect with the polygon of the Nigeria ADM area, and subsequently use the function lengths() to retrieve the number of points that intersects with the polygon.\n\nShow the codenigeria$pct_functional <- (lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Functional\",]))/lengths(st_intersects(nigeria, nigeriaT_sf)))\nnigeria$pct_nonfunctional <- (lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Non\",])) + lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Abandoned\",])))/(lengths(st_intersects(nigeria, nigeriaT_sf)))\n\nnigeria$functional <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Functional\",]))\nnigeria$nonfunctional <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Non\",])) + lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Abandoned\",]))\nnigeria$wp_total <- lengths(st_intersects(nigeria,nigeriaT_sf))\n\n\nComputing Ratio of the Different Water Tap Technology\nSimilar to the way we compute the ratio of functional and non-functional water point, we will extract the percentage of the different water technology within each region. We will not compute the percentage of Rope and Bucket since there is only 1 observation and Unknown pump technology.\n\nShow the codenigeria$pct_mechpump <- (lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$water_tech == \"Mechanized Pump\",])))/(lengths(st_intersects(nigeria, nigeriaT_sf)))\nnigeria$pct_handpump <- (lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$water_tech == \"Hand Pump\",]))/lengths(st_intersects(nigeria, nigeriaT_sf)))\n\n\nComputing Ratio of the Water Point Utilisation Capacity\nFrom the earlier result, we know that there is only 4 unique variables (50, 250, 300 and 1000). But these variables are currently saved as numeric object. Since we want to classify them as a unique identifier rather than a continuous variable, we will convert them to characters using the as.character() function. Lastly, from the distribution, 50 does not seem to have too many observations and therefore will be excluded from the analysis. 250 and 300 will also be combined to form an object of <1000 utilisation.\n\nShow the codenigeriaT_sf$usage_capacity <- as.character(nigeriaT_sf$usage_capacity )\nnigeria$pct_1000M <- (lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$usage_capacity == \"1000\",]))/lengths(st_intersects(nigeria, nigeriaT_sf)))\nnigeria$pct_1000L <- (lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$usage_capacity == \"300\",])) + lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$usage_capacity == \"250\",])))/(lengths(st_intersects(nigeria, nigeriaT_sf))) \n\n\nComputing Ratio of Rural Water Point\nTo extract the ratio of Rural Water Point, we will retrieve the number of water point that is_urban == FALSE and divide by the total number of water point in that region.\n\nShow the codenigeria$pct_rural<- (lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$is_urban == FALSE,]))/lengths(st_intersects(nigeria, nigeriaT_sf)))\n\n\nComputing Average Percentage of Overused water points\nOverused water point is calculated using the usage_capacity and the water_point_population. We will first compute the pecentage overused for each water point, subsequently, using the group_by and summarise function to retrieve the mean of the overuse percentage by region.\n\nShow the codeoveruse <- nigeriaT_sf %>%\n  mutate (pct_overuse = (water_point_population/as.numeric(usage_capacity))*100) %>%\n  group_by(shapeName) %>%\n  summarise (avg_pct_overuse = mean(pct_overuse))  %>%\n  st_drop_geometry() %>%\n  ungroup()\n\n\nNext, we will use the left_join() function to join the nigeria dataframe with the overused data. For areas without any water taps, my assumption is that these areas do not need water taps for many possible reasons (lack of habitat, urbanised areas, etc), and therefore will be excluded from the analysis. I use the filter() function to remove areas without any water taps and mutate() function to create two new columns that shows the percentage of functional and non-functional water points over the total water point in the area. (Including unknown water point status)\n\nShow the codenigeria <- nigeria %>%\n  left_join(overuse,by=\"shapeName\") %>%\n  filter (wp_total != 0) %>%\n  select(-wp_total)\n\n\nWe will use the summary function to retrieve statistical information of each column.\n\nShow the codesummary(nigeria)\n\n  shapeName         pct_functional   pct_nonfunctional   functional    \n Length:761         Min.   :0.0000   Min.   :0.0000    Min.   :  0.00  \n Class :character   1st Qu.:0.3333   1st Qu.:0.2211    1st Qu.: 18.00  \n Mode  :character   Median :0.4792   Median :0.3559    Median : 47.00  \n                    Mean   :0.5070   Mean   :0.3654    Mean   : 68.51  \n                    3rd Qu.:0.6749   3rd Qu.:0.5082    3rd Qu.: 88.00  \n                    Max.   :1.0000   Max.   :1.0000    Max.   :752.00  \n nonfunctional     pct_mechpump     pct_handpump      pct_1000M     \n Min.   :  0.00   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.: 14.00   1st Qu.:0.1250   1st Qu.:0.1860   1st Qu.:0.1250  \n Median : 34.00   Median :0.3193   Median :0.5255   Median :0.3193  \n Mean   : 42.31   Mean   :0.3818   Mean   :0.4956   Mean   :0.3818  \n 3rd Qu.: 61.00   3rd Qu.:0.5843   3rd Qu.:0.7857   3rd Qu.:0.5843  \n Max.   :278.00   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n   pct_1000L        pct_rural      avg_pct_overuse            geometry  \n Min.   :0.0000   Min.   :0.0000   Min.   :    0.0   MULTIPOLYGON :761  \n 1st Qu.:0.4157   1st Qu.:0.5922   1st Qu.:  182.7   epsg:26391   :  0  \n Median :0.6807   Median :0.8717   Median :  310.7   +proj=tmer...:  0  \n Mean   :0.6182   Mean   :0.7395   Mean   :  511.8                      \n 3rd Qu.:0.8750   3rd Qu.:1.0000   3rd Qu.:  587.6                      \n Max.   :1.0000   Max.   :1.0000   Max.   :12064.4                      \n\n\nVisualising Column Distribution\nTo visualise all the distribution easily, we will first use the melt() function from reshape2 to convert the data table to long format. Next, we will visualise the distribution using a density plot and facet_wrap by each variable. To remove the scientific notation from the axes, we will adjust the scale by inputting labels = scales::comma.\n\nShow the codenigeria_density_plot <- nigeria %>%\n  st_drop_geometry() %>%\n  melt()\n\n\n\nShow the codeggplot(data = nigeria_density_plot, aes(x = value)) + \n  stat_density(fill = \"steelblue\", alpha = 0.6) + \n  scale_y_continuous(labels = scales::comma) +\n  scale_x_continuous(labels = scales::comma) +\n  theme_classic() +\n  facet_wrap(~variable, scales = \"free\", ncol = 3)\n\n\n\n\nFrom the distribution, we can identify the variables does not conform to Gaussian distribution and we should perform normalization or standardisation before proceeding with our analysis.\nVariables Collinearity\nWhen clustering variables are collinear, certain variables are given more weight than others. Two variables that are highly correlated indicate the same notion. However, because that notion is now represented twice in the data, it receives double the weight of all other variables. The ultimate solution is likely to be tilted toward that notion, which might be a problem if not foreseen. Therefore there is a need to check for Collinearity before conducting cluster analysis. We will use the cor() and corrplot.mixed function from the corrplot package to visualise and identify highly correlated variables.\n\nShow the codenigeria$functional <- as.numeric(nigeria$functional)\nnigeria$nonfunctional <- as.numeric(nigeria$nonfunctional)\n\nnigeria_var <- nigeria %>%\n  st_drop_geometry()\n\ncluster_vars.cor = cor(nigeria_var[,2:11])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nFrom the result above, we can identify 4 variables that are highly correlated. We will now choose to only retain 1 (pct_mechpump) for the analysis. We will create another dataframe to store all the variables required for the subsequent cluster analysis.\n\nShow the codenigeria <- nigeria %>%\n  select(-pct_handpump, -pct_1000M, -pct_1000L)\n\nnigeria_cluster_var <- nigeria %>%\n  st_drop_geometry()\n\n\nSince the dataframe should only consist of variables used for the clustering, the shapeName should now be an identifier for each row. We will use the row.names() function to set the id for each row using their respective shapeName and remove the shapeName column from the dataframe.\n\nShow the coderow.names(nigeria_cluster_var) <- nigeria_cluster_var$shapeName\nnigeria_cluster_var <- select(nigeria_cluster_var, c(2:8))\n\n\nData Normalization\nLarger variances between data points of input variables increases the model’s results’ uncertainty.\nMachine learning models assign weights to input variables based on data points and output inferences. If the difference between the data points is large, the model will need to give the points more weight, and the model with a large weight value is generally unstable in the end. This implies that the model may produce bad results or perform poorly during training. Therefore, we will have to ensure our variables are scaled if we were to perform any distance-based algorithm such as K-means Clustering. To scale all the columns, we will use mutate_at() to select all the columns, and perform scaling through the scale() function.\n\nShow the codenigeria_cluster_var <- nigeria_cluster_var %>% \n  mutate_at(c(1:7), funs(c(scale(.))))"
  },
  {
    "objectID": "posts/Geo/geospatial_nigeriasegmentation/index.html#conventional-clustering",
    "href": "posts/Geo/geospatial_nigeriasegmentation/index.html#conventional-clustering",
    "title": "Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "Conventional Clustering",
    "text": "Conventional Clustering\nHierarchical Clustering\nHierarchical clustering, also known as hierarchical cluster analysis or HCA, is another unsupervised machine learning approach used to sort unlabeled datasets into clusters.\nThe dendrogram is a tree-shaped structure that we construct in this approach to develop the hierarchy of clusters.\nThe hierarchical clustering technique has two approaches:\n\n\nAgglomerative: Agglomerative is a bottom-up approach, in which the algorithm starts with taking all data points as single clusters and merging them until one cluster is left.\n\nDivisive: Divisive is a top-down approach, in which the algorithm start with all the data points as one cluster, and slowly seperate them until all data points becomes a single clusters.\n\nComputing of Distance Matrix\nWe will now create the distance matrix using the dist() function. Since we have multiple dimensions to the data, we will use the Manhattan distance to compute the distance matrix instead of the standard Eucluidean distance.\n\nShow the codeproxmat <- dist(nigeria_cluster_var, method = 'manhattan')\n\n\nComputing Optimal Hierarchical Clustering Clustering Values\nWe will only use the Gap Statistic Method to determine the optimal cluster. The clusGap() function from the cluster package will be used to calculate the goodness of the clustering measure.\n\nShow the codegap_stat <- clusGap(nigeria_cluster_var, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = nigeria_cluster_var, FUNcluster = hcut, K.max = 10,     B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --> Number of clusters (method 'firstmax'): 4\n          logW   E.logW      gap      SE.sim\n [1,] 6.470009 7.545740 1.075731 0.007361406\n [2,] 6.361211 7.452901 1.091690 0.012434692\n [3,] 6.269918 7.390535 1.120617 0.012058076\n [4,] 6.185244 7.338807 1.153563 0.010242376\n [5,] 6.155719 7.294504 1.138785 0.010224412\n [6,] 6.106019 7.255521 1.149502 0.009719880\n [7,] 6.067774 7.220190 1.152416 0.009033557\n [8,] 6.027122 7.189133 1.162011 0.008551612\n [9,] 5.986136 7.161042 1.174906 0.008710675\n[10,] 5.953721 7.135503 1.181781 0.009210229\n\n\nWe will use the fviz_gap_stat() function to visualise the gap statistics results.\n\nShow the codefviz_gap_stat(gap_stat)\n\n\n\n\nFrom the above plot, we can conclude that the optimal number of cluster for the Hierarchical Clustering Algorithm is 4.\nSelecting of Hierarchical Clustering Algorithm\nTo compute the Hierarchical Cluster, we will use the hclust() function from R stats. The hclust() uses the Agglomerative approach to compute the cluster. There are 8 different clustering algorithm supported by the function: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC). To select the optimal algorithm, we will use the agnes() function from the cluster package.\n\nShow the codem <- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- m\n\nac <- function(x) {\n  agnes(nigeria_cluster_var, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.9564875 0.9246884 0.9633398 0.9831761 \n\n\nFrom the above result, the ward algorithm will produce the optimal cluster coefficient and therefore will be use for this analysis.\nComputing of Hierarchical Clustering and Dendrogram Visualisation\nUsing the hclust() function, we will compute the Hierarchical Cluster and visualise the clusters on a dendrogram using the rect.hclust() function. Based on the plot below, we can clearly identify 4 cluster but is unable to understand which area belongs to which cluster. We will subsequently use another visualisation method to visualise the output.\n\nShow the codeset.seed(1234)\nh_clust_ward <- hclust(proxmat, method = 'ward.D')\nplot(h_clust_ward, cex = 0.5)\nrect.hclust(h_clust_ward, \n            k = 4, \n            border = 2:5)\n\n\n\n\nSummary of Hierarchical Cluster\nWe can conduct a quick summary of the cluster by binding the clusters to the nigeria dataframe using cbind(). To extract the 4 cluster, the cutree() function is used.\n\nShow the codegroups <- as.factor(cutree(h_clust_ward, k=4))\nnigeria <- cbind(nigeria, as.matrix(groups)) %>%\n  rename(`HR_Cluster` = `as.matrix.groups.`)\n\n\nThe code chunk below generates a quick summary of the average values of all the variables within the cluster.\n\nShow the codenigeria %>%\n  group_by(HR_Cluster) %>%\n  summarise_all(\"mean\")\n\nSimple feature collection with 4 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 28879.72 ymin: 30292.37 xmax: 1342613 ymax: 1094244\nProjected CRS: Minna / Nigeria West Belt\n# A tibble: 4 × 10\n  HR_Cluster shapeName pct_functional pct_nonfunctional functional nonfunctional\n  <chr>          <dbl>          <dbl>             <dbl>      <dbl>         <dbl>\n1 1                 NA          0.387             0.333       30.9          26.3\n2 2                 NA          0.313             0.638       25.3          49.3\n3 3                 NA          0.488             0.388       86.9          71.2\n4 4                 NA          0.811             0.174      128.           29.0\n# … with 4 more variables: pct_mechpump <dbl>, pct_rural <dbl>,\n#   avg_pct_overuse <dbl>, geometry <MULTIPOLYGON [m]>\n\n\nVisualisation of Hierarchical Cluster using tmap\nUsing the qtm() function, we plot the Nigeria map based on their designated cluster.\n\nShow the codeqtm(nigeria, \"HR_Cluster\")\n\n\n\n\nVisualisation of Hierarchical Cluster using Parallel Coordinates Plot\nTo better understand the distribution of values based on the different cluster, we will use the ggparcoord() function from the GGally package. The ggparcoord() visualisation provide different scaling methods to the variables such as: Min-Max(uniminmax), Z-Score (std) and many others. For our visualisation, we will be using the Z-score Normalization to visualise the distribution of data. The facet_grid() function is used to seperate the different clusters by their grids.\n\nShow the codeggparcoord(nigeria,\n    columns = 2:8,\n    showPoints = TRUE, \n    title = \"Parallel Coordinate Plot for the Nigeria Attributes using Hierarchical Clustering\",\n    groupColumn = \"HR_Cluster\",\n    alphaLines = 0.3,\n    scale=\"std\",\n    boxplot = TRUE\n    ) + \n  scale_color_viridis(discrete=TRUE) +\n  theme_ipsum()+\n  theme(\n    plot.title = element_text(size=10),\n    axis.text.x = element_text(size = 6)\n  ) +\n  facet_grid(~HR_Cluster)\n\n\n\n\nInterpretation of Hierarchical Clustering Results\nBased on the parallel coordinates plot, we can infer that Cluster 2 should be an area of concern with a higher average of pct_nonfunctional compare to pct_functional. The next area of concern could be Cluster 1 with also a higher average of pct_nonfunctional than pct_functional complemented with a high average of pct_overuse. We will substantiate all the claims in the subsequent section.\nK-Means Clustering\nK-means clustering is a straightforward unsupervised learning approach for grouping issues. It employs a straightforward approach for categorizing a given data set into a specific number of clusters denoted by the letter “k.” The clusters are then positioned as points, and all observations or data points are associated with the nearest cluster, computed, modified, and the process is repeated until the desired result is obtained. K-Means clustering is also often known as a Partition Clustering Method. We will now analyse the difference between K-means and Hierarchical Clustering.\nComputing Optimal K-Means Clustering Values\nSimilar to Hierarchical Clustering, we will use the clusGap() function to obtain the optimal cluster value.\n\nShow the codegap_stat <- clusGap(nigeria_cluster_var, FUN = kmeans, nstart = 25,\n                    K.max = 12, B = 50)\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nFrom the above plot, we can conclude that the optimal number of cluster for the K-Means Clustering Algorithm is 4.\nComputing of K-means Clustering\nUsing the kmeans() function, we will compute the k-means results.\n\nShow the codeset.seed(1234)\nfinal_kmeans <- kmeans(nigeria_cluster_var, 4, nstart = 25)\n\n\nSummary of K-means Cluster\nWe will then have a quick summary on the cluster by first binding the new cluster column to the Nigeria data frame, then the summarise_all function to display the average of each variables based on their cluster.\n\nShow the codenigeria <- nigeria %>%\n  mutate(KM_Cluster = as.factor(final_kmeans$cluster))\n\n\nThe code chunk below generates a quick summary of the average values of all the variables within the cluster.\n\nShow the codenigeria %>%\n  group_by(KM_Cluster) %>%\n  summarise_all(\"mean\")\n\nSimple feature collection with 4 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 28879.72 ymin: 30292.37 xmax: 1342613 ymax: 1094244\nProjected CRS: Minna / Nigeria West Belt\n# A tibble: 4 × 11\n  KM_Cluster shapeName pct_functional pct_nonfunctional functional nonfunctional\n  <fct>          <dbl>          <dbl>             <dbl>      <dbl>         <dbl>\n1 1                 NA          0.479             0.240       32.3          17.3\n2 2                 NA          0.799             0.177      127.           26.7\n3 3                 NA          0.443             0.435       89.8          83.1\n4 4                 NA          0.340             0.551       19.9          29.9\n# … with 5 more variables: pct_mechpump <dbl>, pct_rural <dbl>,\n#   avg_pct_overuse <dbl>, HR_Cluster <dbl>, geometry <MULTIPOLYGON [m]>\n\n\nVisualisation of K-means Cluster using tmap\nUsing the qtm() function, we plot the Nigeria map based on their designated K-means cluster.\n\nShow the codeqtm(nigeria,fill = \"KM_Cluster\")\n\n\n\n\nVisualisation of K-means Cluster using Parallel Coordinates Plot\nWe will now visualise the variables using the ggparcoord() function.\n\nShow the codeggparcoord(nigeria,\n    columns = 2:8,\n    showPoints = TRUE, \n    title = \"Parallel Coordinate Plot for the Nigeria Attributes\",\n    groupColumn = \"KM_Cluster\",\n    alphaLines = 0.3,\n    scale=\"std\",\n    boxplot = TRUE\n    ) + \n  scale_color_viridis(discrete=TRUE) +\n  theme_ipsum()+\n  theme(\n    plot.title = element_text(size=10),\n    axis.text.x = element_text(size = 6)\n  ) +\n  facet_grid(~KM_Cluster)\n\n\n\n\nInterpretation of K-Means Clustering Results\nFrom the above results, we can infer that Cluster 3 and Cluster 4 are areas of concerns with a higher average of pct_nonfunctional compared to pct_functional. The most significant difference is that despite having a higher average of pct_overuse, Cluster 1 have a higher pct_functional compared to pct_nonfunctional.\nPartitioning Around Medodoids (PAM) Clustering\nIn order to construct clusters, the PAM algorithm searches a data set for k representative objects (k medoids) and assigns each object to the closest medoid. Its goal is to minimize the sum of dissimilarities between objects in a cluster and the cluster’s center (medoid). It is regarded to be a more robust variant of k-means since it is less sensitive to outliers.\nComputing Optimal PAM Clustering Clustering Values\nSimilar to K-means Clustering, we will use the clusGap() function to obtain the optimal cluster value.\n\n\n\n\n\n\nNote\n\n\n\nPAM Clustering do not require the nstart component within the clusGap() function.\n\n\n\nShow the codegap_stat <- clusGap(nigeria_cluster_var, FUN = pam,\n                    K.max = 12, B = 50)\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nFrom the above plot, we can conclude that the optimal number of cluster for the PAM Clustering Algorithm is 4.\nComputing of PAM Clustering\nUsing the pam() function, we will compute the PAM results. Since we have identified earlier to use the manhattan distance instead of euclidean distance, the pam() function require an input on the distance metric using the metric variable. We will also use the manhattan distance metric to compute our PAM cluster.\n\nShow the codeset.seed(1234)\nfinal_pam <- pam(nigeria_cluster_var, 4, metric = \"manhattan\")\n\n\nSummary of PAM Cluster\nWe will then have a quick summary on the cluster by first binding the new cluster column to the Nigeria data frame, then the summarise_all function to display the average of each variables based on their cluster.\n\nShow the codenigeria <- nigeria %>%\n  mutate(PAM_Cluster = as.factor(final_pam$cluster))\n\nnigeria %>%\n  group_by(PAM_Cluster) %>%\n  summarise_all(\"mean\")\n\nSimple feature collection with 4 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 28879.72 ymin: 30292.37 xmax: 1342613 ymax: 1094244\nProjected CRS: Minna / Nigeria West Belt\n# A tibble: 4 × 12\n  PAM_Cluster shapeName pct_functional pct_nonfunction… functional nonfunctional\n  <fct>           <dbl>          <dbl>            <dbl>      <dbl>         <dbl>\n1 1                  NA          0.437            0.274       26.8          18.1\n2 2                  NA          0.326            0.612       19.5          34.5\n3 3                  NA          0.796            0.163      127.           25.4\n4 4                  NA          0.466            0.424       89.6          78.9\n# … with 6 more variables: pct_mechpump <dbl>, pct_rural <dbl>,\n#   avg_pct_overuse <dbl>, HR_Cluster <dbl>, geometry <MULTIPOLYGON [m]>,\n#   KM_Cluster <dbl>\n\n\nVisualisation of PAM Cluster using tmap\nUsing the qtm() function, we plot the Nigeria map based on their designated PAM cluster.\n\nShow the codeqtm(nigeria,fill = \"PAM_Cluster\")\n\n\n\n\nVisualisation of PAM Cluster using Parallel Coordinates Plot\nWe will now visualise the variables using the ggparcoord() function.\n\nShow the codeggparcoord(nigeria,\n    columns = 2:8,\n    showPoints = TRUE, \n    title = \"Parallel Coordinate Plot for the Nigeria Attributes\",\n    groupColumn = \"PAM_Cluster\",\n    alphaLines = 0.3,\n    scale=\"std\",\n    boxplot = TRUE\n    ) + \n  scale_color_viridis(discrete=TRUE) +\n  theme_ipsum()+\n  theme(\n    plot.title = element_text(size=10),\n    axis.text.x = element_text(size = 6)\n  ) +\n  facet_grid(~PAM_Cluster)\n\n\n\n\nInterpretation of PAM Clustering Results\nFrom the above results, we can infer that Cluster 2 and Cluster 4 are areas of concerns with a higher average of pct_nonfunctional compared to pct_functional. These clusters also have a higher average of pct_rural which could infer that rural water points have higher percentage of being not functional. The most significant difference is that despite having a higher average of pct_overuse, Cluster 1 have a higher pct_functional compared to pct_nonfunctional.\nVisualisation of Traditional Clustering Results\nWe will now visualise the Nigeria base map with all the different clustering results using tmap. Based on the plot below, we can generally identify that the southern region are the region that are area of concerns based on their pct_nonfunctional and pct_functional. This result is consistent among all the different clustering algorithm.\n\nShow the codetmap_mode (\"plot\")\nHC_Clust <- tm_shape (nigeria) +\n  tm_polygons(\"HR_Cluster\",\n          title = \"Hierarchical Cluster\") +\n  tm_layout(main.title = \"Distribution of Hierarchical Cluster by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nK_Clust <- tm_shape (nigeria) +\n  tm_polygons(\"KM_Cluster\",\n          title = \"K-Means Cluster\") +\n  tm_layout(main.title = \"Distribution of K-Means Cluster by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nPAM_Clust <- tm_shape (nigeria) +\n  tm_polygons(\"PAM_Cluster\",\n          title = \"PAM Cluster\") +\n  tm_layout(main.title = \"Distribution of PAM Cluster by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(HC_Clust, K_Clust, PAM_Clust, ncol = 3, asp = 1)"
  },
  {
    "objectID": "posts/Geo/geospatial_nigeriasegmentation/index.html#spatially-constrainted-clustering",
    "href": "posts/Geo/geospatial_nigeriasegmentation/index.html#spatially-constrainted-clustering",
    "title": "Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "Spatially Constrainted Clustering",
    "text": "Spatially Constrainted Clustering\nClustering data is well-trodden territory, and many approaches apply to geographical data as well. The advantage of spatially limited approaches is that it requires spatial objects in the same cluster to be geographically related. This has a number of benefits in real-world applications that require breaking geographies into discrete sections (regionalization), such as building communities, planning areas, amenity zones, logistical units, or even setting up experiments with real-world geographic limits.\nIn this section, we will explore 3 different spatially constrained clustering algorithm namely:\n\nSpatial ’K’luster Analysis by Tree Edge Removal(SKATER)\nSpatially Constrained Hierarchical Clustering\nRegionalization with Dynamically Constrained Agglomerative clustering and Partitioning (REDCAP)\n\nSKATER\nThe SKATER algorithm is based on a connectivity graph to express spatial relationships between neighboring regions, with each area represented by a node and edges representing connections between areas. The dissimilarity between neighboring areas is used to compute edge costs.\nRetrieving the neighbour list\nWe will use the poly2nb function to construct neighbours list based on the regions with contiguous boundaries. Based on the documentation, user will be able to pass a queen argument that takes in True or False. The argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\nShow the codenigeria_nb <- poly2nb(nigeria, queen=TRUE)\nsummary(nigeria_nb)\n\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 4348 \nPercentage nonzero weights: 0.750793 \nAverage number of links: 5.713535 \nLink number distribution:\n\n  1   2   3   4   5   6   7   8   9  10  11  12  14 \n  4  16  57 122 177 137 121  71  39  11   4   1   1 \n4 least connected regions:\n136 497 513 547 with 1 link\n1 most connected region:\n496 with 14 links\n\n\nFrom the summary, we can identify 4 regions with only 1 neighbour and 1 region with 14 neighbours. The average number of neighbours per region is 5.\nRetrieving the Centroid of Polygon\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. To retrieve the centroid of each area, we will use the st_centroid function.\n\nShow the codecoords <- st_centroid(st_geometry(nigeria))\n\n\nVisualising of neighbours\nWe will now plot and visualise the neighbours relationship for each region.\n\nShow the codeplot(nigeria$geometry, \n     border=grey(.5))\nplot(nigeria_nb, \n     coords, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\nComputing Edge Cost\nUsing the nbcosts() function, we will calculate the distance between each node which is define as the cost of the edge. For each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\n\nShow the codelcosts <- nbcosts(nigeria_nb, nigeria_cluster_var)\n\n\nComputing Neighbour Weights based on Edge Cost\nWe will now incorporate the cost into a weights object. This is done by using the nb2listw() function and setting the style to B.\n\nShow the codenigeria_weight <- nb2listw(nigeria_nb, \n                   lcosts, \n                   style=\"B\")\nsummary(nigeria_weight)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 4348 \nPercentage nonzero weights: 0.750793 \nAverage number of links: 5.713535 \nLink number distribution:\n\n  1   2   3   4   5   6   7   8   9  10  11  12  14 \n  4  16  57 122 177 137 121  71  39  11   4   1   1 \n4 least connected regions:\n136 497 513 547 with 1 link\n1 most connected region:\n496 with 14 links\n\nWeights style: B \nWeights constants summary:\n    n     nn       S0       S1       S2\nB 761 579121 10026.67 62271.18 675249.1\n\n\nComputing Minimal Spanning Tree (MST)\nWe will now compute the minimal spanning tree which depicts a fully connected graph with only one neighbour per node. The MST is computed by the mstree() function from the spdep package.\n\nShow the codenigeria_mst <- mstree(nigeria_weight)\n\nhead(nigeria_mst)\n\n     [,1] [,2]      [,3]\n[1,]  284   43 1.0834428\n[2,]  284  297 1.5171722\n[3,]  297  185 1.0339759\n[4,]  185   15 0.8213166\n[5,]   15  322 1.3167224\n[6,]  297  586 1.3700931\n\n\nComputing SKATER clustering\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\nThe skater() takes three mandatory arguments:\n\nfirst two columns of the MST matrix (i.e. not the cost)\ndata matrix (to update the costs as units are being grouped)\nnumber of cuts.\n\n\n\n\n\n\n\nNote\n\n\n\nThe cut is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph.\n\n\n\nShow the codeset.seed(1234)\nclust4 <- spdep::skater(edges = nigeria_mst[,1:2], \n                 data = nigeria_cluster_var, \n                 method = \"manhattan\", \n                 ncuts = 3)\n\n\nVisualisation of SKATER group Summary\nUsing the table() function, we can retrieve the number of areas within each cluster.\n\nShow the codeccs4 <- clust4$groups\ntable(ccs4)\n\nccs4\n  1   2   3   4 \n430 120 134  77 \n\n\nSummary of SKATER Cluster\nWe will then have a quick summary on the cluster by first binding the new cluster column to the Nigeria data frame, then the summarise_all function to display the average of each variables based on their cluster.\n\nShow the codegroups_mat <- as.matrix(clust4$groups)\nnigeria <- cbind(nigeria, as.factor(groups_mat)) %>%\n  rename(Skater_Cluster =`as.factor.groups_mat.`)\n\nnigeria %>%\n  group_by(Skater_Cluster) %>%\n  summarise_all(\"mean\")\n\nSimple feature collection with 4 features and 12 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 28879.72 ymin: 30292.37 xmax: 1342613 ymax: 1094244\nProjected CRS: Minna / Nigeria West Belt\n# A tibble: 4 × 13\n  Skater_Cluster shapeName pct_functional pct_nonfunctional functional\n  <fct>              <dbl>          <dbl>             <dbl>      <dbl>\n1 1                     NA          0.503             0.390       69.2\n2 2                     NA          0.725             0.208      152. \n3 3                     NA          0.339             0.389       16.8\n4 4                     NA          0.483             0.430       24.2\n# … with 8 more variables: nonfunctional <dbl>, pct_mechpump <dbl>,\n#   pct_rural <dbl>, avg_pct_overuse <dbl>, HR_Cluster <dbl>, KM_Cluster <dbl>,\n#   PAM_Cluster <dbl>, geometry <GEOMETRY [m]>\n\n\nVisualisation of SKATER Cluster using tmap\nUsing the qtm() function, we plot the Nigeria map based on their designated SKATER cluster.\n\nShow the codeqtm(nigeria, \"Skater_Cluster\")\n\n\n\n\nVisualisation of SKATER Cluster using Parallel Coordinates Plot\nWe will visualise the results using the ggparcoord() function.\n\nShow the codeggparcoord(nigeria,\n    columns = 2:8,\n    showPoints = TRUE, \n    title = \"Parallel Coordinate Plot of SKATER for the Nigeria Attributes\",\n    groupColumn = \"Skater_Cluster\",\n    alphaLines = 0.3,\n    scale=\"std\",\n    boxplot = TRUE\n    ) + \n  scale_color_viridis(discrete=TRUE) +\n  theme_ipsum()+\n  theme(\n    plot.title = element_text(size=10),\n    axis.text.x = element_text(size = 6)\n  ) +\n  facet_grid(~Skater_Cluster)\n\n\n\n\nInterpretation of SKATER Clustering Results\nFrom the above results, we can infer that Cluster 3 and Cluster 4 are areas of concerns with a higher average of pct_nonfunctional compared to pct_functional. These clusters also have a higher average of pct_overuse and pct_mechpump which could infer that mech pump are placed in areas that areas that have higher population to water point.\nSpatially Constrained Hierarchical Clustering\nIn this section, we will conduct Spatially Constrained Hierarchical Clustering using the hclustgeo() function from the ClustGeo package. The ClustGeo implements a Ward-like hierarchical clustering algorithm including spatial/geographical constraints. It also includes a mixing parameter \\(\\alpha\\) that will use to increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest.\nComputing Distance Matrix of Polygon\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() from the sf package.\n\n\n\n\n\n\nNote\n\n\n\nas.dist() instead of as.matrix() is used to convert the data frame into matrix.\n\n\n\nShow the codedist <- st_distance(nigeria, nigeria)\ndistmat <- as.dist(dist)\n\n\nComputing Optimal Mixing Parameter\nTo retrieve the optimal mixing parameter, we will use the choicealpha() function from ClustGeo package. From the results below, we can see that \\(\\alpha\\) 0.2 produces the\n\nShow the codeset.seed(1234)\ncr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=4, graph = TRUE)\n\n\n\n\n\n\n\nComputing of Spatially Constrained Hierarchical Clustering\n\nShow the codeset.seed(1234)\nclustG <- hclustgeo(proxmat, distmat, alpha = 0.2)\n\n\nSummary of Spatially Constrained Hierarchical Cluster\nWe will then have a quick summary on the cluster by first binding the new cluster column to the Nigeria data frame, then the summarise_all function to display the average of each variables based on their cluster. Similar to our unconstrained hierarchical cluster, we will use the cutree() function to retrieve 4 cluster based on the hierarchy.\n\nShow the codegroups <- as.factor(cutree(clustG, k=4))\n\nnigeria <- cbind(nigeria, as.matrix(groups)) %>%\n  rename(`HR_Spatial_Cluster` = `as.matrix.groups.`)\n\nnigeria %>%\n  group_by(HR_Spatial_Cluster) %>%\n  summarise_all(\"mean\")\n\nSimple feature collection with 4 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 28879.72 ymin: 30292.37 xmax: 1342613 ymax: 1094244\nProjected CRS: Minna / Nigeria West Belt\n# A tibble: 4 × 14\n  HR_Spatial_Cluster shapeName pct_functional pct_nonfunctional functional\n  <chr>                  <dbl>          <dbl>             <dbl>      <dbl>\n1 1                         NA          0.393             0.393       20.7\n2 2                         NA          0.454             0.442       71.4\n3 3                         NA          0.806             0.192      175. \n4 4                         NA          0.620             0.232       22.5\n# … with 9 more variables: nonfunctional <dbl>, pct_mechpump <dbl>,\n#   pct_rural <dbl>, avg_pct_overuse <dbl>, HR_Cluster <dbl>, KM_Cluster <dbl>,\n#   PAM_Cluster <dbl>, Skater_Cluster <dbl>, geometry <MULTIPOLYGON [m]>\n\n\nVisualisation of Spatially Constrained Hierarchical Cluster using tmap\nUsing the qtm() function, we plot the Nigeria map based on their designated Hierarchical Cluster.\n\nShow the codeqtm(nigeria, \"HR_Spatial_Cluster\")\n\n\n\n\nVisualisation of Spatially Constrained Hierarchical Cluster using Parallel Coordinates Plot\nWe will visualise the results using the ggparcoord() function.\n\nShow the codeggparcoord(nigeria,\n    columns = 2:8,\n    showPoints = TRUE, \n    title = \"Parallel Coordinate Plot for the Nigeria Attributes\",\n    groupColumn = \"HR_Spatial_Cluster\",\n    alphaLines = 0.3,\n    scale=\"std\",\n    boxplot = TRUE\n    ) + \n  scale_color_viridis(discrete=TRUE) +\n  theme_ipsum()+\n  theme(\n    plot.title = element_text(size=10),\n    axis.text.x = element_text(size = 6)\n  ) +\n  facet_grid(~HR_Spatial_Cluster)\n\n\n\n\nInterpretation of Spatially Constrained Hierarchical Cluster Results\nFrom the above results, we can infer that Cluster 1 with a higher average of pct_nonfunctional compared to pct_functional can be deemed as an area of concern. The cluster also have a higher average of pct_overuse. Cluster 3 and Cluster 4 are areas that are less of a concern with a higher average of pct_functional than pct_nonfunctional.\nREDCAP\nREDCAP starts from building a spanning tree with 4 different ways (single-linkage, average-linkage, ward-linkage and the complete-linkage). The single-linkage way leads to build a minimum spanning tree which is similar to SKATER. Then,REDCAP provides 2 different ways (first-order and full-order constraining) to prune the tree to find clusters. The first-order approach with a minimum spanning tree is exactly the same with SKATER.\nIn GeoDa and pygeoda, the following methods are provided: First-order and Single-linkage, Full-order and Complete-linkage, Full-order and Average-linkage, Full-order and Single-linkage and Full-order and Ward-linkage.\nFor this analysis, we will conduct the Full- Order and Complete-Linkage approach.\nComputing REDCAP Cluster\nBefore we compute the REDCAP cluster, we need to derive the weights in a weights class using the queen_weights() function from the sf package. Next we will use the redcap() function from the sf package that takes in 4 mandatory inputs:\n\nCluster Size\nWeights in Weights class\nDataframe of variables\nApproach\n\nFor this approach, we will analyse with 6 cluster.\n\nShow the codeset.seed(1234)\nnigeria_weights_sf <- queen_weights(nigeria)\nnigeria_clusters  <- redcap(6, nigeria_weights_sf, nigeria_cluster_var, \"fullorder-completelinkage\")\n\n\nUsing the table() function, we can retrieve the number of areas within each cluster.\n\nShow the codetable(nigeria_clusters$Clusters)\n\n\n  1   2   3   4   5   6 \n405 140 105  92  17   2 \n\n\nSummary of REDCAP\nWe will then have a quick summary on the cluster by first binding the new cluster column to the Nigeria data frame, then the summarise_all function to display the average of each variables based on their cluster.\n\nShow the coderedcap_cluster <- as.matrix(nigeria_clusters$Clusters)\nnigeria <- cbind(nigeria, as.factor(redcap_cluster)) %>%\n  rename(`RC_Cluster`=`as.factor.redcap_cluster.`)\n\nnigeria %>%\n  group_by(RC_Cluster) %>%\n  summarise_all(\"mean\")\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 28879.72 ymin: 30292.37 xmax: 1342613 ymax: 1094244\nProjected CRS: Minna / Nigeria West Belt\n# A tibble: 6 × 15\n  RC_Cluster shapeName pct_functional pct_nonfunctional functional nonfunctional\n  <fct>          <dbl>          <dbl>             <dbl>      <dbl>         <dbl>\n1 1                 NA          0.480            0.375        60.8          51.6\n2 2                 NA          0.745            0.252       125.           40.9\n3 3                 NA          0.291            0.438        15.3          24.8\n4 4                 NA          0.457            0.455        18.2          18.4\n5 5                 NA          0.831            0.169       401            75.5\n6 6                 NA          0.25             0.0625        2             0.5\n# … with 9 more variables: pct_mechpump <dbl>, pct_rural <dbl>,\n#   avg_pct_overuse <dbl>, HR_Cluster <dbl>, KM_Cluster <dbl>,\n#   PAM_Cluster <dbl>, Skater_Cluster <dbl>, HR_Spatial_Cluster <dbl>,\n#   geometry <GEOMETRY [m]>\n\n\nVisualisation of REDCAP using tmap\nUsing the qtm() function, we plot the Nigeria map based on their designated REDCAP.\n\nShow the codeqtm(nigeria, \"RC_Cluster\")\n\n\n\n\nVisualisation of REDCAP using Parallel Coordinates Plot\nWe will visualise the results using the ggparcoord() function.\n\nShow the codeggparcoord(nigeria,\n    columns = 2:8,\n    showPoints = TRUE, \n    title = \"Parallel Coordinate Plot of REDCAP for the Nigeria Attributes\",\n    groupColumn = \"RC_Cluster\",\n    alphaLines = 0.3,\n    scale=\"std\",\n    boxplot = TRUE\n    ) + \n  scale_color_viridis(discrete=TRUE) +\n  theme_ipsum()+\n  theme(\n    plot.title = element_text(size=10),\n    axis.text.x = element_text(size = 6)\n  ) +\n  facet_grid(~RC_Cluster)\n\n\n\n\nInterpretation of REDCAP Results\nFrom the above results, we can infer that Cluster 3 and Cluster 4 with a higher average of pct_nonfunctional compared to pct_functional can be deemed as an area of concern. These cluster also have a higher average of pct_overuse and pct_mechpump. Cluster 1 and Cluster 2 are areas that are less of a concern with a higher average of pct_functional than pct_nonfunctional. Cluster 6 with only 2 observations seems to be an area of concern the highest average of pct_overuse.\nVisualisation of Spatially Constrained Clustering Results\nWe will now visualise the Nigeria base map with all the different clustering results using tmap. Based on the plot below, we can generally identify that the southern region are the highlighted as the same cluster and these regions are deemed area of concerns based on their pct_nonfunctional and pct_functional on the earlier results. This result is consistent among all the different spatially clustering algorithm.\n\nShow the codetmap_mode (\"plot\")\nSKATER_Clust <- tm_shape (nigeria) +\n  tm_polygons(\"Skater_Cluster\",\n          title = \"SKATER Cluster\") +\n  tm_layout(main.title = \"Distribution of SKATER Cluster by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nSHC_Clust <- tm_shape (nigeria) +\n  tm_polygons(\"HR_Spatial_Cluster\",\n          title = \"Spatially Constrained Hierarchical Cluster\") +\n  tm_layout(main.title = \"Distribution of Spatially Constrained Hierarchical Cluster by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nREDCAP_Clust <- tm_shape (nigeria) +\n  tm_polygons(\"RC_Cluster\",\n          title = \"REDCAP Cluster\") +\n  tm_layout(main.title = \"Distribution of REDCAP Cluster by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(SKATER_Clust, SHC_Clust, REDCAP_Clust, ncol = 3, asp = 1)"
  },
  {
    "objectID": "posts/Geo/geospatial_nigeriasegmentation/index.html#interpretation-of-all-the-clustering-results-using-statistical-graph",
    "href": "posts/Geo/geospatial_nigeriasegmentation/index.html#interpretation-of-all-the-clustering-results-using-statistical-graph",
    "title": "Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "Interpretation of all the Clustering Results using Statistical graph",
    "text": "Interpretation of all the Clustering Results using Statistical graph\nVisualisation of all the Clustering Results\n\nShow the codetmap_arrange(HC_Clust, K_Clust, PAM_Clust, SKATER_Clust, SHC_Clust, REDCAP_Clust, ncol = 3, asp = 1)\n\n\n\n\nVisualisation of all the Clustering Results using statistical plot\nBefore plotting the statistical plot, we will need to prepare the data frame to be able to group the different clusters as a group. We will have to pivot the dataframe from wide to long using the pivot_longer() function from dpylr. Once we have done that, we will change the class of the Cluster Type to factor and arrange the order of presentation (Traditional Clustering -> Spatially Constrainted Clustering).\n\nShow the codenigeria_stats <- nigeria %>%\n  pivot_longer(\n    cols = (9:14),\n    names_to = \"Cluster_Type\",\n    values_to = \"Cluster_Number\"\n  ) %>%\n  st_drop_geometry()\n\nnigeria_stats$Cluster_Type <- factor(nigeria_stats$Cluster_Type, order = TRUE, levels = c(\"HR_Cluster\", \"KM_Cluster\", \"PAM_Cluster\",\"Skater_Cluster\",\"HR_Spatial_Cluster\",\"RC_Cluster\"))\n\n\nWe will use the grouped_ggbetweenstats() from the ggstatsplot package to visualise the distribution of pct_nonfunctional and see if indeed the clusters have higher/lower areas with pct_nonfunctional.\nFrom the plot below, we can see that clustering results from the traditional clustering provide better statistical prove that the areas identified earlier through the interpretation of the ggparcoord are statistically significant. Whereas the distribution from the spatially constrainted clustering despite having a p-value of < 0.05, not all areas identified above have a significant difference in their pct_nonfunctional.\n\nShow the codegrouped_ggbetweenstats(\n  data             = nigeria_stats,\n  x                = Cluster_Number,\n  y                = pct_nonfunctional,\n  grouping.var     = Cluster_Type,\n  p.adjust.method  = \"bonferroni\",\n  palette          = \"default_jama\",\n  package          = \"ggsci\",\n  plotgrid.args    = list(nrow = 2),\n  annotation.args  = list(title = \"Does the Percentage of Non-Functional Water Point Significantly different from each Cluster?\")\n)"
  },
  {
    "objectID": "posts/Geo/geospatial_nigeriasegmentation/index.html#conclusion",
    "href": "posts/Geo/geospatial_nigeriasegmentation/index.html#conclusion",
    "title": "Regionalisation of Multivariate Water Point Attributes with Non-spatially Constrained and Spatially Constrained Clustering Methods",
    "section": "Conclusion",
    "text": "Conclusion\nFrom our analysis, we can clearly identify the constraint of using spatially constrainted clustering with the areas having not so clearly define variables compared to other clusters. Traditional Clustering algorithm still provide better results in clearly defining the areas. But, spatially constrained clustering will be useful if there is a need to have concentrated resources/limited resources in a specific area, and therefore the clusters should be spatially connected instead of sparse."
  },
  {
    "objectID": "posts/Geo/geospatial_linearRegression/index.html",
    "href": "posts/Geo/geospatial_linearRegression/index.html",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "posts/Geo/geospatial_linearRegression/index.html#the-data",
    "href": "posts/Geo/geospatial_linearRegression/index.html#the-data",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6.2 The Data",
    "text": "6.2 The Data\nTwo data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)"
  },
  {
    "objectID": "posts/Geo/geospatial_linearRegression/index.html#getting-started",
    "href": "posts/Geo/geospatial_linearRegression/index.html#getting-started",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6.3 Getting Started",
    "text": "6.3 Getting Started\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\n\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\n\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\n\n\nSpatial data handling\n\nsf\n\n\n\nAttribute data handling\n\n\ntidyverse, especially readr, ggplot2 and dplyr\n\n\n\n\nChoropleth mapping\n\ntmap\n\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\nShow the codepacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)"
  },
  {
    "objectID": "posts/Geo/geospatial_linearRegression/index.html#a-shirt-note-about-gwmodel",
    "href": "posts/Geo/geospatial_linearRegression/index.html#a-shirt-note-about-gwmodel",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6.4 A shirt note about GWmodel",
    "text": "6.4 A shirt note about GWmodel\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis."
  },
  {
    "objectID": "posts/Geo/geospatial_linearRegression/index.html#geospatial-data-wrangling",
    "href": "posts/Geo/geospatial_linearRegression/index.html#geospatial-data-wrangling",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6.5 Geospatial Data Wrangling",
    "text": "6.5 Geospatial Data Wrangling\n6.5.1 Importing geospatial data\nThe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nShow the codempsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\geospatial_linearRegression\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n6.5.2 Updating CRS information\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\nShow the codempsz_svy21 <- st_transform(mpsz, 3414)\n\n\nAfter transforming the projection metadata, you can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nShow the codest_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\nNext, you will reveal the extent of mpsz_svy21 by using st_bbox() of sf package.\n\nShow the codest_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n6.6.1 Importing the aspatial data\nThe condo_resale_2015 is in csv file format. The codes chunk below uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\nShow the codecondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe codes chunks below uses glimpse() to display the data structure of will do the job.\n\nShow the codeglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             <dbl> 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            <dbl> 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             <dbl> 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nShow the codesummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n6.6.2 Converting aspatial data frame into a sf object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\nShow the codecondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\nShow the codehead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     <dbl>         <dbl>    <dbl> <dbl>    <dbl>          <dbl>            <dbl>\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# … with 15 more variables: PROX_URA_GROWTH_AREA <dbl>,\n#   PROX_HAWKER_MARKET <dbl>, PROX_KINDERGARTEN <dbl>, PROX_MRT <dbl>,\n#   PROX_PARK <dbl>, PROX_PRIMARY_SCH <dbl>, PROX_TOP_PRIMARY_SCH <dbl>,\n#   PROX_SHOPPING_MALL <dbl>, PROX_SUPERMARKET <dbl>, PROX_BUS_STOP <dbl>,\n#   NO_Of_UNITS <dbl>, FAMILY_FRIENDLY <dbl>, FREEHOLD <dbl>,\n#   LEASEHOLD_99YR <dbl>, geometry <POINT [m]>"
  },
  {
    "objectID": "posts/Geo/geospatial_linearRegression/index.html#exploratory-data-analysis-eda",
    "href": "posts/Geo/geospatial_linearRegression/index.html#exploratory-data-analysis-eda",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6.7 Exploratory Data Analysis (EDA)",
    "text": "6.7 Exploratory Data Analysis (EDA)\nIn the section, you will learn how to use statistical graphics functions of ggplot2 package to perform EDA.\n6.7.1 EDA using statistical graphics\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\nShow the codeggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\nShow the codecondo_resale.sf <- condo_resale.sf %>%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\nShow the codeggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation.\n6.7.2 Multiple Histogram Plots distribution of variables\nIn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\nShow the codeAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n6.7.3 Drawing Statistical Point Map\nLastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\n\nShow the codetmap_mode(\"view\")\n\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\nShow the codest_is_valid(mpsz_svy21, reason = TRUE)\n\n  [1] \"Valid Geometry\"                                           \n  [2] \"Valid Geometry\"                                           \n  [3] \"Valid Geometry\"                                           \n  [4] \"Valid Geometry\"                                           \n  [5] \"Valid Geometry\"                                           \n  [6] \"Valid Geometry\"                                           \n  [7] \"Valid Geometry\"                                           \n  [8] \"Valid Geometry\"                                           \n  [9] \"Valid Geometry\"                                           \n [10] \"Valid Geometry\"                                           \n [11] \"Valid Geometry\"                                           \n [12] \"Valid Geometry\"                                           \n [13] \"Valid Geometry\"                                           \n [14] \"Valid Geometry\"                                           \n [15] \"Valid Geometry\"                                           \n [16] \"Valid Geometry\"                                           \n [17] \"Valid Geometry\"                                           \n [18] \"Valid Geometry\"                                           \n [19] \"Ring Self-intersection[27932.3925999999 21982.7971999999]\"\n [20] \"Ring Self-intersection[26885.4439000003 26668.3121000007]\"\n [21] \"Valid Geometry\"                                           \n [22] \"Valid Geometry\"                                           \n [23] \"Valid Geometry\"                                           \n [24] \"Ring Self-intersection[26920.1689999998 26978.5440999996]\"\n [25] \"Valid Geometry\"                                           \n [26] \"Valid Geometry\"                                           \n [27] \"Valid Geometry\"                                           \n [28] \"Valid Geometry\"                                           \n [29] \"Valid Geometry\"                                           \n [30] \"Valid Geometry\"                                           \n [31] \"Valid Geometry\"                                           \n [32] \"Valid Geometry\"                                           \n [33] \"Valid Geometry\"                                           \n [34] \"Valid Geometry\"                                           \n [35] \"Valid Geometry\"                                           \n [36] \"Valid Geometry\"                                           \n [37] \"Valid Geometry\"                                           \n [38] \"Valid Geometry\"                                           \n [39] \"Valid Geometry\"                                           \n [40] \"Valid Geometry\"                                           \n [41] \"Valid Geometry\"                                           \n [42] \"Valid Geometry\"                                           \n [43] \"Valid Geometry\"                                           \n [44] \"Valid Geometry\"                                           \n [45] \"Valid Geometry\"                                           \n [46] \"Valid Geometry\"                                           \n [47] \"Valid Geometry\"                                           \n [48] \"Valid Geometry\"                                           \n [49] \"Valid Geometry\"                                           \n [50] \"Valid Geometry\"                                           \n [51] \"Valid Geometry\"                                           \n [52] \"Valid Geometry\"                                           \n [53] \"Valid Geometry\"                                           \n [54] \"Valid Geometry\"                                           \n [55] \"Valid Geometry\"                                           \n [56] \"Valid Geometry\"                                           \n [57] \"Valid Geometry\"                                           \n [58] \"Valid Geometry\"                                           \n [59] \"Valid Geometry\"                                           \n [60] \"Valid Geometry\"                                           \n [61] \"Valid Geometry\"                                           \n [62] \"Valid Geometry\"                                           \n [63] \"Valid Geometry\"                                           \n [64] \"Valid Geometry\"                                           \n [65] \"Valid Geometry\"                                           \n [66] \"Valid Geometry\"                                           \n [67] \"Valid Geometry\"                                           \n [68] \"Valid Geometry\"                                           \n [69] \"Valid Geometry\"                                           \n [70] \"Valid Geometry\"                                           \n [71] \"Valid Geometry\"                                           \n [72] \"Valid Geometry\"                                           \n [73] \"Valid Geometry\"                                           \n [74] \"Valid Geometry\"                                           \n [75] \"Valid Geometry\"                                           \n [76] \"Valid Geometry\"                                           \n [77] \"Valid Geometry\"                                           \n [78] \"Valid Geometry\"                                           \n [79] \"Valid Geometry\"                                           \n [80] \"Valid Geometry\"                                           \n [81] \"Valid Geometry\"                                           \n [82] \"Valid Geometry\"                                           \n [83] \"Valid Geometry\"                                           \n [84] \"Valid Geometry\"                                           \n [85] \"Valid Geometry\"                                           \n [86] \"Valid Geometry\"                                           \n [87] \"Valid Geometry\"                                           \n [88] \"Valid Geometry\"                                           \n [89] \"Valid Geometry\"                                           \n [90] \"Valid Geometry\"                                           \n [91] \"Valid Geometry\"                                           \n [92] \"Valid Geometry\"                                           \n [93] \"Valid Geometry\"                                           \n [94] \"Valid Geometry\"                                           \n [95] \"Valid Geometry\"                                           \n [96] \"Valid Geometry\"                                           \n [97] \"Valid Geometry\"                                           \n [98] \"Valid Geometry\"                                           \n [99] \"Valid Geometry\"                                           \n[100] \"Valid Geometry\"                                           \n[101] \"Valid Geometry\"                                           \n[102] \"Valid Geometry\"                                           \n[103] \"Valid Geometry\"                                           \n[104] \"Valid Geometry\"                                           \n[105] \"Valid Geometry\"                                           \n[106] \"Valid Geometry\"                                           \n[107] \"Valid Geometry\"                                           \n[108] \"Valid Geometry\"                                           \n[109] \"Valid Geometry\"                                           \n[110] \"Valid Geometry\"                                           \n[111] \"Valid Geometry\"                                           \n[112] \"Valid Geometry\"                                           \n[113] \"Valid Geometry\"                                           \n[114] \"Valid Geometry\"                                           \n[115] \"Valid Geometry\"                                           \n[116] \"Valid Geometry\"                                           \n[117] \"Valid Geometry\"                                           \n[118] \"Valid Geometry\"                                           \n[119] \"Valid Geometry\"                                           \n[120] \"Valid Geometry\"                                           \n[121] \"Valid Geometry\"                                           \n[122] \"Ring Self-intersection[15432.4749999996 31319.716]\"       \n[123] \"Ring Self-intersection[12861.3828999996 32207.4923]\"      \n[124] \"Valid Geometry\"                                           \n[125] \"Valid Geometry\"                                           \n[126] \"Valid Geometry\"                                           \n[127] \"Valid Geometry\"                                           \n[128] \"Ring Self-intersection[19681.2353999997 31294.4521999992]\"\n[129] \"Valid Geometry\"                                           \n[130] \"Valid Geometry\"                                           \n[131] \"Valid Geometry\"                                           \n[132] \"Valid Geometry\"                                           \n[133] \"Valid Geometry\"                                           \n[134] \"Valid Geometry\"                                           \n[135] \"Valid Geometry\"                                           \n[136] \"Valid Geometry\"                                           \n[137] \"Valid Geometry\"                                           \n[138] \"Valid Geometry\"                                           \n[139] \"Valid Geometry\"                                           \n[140] \"Valid Geometry\"                                           \n[141] \"Valid Geometry\"                                           \n[142] \"Valid Geometry\"                                           \n[143] \"Valid Geometry\"                                           \n[144] \"Valid Geometry\"                                           \n[145] \"Valid Geometry\"                                           \n[146] \"Valid Geometry\"                                           \n[147] \"Valid Geometry\"                                           \n[148] \"Valid Geometry\"                                           \n[149] \"Valid Geometry\"                                           \n[150] \"Valid Geometry\"                                           \n[151] \"Valid Geometry\"                                           \n[152] \"Valid Geometry\"                                           \n[153] \"Valid Geometry\"                                           \n[154] \"Valid Geometry\"                                           \n[155] \"Valid Geometry\"                                           \n[156] \"Valid Geometry\"                                           \n[157] \"Valid Geometry\"                                           \n[158] \"Valid Geometry\"                                           \n[159] \"Valid Geometry\"                                           \n[160] \"Valid Geometry\"                                           \n[161] \"Valid Geometry\"                                           \n[162] \"Valid Geometry\"                                           \n[163] \"Valid Geometry\"                                           \n[164] \"Valid Geometry\"                                           \n[165] \"Valid Geometry\"                                           \n[166] \"Valid Geometry\"                                           \n[167] \"Valid Geometry\"                                           \n[168] \"Valid Geometry\"                                           \n[169] \"Valid Geometry\"                                           \n[170] \"Valid Geometry\"                                           \n[171] \"Valid Geometry\"                                           \n[172] \"Valid Geometry\"                                           \n[173] \"Valid Geometry\"                                           \n[174] \"Valid Geometry\"                                           \n[175] \"Valid Geometry\"                                           \n[176] \"Valid Geometry\"                                           \n[177] \"Valid Geometry\"                                           \n[178] \"Valid Geometry\"                                           \n[179] \"Valid Geometry\"                                           \n[180] \"Valid Geometry\"                                           \n[181] \"Valid Geometry\"                                           \n[182] \"Valid Geometry\"                                           \n[183] \"Valid Geometry\"                                           \n[184] \"Valid Geometry\"                                           \n[185] \"Valid Geometry\"                                           \n[186] \"Valid Geometry\"                                           \n[187] \"Valid Geometry\"                                           \n[188] \"Valid Geometry\"                                           \n[189] \"Valid Geometry\"                                           \n[190] \"Valid Geometry\"                                           \n[191] \"Valid Geometry\"                                           \n[192] \"Valid Geometry\"                                           \n[193] \"Valid Geometry\"                                           \n[194] \"Valid Geometry\"                                           \n[195] \"Valid Geometry\"                                           \n[196] \"Valid Geometry\"                                           \n[197] \"Valid Geometry\"                                           \n[198] \"Valid Geometry\"                                           \n[199] \"Valid Geometry\"                                           \n[200] \"Valid Geometry\"                                           \n[201] \"Valid Geometry\"                                           \n[202] \"Valid Geometry\"                                           \n[203] \"Valid Geometry\"                                           \n[204] \"Valid Geometry\"                                           \n[205] \"Valid Geometry\"                                           \n[206] \"Valid Geometry\"                                           \n[207] \"Valid Geometry\"                                           \n[208] \"Valid Geometry\"                                           \n[209] \"Valid Geometry\"                                           \n[210] \"Valid Geometry\"                                           \n[211] \"Valid Geometry\"                                           \n[212] \"Valid Geometry\"                                           \n[213] \"Valid Geometry\"                                           \n[214] \"Valid Geometry\"                                           \n[215] \"Valid Geometry\"                                           \n[216] \"Valid Geometry\"                                           \n[217] \"Valid Geometry\"                                           \n[218] \"Valid Geometry\"                                           \n[219] \"Valid Geometry\"                                           \n[220] \"Valid Geometry\"                                           \n[221] \"Valid Geometry\"                                           \n[222] \"Valid Geometry\"                                           \n[223] \"Valid Geometry\"                                           \n[224] \"Valid Geometry\"                                           \n[225] \"Valid Geometry\"                                           \n[226] \"Valid Geometry\"                                           \n[227] \"Valid Geometry\"                                           \n[228] \"Valid Geometry\"                                           \n[229] \"Valid Geometry\"                                           \n[230] \"Valid Geometry\"                                           \n[231] \"Valid Geometry\"                                           \n[232] \"Valid Geometry\"                                           \n[233] \"Valid Geometry\"                                           \n[234] \"Valid Geometry\"                                           \n[235] \"Valid Geometry\"                                           \n[236] \"Valid Geometry\"                                           \n[237] \"Valid Geometry\"                                           \n[238] \"Valid Geometry\"                                           \n[239] \"Valid Geometry\"                                           \n[240] \"Valid Geometry\"                                           \n[241] \"Valid Geometry\"                                           \n[242] \"Valid Geometry\"                                           \n[243] \"Valid Geometry\"                                           \n[244] \"Valid Geometry\"                                           \n[245] \"Valid Geometry\"                                           \n[246] \"Valid Geometry\"                                           \n[247] \"Valid Geometry\"                                           \n[248] \"Valid Geometry\"                                           \n[249] \"Valid Geometry\"                                           \n[250] \"Valid Geometry\"                                           \n[251] \"Valid Geometry\"                                           \n[252] \"Valid Geometry\"                                           \n[253] \"Valid Geometry\"                                           \n[254] \"Valid Geometry\"                                           \n[255] \"Valid Geometry\"                                           \n[256] \"Valid Geometry\"                                           \n[257] \"Valid Geometry\"                                           \n[258] \"Ring Self-intersection[41375.108 40432.8588999994]\"       \n[259] \"Valid Geometry\"                                           \n[260] \"Valid Geometry\"                                           \n[261] \"Valid Geometry\"                                           \n[262] \"Valid Geometry\"                                           \n[263] \"Valid Geometry\"                                           \n[264] \"Valid Geometry\"                                           \n[265] \"Valid Geometry\"                                           \n[266] \"Valid Geometry\"                                           \n[267] \"Valid Geometry\"                                           \n[268] \"Valid Geometry\"                                           \n[269] \"Valid Geometry\"                                           \n[270] \"Valid Geometry\"                                           \n[271] \"Valid Geometry\"                                           \n[272] \"Valid Geometry\"                                           \n[273] \"Valid Geometry\"                                           \n[274] \"Valid Geometry\"                                           \n[275] \"Valid Geometry\"                                           \n[276] \"Valid Geometry\"                                           \n[277] \"Valid Geometry\"                                           \n[278] \"Valid Geometry\"                                           \n[279] \"Valid Geometry\"                                           \n[280] \"Valid Geometry\"                                           \n[281] \"Valid Geometry\"                                           \n[282] \"Valid Geometry\"                                           \n[283] \"Valid Geometry\"                                           \n[284] \"Valid Geometry\"                                           \n[285] \"Valid Geometry\"                                           \n[286] \"Valid Geometry\"                                           \n[287] \"Valid Geometry\"                                           \n[288] \"Valid Geometry\"                                           \n[289] \"Valid Geometry\"                                           \n[290] \"Valid Geometry\"                                           \n[291] \"Valid Geometry\"                                           \n[292] \"Valid Geometry\"                                           \n[293] \"Valid Geometry\"                                           \n[294] \"Valid Geometry\"                                           \n[295] \"Valid Geometry\"                                           \n[296] \"Valid Geometry\"                                           \n[297] \"Valid Geometry\"                                           \n[298] \"Valid Geometry\"                                           \n[299] \"Valid Geometry\"                                           \n[300] \"Valid Geometry\"                                           \n[301] \"Valid Geometry\"                                           \n[302] \"Ring Self-intersection[38542.2260999996 44605.4089000002]\"\n[303] \"Valid Geometry\"                                           \n[304] \"Valid Geometry\"                                           \n[305] \"Valid Geometry\"                                           \n[306] \"Valid Geometry\"                                           \n[307] \"Valid Geometry\"                                           \n[308] \"Valid Geometry\"                                           \n[309] \"Valid Geometry\"                                           \n[310] \"Valid Geometry\"                                           \n[311] \"Valid Geometry\"                                           \n[312] \"Valid Geometry\"                                           \n[313] \"Valid Geometry\"                                           \n[314] \"Valid Geometry\"                                           \n[315] \"Valid Geometry\"                                           \n[316] \"Valid Geometry\"                                           \n[317] \"Valid Geometry\"                                           \n[318] \"Valid Geometry\"                                           \n[319] \"Valid Geometry\"                                           \n[320] \"Ring Self-intersection[21702.5623000003 48125.1154999994]\"\n[321] \"Valid Geometry\"                                           \n[322] \"Valid Geometry\"                                           \n[323] \"Valid Geometry\"                                           \n\n\n\nShow the codetmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\n\nShow the codetmap_mode(\"plot\")"
  },
  {
    "objectID": "posts/Geo/geospatial_linearRegression/index.html#hedonic-pricing-modelling-in-r",
    "href": "posts/Geo/geospatial_linearRegression/index.html#hedonic-pricing-modelling-in-r",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6.8 Hedonic Pricing Modelling in R",
    "text": "6.8 Hedonic Pricing Modelling in R\nIn this section, you will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n6.8.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\nShow the codecondo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\nShow the codesummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: < 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n      *y = -258121.1 + 14719x1*\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nShow the codeggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n6.8.2 Multiple Linear Regression Method\n6.8.2.1 Visualising the relationships of the independent variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\nShow the codecorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n6.8.3 Building a hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\nShow the codecondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  < 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  < 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  < 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  < 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: < 2.2e-16\n\n\n6.8.4 Preparing Publication Quality Table: olsrr method\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\nShow the codecondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.591 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n6.8.5 Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\nShow the codetbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\nCharacteristic\n      Beta\n      \n95% CI1\n\n      p-value\n    \n\n\n(Intercept)\n527,633\n315,417, 739,849\n<0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n<0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n<0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n<0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n<0.001\n\n\n\n\n1 CI = Confidence Interval\n    \n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\nShow the codetbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n\nCharacteristic\n      Beta\n      \n95% CI1\n\n      p-value\n    \n\n\n(Intercept)\n527,633\n315,417, 739,849\n<0.001\n\n\nAREA_SQM\n12,778\n12,057, 13,498\n<0.001\n\n\nAGE\n-24,688\n-30,092, -19,284\n<0.001\n\n\nPROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n\n\nPROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n\n\nPROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n\n\nPROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n\n\nPROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n\n\nPROX_PARK\n570,505\n442,004, 699,006\n<0.001\n\n\nPROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n\n\nPROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n\n\nPROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n\n\nNO_Of_UNITS\n-245\n-418, -73\n0.005\n\n\nFAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n\n\nFREEHOLD\n350,600\n255,448, 445,752\n<0.001\n\n\n\nR² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = <0.001; σ = 755,957\n    \n\n\n1 CI = Confidence Interval\n    \n\n\n\n\nFor more customisation options, refer to Tutorial: tbl_regression\n6.8.5.1 Checking for multicolinearity\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\nShow the codeols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n6.8.5.2 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nShow the codeols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n6.8.5.3 Test for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nShow the codeols_plot_resid_hist(condo.mlr1)\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\nShow the codeols_test_normality(condo.mlr1)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n6.8.5.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nShow the codemlr.output <- as.data.frame(condo.mlr1$residuals)\n\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\nShow the codecondo_resale.res.sf <- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %>%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\nShow the codecondo_resale.sp <- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\nShow the codetmap_mode(\"view\")\n\n\nThe code chunks below is used to create an interactive point symbol map.\n\nShow the codetm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nRemember to switch back to “plot” mode before continue.\n\nShow the codetmap_mode(\"plot\")\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nShow the codenb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nShow the codenb_lw <- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nShow the codelm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "posts/Geo/geospatial_linearRegression/index.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "posts/Geo/geospatial_linearRegression/index.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6.9 Building Hedonic Pricing Models using GWmodel",
    "text": "6.9 Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n6.9.1 Building Fixed Bandwidth GWR Model\n6.9.1.1 Computing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\nShow the codebw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3398 metres.\n6.9.1.2 GWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\nShow the codegwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\n\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\n\nShow the codegwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-10 11:38:55 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2022-12-10 11:38:56 \n\n\nThe report shows that the adjusted r-square of the gwr is 0.8430 which is significantly better than the globel multiple linear regression model of 0.6472.\n6.9.2 Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-absed hedonic pricing model by using adaptive bandwidth approach.\n6.9.2.1 Computing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.ger() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\nShow the codebw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n6.9.2.2 Constructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\nShow the codegwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\n\nThe code below can be used to display the model output.\n\nShow the codegwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-10 11:39:02 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2022-12-10 11:39:03 \n\n\nThe report shows that the adjusted r-square of the gwr is 0.8561 which is significantly better than the globel multiple linear regression model of 0.6472. The AICc for the weighted regression also shows a smaller value compared to the global regression model which mean that the weighted regression model is slightly better in performance that the global regression model.\nThe result also shows illogical based on estimate derive from the summary of the regression model.\n6.9.3 Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n6.9.4 Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\nShow the codecondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%\n  st_transform(crs=3414)\n\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)\ncondo_resale.sf.adaptive.svy21  \n\nSimple feature collection with 1436 features and 51 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 14940.85 ymin: 24765.67 xmax: 43352.45 ymax: 48382.81\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n    Intercept  AREA_SQM        AGE  PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n1   2050011.7  9561.892  -9514.634 -120681.9      319266.92       -393417.79\n2   1633128.2 16576.853 -58185.479 -149434.2      441102.18        325188.74\n3   3433608.2 13091.861 -26707.386 -259397.8     -120116.82        535855.81\n4    234358.9 20730.601 -93308.988 2426853.7      480825.28        314783.72\n5   2285804.9  6722.836 -17608.018 -316835.5       90764.78       -137384.61\n6  -3568877.4  6039.581 -26535.592  327306.1     -152531.19       -700392.85\n7  -2874842.4 16843.575 -59166.727 -983577.2     -177810.50       -122384.02\n8   2038086.0  6905.135 -17681.897 -285076.6       70259.40        -96012.78\n9   1718478.4  9580.703 -14401.128  105803.4     -657698.02       -123276.00\n10  3457054.0 14072.011 -31579.884 -234895.4       79961.45        548581.04\n   PROX_URA_GROWTH_AREA    PROX_MRT  PROX_PARK PROX_PRIMARY_SCH\n1            -159980.20  -299742.96 -172104.47        242668.03\n2            -142290.39 -2510522.23  523379.72       1106830.66\n3            -253621.21  -936853.28  209099.85        571462.33\n4           -2679297.89 -2039479.50 -759153.26       3127477.21\n5             303714.81   -44567.05  -10284.62         30413.56\n6             -28051.25   733566.47 1511488.92        320878.23\n7            1397676.38 -2745430.34  710114.74       1786570.95\n8             269368.71   -14552.99   73533.34         53359.73\n9            -361974.72  -476785.32 -132067.59        -40128.92\n10           -150024.38 -1503835.53  574155.47        108996.67\n   PROX_SHOPPING_MALL PROX_BUS_STOP  NO_Of_UNITS FAMILY_FRIENDLY  FREEHOLD\n1          300881.390     1210615.4  104.8290640       -9075.370  303955.6\n2          -87693.378     1843587.2 -288.3441183      310074.664  396221.3\n3         -126732.712     1411924.9   -9.5532945        5949.746  168821.7\n4          -29593.342     7225577.5 -161.3551620     1556178.531 1212515.6\n5           -7490.586      677577.0   42.2659674       58986.951  328175.2\n6          258583.881     1086012.6 -214.3671271      201992.641  471873.1\n7         -384251.210     5094060.5   -0.9212521      359659.512  408871.9\n8          -39634.902      735767.1   30.1741069       55602.506  347075.0\n9          276718.757     2815772.4  675.1615559      -30453.297  503872.8\n10        -454726.822     2123557.0  -21.3044311     -100935.586  213324.6\n         y    yhat    residual CV_Score Stud_residual Intercept_SE AREA_SQM_SE\n1  3000000 2886532   113468.16        0    0.38207013     516105.5    823.2860\n2  3880000 3466801   413198.52        0    1.01433140     488083.5    825.2380\n3  3325000 3616527  -291527.20        0   -0.83780678     963711.4    988.2240\n4  4250000 5435482 -1185481.63        0   -2.84614670     444185.5    617.4007\n5  1400000 1388166    11834.26        0    0.03404453    2119620.6   1376.2778\n6  1320000 1516702  -196701.94        0   -0.72065800   28572883.7   2348.0091\n7  3410000 3266881   143118.77        0    0.41291992     679546.6    893.5893\n8  1420000 1431955   -11955.27        0   -0.03033109    2217773.1   1415.2604\n9  2025000 1832799   192200.83        0    0.52018109     814281.8    943.8434\n10 2550000 2223364   326635.53        0    1.10559735    2410252.0   1271.4073\n      AGE_SE PROX_CBD_SE PROX_CHILDCARE_SE PROX_ELDERLYCARE_SE\n1   5889.782    37411.22          319111.1           120633.34\n2   6226.916    23615.06          299705.3            84546.69\n3   6510.236    56103.77          349128.5           129687.07\n4   6010.511   469337.41          304965.2           127150.69\n5   8180.361   410644.47          698720.6           327371.55\n6  14601.909  5272846.47         1141599.8          1653002.19\n7   8970.629   346164.20          530101.1           148598.71\n8   8661.309   438035.69          742532.8           399221.05\n9  11791.208    89148.35          704630.7           329683.30\n10  9941.980   173532.77          500976.2           281876.74\n   PROX_URA_GROWTH_AREA_SE PROX_MRT_SE PROX_PARK_SE PROX_PRIMARY_SCH_SE\n1                 56207.39    185181.3     205499.6            152400.7\n2                 76956.50    281133.9     229358.7            165150.7\n3                 95774.60    275483.7     314124.3            196662.6\n4                470762.12    279877.1     227249.4            240878.9\n5                474339.56    363830.0     364580.9            249087.7\n6               5496627.21    730453.2    1741712.0            683265.5\n7                371692.97    375511.9     297400.9            344602.8\n8                517977.91    423155.4     440984.4            261251.2\n9                153436.22    285325.4     304998.4            278258.5\n10               239182.57    571355.7     599131.8            331284.8\n   PROX_SHOPPING_MALL_SE PROX_BUS_STOP_SE NO_Of_UNITS_SE FAMILY_FRIENDLY_SE\n1               109268.8         600668.6       218.1258           131474.7\n2                98906.8         410222.1       208.9410           114989.1\n3               119913.3         464156.7       210.9828           146607.2\n4               177104.1         562810.8       361.7767           108726.6\n5               301032.9         740922.4       299.5034           160663.7\n6              2931208.6        1418333.3       602.5571           331727.0\n7               249969.5         821236.4       532.1978           129241.2\n8               351634.0         775038.4       338.6777           171895.1\n9               289872.7         850095.5       439.9037           220223.4\n10              265529.7         631399.2       259.0169           189125.5\n   FREEHOLD_SE Intercept_TV AREA_SQM_TV     AGE_TV PROX_CBD_TV\n1     115954.0    3.9720784   11.614302  -1.615447 -3.22582173\n2     130110.0    3.3460017   20.087361  -9.344188 -6.32792021\n3     141031.5    3.5629010   13.247868  -4.102368 -4.62353528\n4     138239.1    0.5276150   33.577223 -15.524302  5.17080808\n5     210641.1    1.0784029    4.884795  -2.152474 -0.77155660\n6     374347.3   -0.1249043    2.572214  -1.817269  0.06207388\n7     182216.9   -4.2305303   18.849348  -6.595605 -2.84136028\n8     216649.4    0.9189786    4.879056  -2.041481 -0.65080678\n9     220473.7    2.1104224   10.150733  -1.221345  1.18682383\n10    206346.2    1.4343123   11.068059  -3.176418 -1.35360852\n   PROX_CHILDCARE_TV PROX_ELDERLYCARE_TV PROX_URA_GROWTH_AREA_TV PROX_MRT_TV\n1         1.00048819          -3.2612693            -2.846248368 -1.61864578\n2         1.47178634           3.8462625            -1.848971738 -8.92998600\n3        -0.34404755           4.1319138            -2.648105057 -3.40075727\n4         1.57665606           2.4756745            -5.691404992 -7.28705261\n5         0.12990138          -0.4196596             0.640289855 -0.12249416\n6        -0.13361179          -0.4237096            -0.005103357  1.00426206\n7        -0.33542751          -0.8235874             3.760298131 -7.31116712\n8         0.09462126          -0.2405003             0.520038994 -0.03439159\n9        -0.93339393          -0.3739225            -2.359121712 -1.67102293\n10        0.15961128           1.9461735            -0.627237944 -2.63204802\n   PROX_PARK_TV PROX_PRIMARY_SCH_TV PROX_SHOPPING_MALL_TV PROX_BUS_STOP_TV\n1   -0.83749312           1.5923022            2.75358842        2.0154464\n2    2.28192684           6.7019454           -0.88662640        4.4941192\n3    0.66565951           2.9058009           -1.05686949        3.0419145\n4   -3.34061770          12.9836105           -0.16709578       12.8383775\n5   -0.02820944           0.1220998           -0.02488294        0.9145046\n6    0.86781794           0.4696245            0.08821750        0.7656963\n7    2.38773567           5.1844351           -1.53719231        6.2029165\n8    0.16674816           0.2042469           -0.11271635        0.9493299\n9   -0.43301073          -0.1442145            0.95462153        3.3123012\n10   0.95831249           0.3290120           -1.71252687        3.3632555\n   NO_Of_UNITS_TV FAMILY_FRIENDLY_TV FREEHOLD_TV  Local_R2\n1     0.480589953        -0.06902748    2.621347 0.8846744\n2    -1.380026395         2.69655779    3.045280 0.8899773\n3    -0.045279967         0.04058290    1.197050 0.8947007\n4    -0.446007570        14.31276425    8.771149 0.9073605\n5     0.141120178         0.36714544    1.557983 0.9510057\n6    -0.355762335         0.60891234    1.260522 0.9247586\n7    -0.001731033         2.78285441    2.243875 0.8310458\n8     0.089093858         0.32346758    1.602012 0.9463936\n9     1.534793921        -0.13828365    2.285410 0.8380365\n10   -0.082251138        -0.53369623    1.033819 0.9080753\n                    geometry\n1  POINT (22085.12 29951.54)\n2   POINT (25656.84 34546.2)\n3   POINT (23963.99 32890.8)\n4  POINT (27044.28 32319.77)\n5  POINT (41042.56 33743.64)\n6   POINT (39717.04 32943.1)\n7   POINT (28419.1 33513.37)\n8  POINT (40763.57 33879.61)\n9  POINT (23595.63 28884.78)\n10 POINT (24586.56 33194.31)\n\nShow the codegwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\n\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\n\nShow the codeglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                <dbl> 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       <dbl> 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 <dbl> -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               <dbl> 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              <dbl> 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   <dbl> -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              <dbl> -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        <dbl> 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      <dbl> -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  <dbl> -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              <dbl> -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             <dbl> -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      <dbl> 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    <dbl> 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         <dbl> 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           <dbl> 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       <dbl> -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              <dbl> 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    <dbl> 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                <dbl> 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           <dbl> 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            <dbl> 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             <dbl> 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  <dbl> 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             <dbl> 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       <dbl> 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     <dbl> 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE <dbl> 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             <dbl> 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            <dbl> 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     <dbl> 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   <dbl> 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        <dbl> 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          <dbl> 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      <dbl> 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             <dbl> 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            <dbl> 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             <dbl> 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  <dbl> -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             <dbl> -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       <dbl> 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     <dbl> -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV <dbl> -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             <dbl> -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            <dbl> -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     <dbl> 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   <dbl> 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        <dbl> 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          <dbl> 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      <dbl> -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             <dbl> 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                <dbl> 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               <dbl> 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               <dbl> 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                <POINT [m]> POINT (22085.12 29951.54), POINT (25656.…\n\n\n\nShow the codesummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n6.9.5 Visualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\nShow the codetmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\nShow the codetmap_mode(\"plot\")\n\n\n6.9.5.1 By URA Plannign Region\n\nShow the codetm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)\n\n\n\n\n6.9.6 Visualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\nShow the codetmap_mode(\"view\")\nAREA_SQM_SE <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)"
  }
]